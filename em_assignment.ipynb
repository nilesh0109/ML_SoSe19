{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "em_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlo4V6MRBEDQ",
        "colab_type": "text"
      },
      "source": [
        "# First things first\n",
        "Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMlNHfVxBEDT",
        "colab_type": "text"
      },
      "source": [
        "# Expectation-maximization algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icS4MsxIBEDU",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, we will derive and implement formulas for Gaussian Mixture Model — one of the most commonly used methods for performing soft clustering of the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jequoJfSBEDV",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "Loading auxiliary files and importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZo-y9UBEDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7672026f-7537-467d-f227-88744dfc1d30"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    print(\"Downloading Colab files\")\n",
        "    ! shred -u setup_google_colab.py\n",
        "    ! wget https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/setup_google_colab.py -O setup_google_colab.py\n",
        "    import setup_google_colab\n",
        "    setup_google_colab.load_data_week2()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Colab files\n",
            "--2020-04-13 10:42:19--  https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1254 (1.2K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "setup_google_colab. 100%[===================>]   1.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-13 10:42:19 (206 MB/s) - ‘setup_google_colab.py’ saved [1254/1254]\n",
            "\n",
            "https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/week2/w2_grader.py w2_grader.py\n",
            "https://raw.githubusercontent.com/hse-aml/bayesian-methods-for-ml/master/week2/samples.npz samples.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urylZcbeBEDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import slogdet, det, solve\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy\n",
        "from sklearn.datasets import load_digits\n",
        "from w2_grader import EMGrader\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP8l9frZBEDf",
        "colab_type": "text"
      },
      "source": [
        "### Grading\n",
        "We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gvy3EOvBEDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grader = EMGrader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL3A2sntBEDj",
        "colab_type": "text"
      },
      "source": [
        "## Implementing EM for GMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPS_VdpBEDk",
        "colab_type": "text"
      },
      "source": [
        "For debugging, we will use samples from a Gaussian mixture model with unknown mean, variance, and priors. We also added initial values of parameters for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9_aOn94BEDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cf14fd80-cc72-46d6-cbcb-dd1c0fd163d2"
      },
      "source": [
        "samples = np.load('samples.npz')\n",
        "X = samples['data']\n",
        "pi0 = samples['pi0']\n",
        "mu0 = samples['mu0']\n",
        "sigma0 = samples['sigma0']\n",
        "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2de2xc53nmn3fIIVXKzsT2UE45ytQGVMuVrUld0t22A3TrpLtI1LiOgSLJig2qdiFBwlZNuwWKXoDtXwsU2KJbw2kjiEnKLiK1K6RJL4vZJm7lYrEDNBCHkYd2JGqJFKVFSuSMnUxsXXh99w/yjM+cObeZc4ZzzszzAwyLcznzcTjznPc833sRVQUhhJD4kuj2AgghhASDQk4IITGHQk4IITGHQk4IITGHQk4IITFnsBsvmk6n9bHHHuvGSxNCSGwplUpVVR213t4VIX/ssccwMzPTjZcmhJDYIiL/anc7rRVCCIk5FHJCCIk5FHJCCIk5FHJCCIk5FHJCCIk5XclaIfGkVquhWCxiaWkJmUwG+XweqVSq28sipO+hkBNf1Go1nDt3Duvr69je3sbt27cxNzeH06dPU8wJ6TKhWCsi8hsi8oaIvC4ifyEi+8I4LokOxWKxLuIAsL29jfX1dRSLRQA7Ql8oFDA1NYVCoYBardbN5RLSVwSOyEUkA+DXABxR1XsicgnApwFMBz02iQ5LS0t1ETfY3t7G0tISo3VCukxY1soggB8QkQ0AIwCWQzouiQiZTAa3b99uEPNEIoFMJuMarR87dgyAs79O352Q4AQWclVdEpE/BLAI4B6Ab6jqN6yPE5FTAE4BQDabDfqyZI/J5/OYm5urC3YikcDQ0BDy+TwuXbrkGK0Dzv765OQkLly4gLW1NagqlpeXMTs7ixMnTuDgwYPd+DUJiSWBPXIReQjACwAeBzAGYL+I/KL1cap6XlUnVHVidLSp5wuJOKlUCqdPn8b4+DjGxsYwPj5et04ymQwSicaPkhGtA87++t/93d/VRdxga2sL09PT9NgJaYEwrJWfBfAvqloBABH5KoCfAvDlEI5NIkQqlapbJWbconXA2V//7ne/C7uZsVtbWw22DCHEnTCEfBHAT4jICHaslY8AYGvDPsKI1q1eNwDHDJZEIoGHHnoIq6urtsc0bBlCiDdheOTfFJGvAJgFsAngWwDOBz0uiRfmaL1Wq+Hy5cuYm5uzjbiNiP3555/H9PQ0tra2mu43bBlCiDehZK2o6u8D+P0wjkXijbGxef/+fdv7R0ZG8NRTT9WzU06cONEg5lZbhhDiDSs7SagYG5tOPPjggw3e98GDB3H27FmmIBISAAo58Y2fnG+7jU0zdlaL0yZqJ9dJSC9BISe+uHnzZoMF4lS9aVc4ZEZEOrpOVpmSfoRCTpqwRrS5XK5pU9KuehN4LxXRziMXEXzgAx9AoVDwjJbbjar9VJkS0mtQyEkDdhHt7OxsU2YJ0Fi9aWCkIjplrRi3qapjtBwkqnbrCUNIr8LBEqQBu4jWTsQN7NIEU6kUXnzxRXz2s59FLper2ymqiu3t7bq4WzsoAjsifvHiRdy/f9+x06IbXlWmhPQiFHLSgNdmpZmBgQHXNMFUKoXh4WFXX9yuJ4tdkZDfqDqfz2NoaKgu5kxnJP0AhZw0YBfROpFMJlEsFl37ovg5MVh7stjhN6p26wlDSK9Cj5w0YNc3xfC0rdy/fx+lUgnlchmHDx9GtVpt2pj0k8Xi1pPFoJWoOux0RkKiDoWcNGDXN2VtbQ2vv/66rchub29jbW0N5XIZQHNaolsWCwDkcjlP0T9w4ACOHz/eVlTNnHLSD9BaIZ48++yzDb6zG9aNSePEYN70BHaskn379uG5556r32bnb+/bty+QiJ87dw6lUgnLy8solUo4d+4cW+SSnkPsLpk7zcTEhM7MsEFiFLGm/hmbhZOTkyiXy3j99ddx7949z+OMjY3h5MmTTcf2io7dHtNqdF0oFFAqlZqmGo2Pj9N6IbFEREqqOmG9ndYKacCpoKZcLiOfz9ctFDecNib9eNdOj2knt5w55aRfoJCTBpzEb3FxEQCwsbHR9BzDCtne3oaIQESwuLiIQqFQ36AsFotYXFyEqkJEkM1mW/Kr26nYdJszSkgvQSEnDWQyGdy6daspS6VarQKA7Ybn6OgostksFhcXUa1WoapYWVlBpVKpR/AbGxsNz11dXW2IqL1sk3aia6/JRYT0CtzsJA3k83nbTU0jBdGuajKbzeLYsWPIZrP16k3gvYyWtbW1JhFW1XpE7WdT0im/fXNz03HzkjnlpF+gkJMGUqkU0ul00+2GbeJWNdlKVahxzKWlJVvb5P79+zh//nx9VJw1o8WgUqm4ZqIYnvvJkydx7NgxijjpSSjkpIlsNusYebtFuK1UhRrHzGQyWFxctD0B3L17tx6dA8Dp06ebTjLmyJ6QfoUeOWnCzVt2yzyxPg9AffPTKtRGdJ/L5TA7O+u4Fuum5uBg80eWmSi9Dwu73KGQR5RufnDtqjv99g2fnJzElStXGtrV2tUq5HI5PPfccygWi552jFmomYnSf3BYiDcU8ggShQ+un5xvp3UePnwYImIr4MB73noqlcLS0pLj48yPN4TaKerP5XJt/JaNvwsjvr3F73vOYSHe0COPIG4f3CjhtM6FhQXXKNsaYbv56tYN1VQqhcnJyYZy/+3tbVy4cKHt0nuW8u89rbznLOzyhhF5BAn7g9tutGk38q1cLtd/ttuk3N7e9rRK3CLsRCKBZDKJJ598EpVKxXa95XK5IYo3b3i2E6Ex4tt7WnnPaad5QyGPIGF+cJ3sD6N3ipO4W4ct37p1C1euXKlbJrdv365vZJpFNZFIYGRkxLEfi12E7dePNwj7RMeIb+9p5T1nYZc3FPIIEuYH1ynymZ6erhfvWD34Wq3WNGzZEGvzmLZEIlHvV25ep11mCQAMDg7imWeeaRLqVvuHhx2hMeLbe1p5z9s52fcbFPIIEuYH1ynysf5svqwtFouuczrNz3v00UeRzWYb1lksFlGpVGyrOY3fw2zbpNNpiIijlWLFyY5ZX1/H1NRUKMdjxNdZWn3POSzEHbax7XHsWrk6YbSenZqawvLysufjzS1hzcI8OjqKubk529zxiYkJ5PP5BrvHesyhoSHPDB3riWB+fr6pn4vT8ez2DAAw4ttjmCnUOmxj2+M4fSnsIh8RaYq4zZe1To2zANQ9cXMEZefD2504VLXeRdFOxAH/G43mCK1QKNiKuN3x3FI7GfHtLYyyw4Pphz2AWyqXYdMcPXoU+/fvx/DwsK3gJZPJemSaz+cxPDzckOI3MDCA559/HqOjo0gmk0in05icnEQqlbL14Z2oVquOJfkGrW40evV4MR8vLqmdhLQChbwH8CNO8/PzuHfvHu7du2cbaScSiXonQkP8JyYmMDY2hmeffRYnTpzAK6+8gmq1io2NDVSr1Xrutpcwm3HqomhdSysbjX5y0UdHR1EoFHD16lVmqHSBWq2GQqGAqampeiM0Eh60VmKMYad4iZNV6O24d+8eSqVSQ/aKcdlbq9Vw8eLFhgHK5pNFK/ss5i6KTh65dTCFl29qV+1pPl4ymcT169cd7RdmqHSWKFQq9zqhROQi8n4R+YqIXBeRayLyk2EclzhjtlOcpvYY4uS3vaxdJG+8zurqqu3jl5aWGiwYP6gqJicn610Uc7kcPvShD+HRRx+tN9haWVnxXWFp7TtuHG9sbAxHjx7FAw88YNsTHWjOayfhQzur84QVkb8E4O9V9RdEZAjASEjH7Xna3bl3i7Kt4mSXs+uE1WYwXscO88lidXXVd2ReqVRw4cKFpojsa1/7GlZWVhrW4rfC0m7jzDgJma8kzCSTSfzoj/4osyU6DAuuOk9gIReRFICfBnACAFR1HYD9N580EOSS0ynKthMnN+vBjrW1tbpX7hbNm08Wc3NzWFtbsxXz4eFhrK2t1X+2K6mv1WqYm5trem6QL7zXSch4n9zaEFDkg8OCq84ThrXyOIAKgD8TkW+JyBdEZL/1QSJySkRmRGSmUqmE8LLxJ8glp90GnyFO1kk4VuthZMT9gumtt97Cn/7pn+Kv//qv8b3vfc/2MY888kj9hGMcf3R01PaxdmJqF/k7RfTtfuHdTkKqinfffRef//zn69k+MzMz+OIXv8jmWSFjne5EOyt8whDyQQA/BuDzqvoMgDsAftv6IFU9r6oTqjrh9IXvN4Jccrb65TCPPHvqqac8J/msr6/jtddew927d23vf/fdd/HOO+/UMxGKxaKjV+6UJWMWaLffud0WtW7ZLKqKa9euNXjn5vYDxv/p5QaHs1M7Txge+U0AN1X1m7s/fwU2Qk6aCXLJGaSM37BanLxjP2xsbDT0a3EqILLD7qSTyWQcq0lnZmZw8ODBltfYqqVkB73ccGDxT2cJHJGr6m0Ab4rI4d2bPgLg20GP2w8EveRsd7CwcRLYv7/JAfPN9vY2tra2mqJZLwYHB20jMreou1wut2VvGL3L0+k0ksmkYzMvN+jltg9zx/eOsLJWzgK4sJux8h0AvxzScXuabnd1279/P+7cubMnr2WwubnZsPFpUC6XHZ+jqm31Bq/Varhw4YLviNzcfoDNs4LB3PG9JRQhV9WrAJoauRBvunHJaf6S2XHo0CG8+eab9QIaq8AFpVwu48aNGw1fai/7oh17w6sQym6IRb9nrYTVyIrDOvYWVnb2IU4Cd+DAARw/frypzaxZ4K5evWpbgNQqa2trDV9qt1x3P/aGnQA5Za3s378fqVTKUaja8eN7gTCj6FY28tkFMTgU8j7ESeAGBwfrXyBrib7xRXvooYdQqVRaKsu3Q1XrX+pareaYg26U87vZG25DoO02k48cOcKo0IYwo2i/G/m0YMKBQt6HtJItY/2iGROBwmBzcxM3b96s+9jGcUUEDz/8MAYHB5HNZj0jNCcBUtWGni70vN0JswLT7+AIWjDhQCHvYVrpUe4kcHZfNOPxdumLjzzyCL773e/68tKr1WrTSDlgJ1rPZDJ48cUXff0+TgJUrVY5IqwFwqzA9LuRz/L9cKCQ9yhel6x+Bc6uRa2RemjH4OBgQxRsbJRahzQbx3Fibm4OH/7wh+trsg6DNv8+bgLE/GX/2OXdG+0UjJYNgH9P2897z/L9cGA/8pjilaPrVf7vJwe9VquhWq023Z5IJPDQQw/ZtgjIZrM4ffo0nn76aYyMjGBkZASHDh3Cww8/3NLvZ6QcGuuwRu7m34cl4OFgnOCffvrpepWuqmJubq7eqsBtiEk78G8XDozIY4ifDSK/l6xu0VWxWLSNmkUEzz//fEOOtvULeOPGjfp9CwsLbf2e5n7qdlcAxu/T7Xz8XiKVStWnQ5lbFpiDgDA9bf7twoFCHkP8bBD5uWT1OiEsLS3Zbmym02kcPHjQ8QtYKBQClcVb1+rml25ubtYv+2mhhINXEBC2p82/XXBorcQQP9G2n0tWL/vFqcNiNpsF4GzPtDL6zQ5rymEmk3FsyFWpVNihMGSc/u6ZTMb1PtI9GJHHBLMFsrm52VRlaf0yWS9ZR0dHoaq4dOmSa8GM+YTQSnaLsb7FxcWG4RCtkk6n8fjjj9v2U7fLNbfrbU6C4fV3t7svl8uhUCi0ZI+wECg8JKyc4FaYmJjQmZmZPX/duGK1QJx6gjgVUdjlgg8NDeGJJ57A66+/3iTmuVyunvrn58tmHN+pqMcvhw4dqndRPHToUEPWink+qV1l6djYGE6ePNn2a5NGzH93IwioVqtNbQxGR0exvr6O69ev1//2Xp9H4/h2n0kWArkjIiVVbWqHwog8BlgtECOdL51OY3Bw0DOacbJQnErt5+fn676zH/+yWCx6ivi+ffvw8MMPI51O2548ADRsipbLZczPz+PMmTNN6yiVSkxX6xDWE/fHPvaxhk1t8z4KANtRen42QFkIFC4U8hhgZ4GoKgYHB31FoU4WijmKMrOxsdH0hXKLzJ02RQ0SiQSOHj1aH6vmt/mWtR8L0JrdQ1rDbvN7dnYW29vbrhksdnhtgLIQKFwo5DEgaNGEU0MqJ/G1fqG8slvchkIAO3NE3333Xbz00kstWy+Li4tN3ivT1TqDXZRsh1sGi4HX55OFQOHCrJUYELRowu75Tlkgxv3mL5RXdovbUIjh4WEAwLVr19ryz6vValPxCYC2BmoQd9xmnJpxy2Ax8Pp8shAoXBiRx4CgRRN2z19bW3P0qq1fKK/LYLehEIlEwnaQhEEymcSRI0dw7dq1pst0o0EXfdS9YXR01PbKyhBbrwwWYCd19OjRow0b1XawEChcKOQxIWjRhPX5tVqtofoS2PkS5nI5PPfccw1fKK/LYDdfU0Rco7xkMomhoSF85jOfwczMDBYWFupZKysrK02pjMYJhKlr4eN0xfTEE0/gwQcftH2vgwYYPCGHA4W8j7CK3+TkpK9pOF7NlDKZjO3w5YGBARw6dMgx8geAu3fvolQq1T33T3ziE/X7CoUCKpVK0wkklUrh5Zdfrpft37p1iz2sQ8Curw4AfP/738enPvUp2/soxtGAQt4neG1YmkU+nU5DROrjz4wNxsuXL2Nubg6qWm+mND8/j8nJyaaCnYGBAZw4cQIPPvhgU+RvxckysctQSSaTuHHjRkPvFVW1zXAhrcENyPhCIe8T3DYs8/l8g8ibfVKz4Ds1UyqXy66X2Nb77Ko/7VLP7HzU9fV1vPbaa02/n3niEGkPpnbGFwp5n+DUV3xpacl1SLFZ8N02Pd0usa33OVkmdpGf9blTU1OOvyMjx2BwAzK+UMj7AKe+4iKCTCbjmXZmiHVYl95BIj+nnPiBgQFGjiFAzzueUMj7AKe+4qpazwF3mmAPvCfW7QiwU3ZJu5Gf3car4cczciT9CoW8D3AroS+Xy7biaGAWa78CbO6EWK1W6yXe1g1WDiIgJBwo5HtIt3Kf3Uro7Sbs2GWtGOv0EmBrdoyZsAp6ePlPSCMU8j3Cz3i2TpHP5zE7O9s0Ls3wyIHwxNFt4xTYEXO7/intvAcsCmoPvm+9B4V8j+hm285UKoUTJ040DDAWEQwPD4e+Qei1cSoiqFar9awVo8NeOp1GNpv1LSrdPDHGGb5vvQmbZu0R3W7befDgQZw9exbPPvssxsbGMDEx0ZEvr1sjpUQiYds/ZWtrCysrKy1NZPdq5EV2RLtQKGBqagqFQqEeifN96z0Yke8RUaia82uf+J0KZPcYu8wWYwhGNpt1HQXXylVKt0+MUccp8k6lUnzfehAK+R4Rl6o5P6X8r776Ksrlcj0TxvoYt6wSu2IgM35FxSmf3IhC+933dYq8zSMCDViGH38o5HtEXNLm/JTye432cov8c7kcZmdnHV/fr6g4pUzeuXOnoQlX1N7fvcLpikVEMDQ0FPmAgrRGaEIuIgMAZgAsqerHwzpuLxGHtDk3y8IQeTv8RNK1Wg0XLlxoOr55mLRfUTGfGN944w3cvXu3YS391rfcanWl02lbK8/YUI56QEFaI8yI/LMArgF4X4jHJHuMm5fvlZGSTqddj22cCMzFSSKC0dFRX0OkrRgnxqWlpQYhB6Lj+4aR6ud1DDs7LJlMIplMYmNjoynyjkNAQVojFCEXkYMAfg7AfwXwn8M4JukObl5+sVh0LeWfn59HrVZzFKqgQ6SdiMJGsh1hpPr5OUaxWGxoIWxckeRyOQwNDTHy7gPCSj/8YwC/BcAxXBORUyIyIyIzlUolpJclYWNYFuPj4xgbG8P4+HhdNKxzFq1sbGy4prHZpSaGIbhRnf8YRqqfn2MsLi42tWAwWiJwtml/EDgiF5GPA1hV1ZKI/IzT41T1PIDzADAxMdH6FF6yZzhdept96atXr2JjY6Phfi87o1OZO1HdSA4jRdLPMZz66Lz11lvM4OkTwrBW8gB+XkSOAdgH4H0i8mVV/cUQjk0ihlnkS6VSS3ZGq4Lbir8cRd83DMvHzzFExPa5m5ubzODpEwJbK6r6O6p6UFUfA/BpAJcp4r1Pu3aGIbhel/uGN1wqlbC8vNxS1WdUCMPy8XOMbDbraHexcrM/YB45aYtO2xnd7E0TFmG8R07HAFBvPDY6OtqQoWIlKhk8pHOEKuSq+k8A/inMYxJvutXNrpN2Rq+U4IfxHlmP4ZRu+PTTT2NhYaEpFTMKGTykszAijzntprhFvZVpVFMKo4Dd1cra2hpEBKdOnWr4PEQlg4d0FnY/jDntpLjFwX+OakphNzH6yFy9etXWQimXywDgmD5KehdG5DGnHQsiDv6z2Rs28qRFpN7zpd+EyW3ykoGq1v+GUfk7kr2BQh5z2rEguuk/t5pSaM09r1QqfZlO5zV5ySBuewgkHGitxJx2LIhOVVh60Y6lw0EIO3j1uQG4h9DPUMhjjltJvRPd8p/bEeVeyV4JitvkJYB7CP0OrZUeoNUUt26VtLcjyul0GsvLy7a39xN27Q2SySSefPJJVCqVSGYekb2DQt6ndKOkvR0/36n83On2XiWq/WRINKCQkz2jnaZZTp0y+6GDpt3GMLNRiB0UcrJntBNV9mthkJ9Cr6gXdZG9g0JO2qJdEWnV0onL0Oqw8cr1D2NoBekdKOSkZcISET8ng371hr02huNQ1EX2Dgo5aZkwRKSVk0EUe413Gi9LiWmZxAzzyEnLhCEi/VboY/RJmZqaQqFQ8Oxr45Xr362iLhJNGJETT6wWSDqdDrwBGbeIMsjGot+rD+trTE5Oolwu275mv+4dEHso5MQVp97X5kEG7YhInLJRgu4J+LGiWn2Nft07IPZQyIkrdiK0sbGBo0ePYmhoqGURMaLOxcVFiAhEBKoa6Ygy6J6An6uPdl6jH/cOiD0U8j7FzioA0HSbkwhVKhWcPHmy5de0Dj1IJBJIp9PIZrORjSiD2kB+rj7iZjWRaEEhjxHmaNboz+1XAM3CnU6nMT8/X7dGbt++XR9KYL5tdnYWQ0NDTcdq1wKxizoTiQSy2WykI8ugNpAfPztOVhOJHhTymOA0WGB1ddXTr7U+19qEyhgVZr0NAO7du9dwexALJC5Rp/VqJZfLBdpY9ONnc/OSBIFCHhOcBguoqqeX6ncogRf79+/HkSNH2rZA4hB1Om06umWQ+MHLz+bmJQkChTwmuA0W8Ipq/Qwl8EPQzbU4RJ1Om47lcrnj9g83L0m7UMhjgl00a+AV1bo913h+MpkE8J5H3upr+CEOUWdc7B9CzFDIY4I1mjUQEc+o1u9QAgD1zdRqtYrt7e3QUwOjHnXGwf4hxIqo6p6/6MTEhM7MzOz568adsLJW/ETC/doi1S5FcmhoiF0FSSQQkZKqTjTdTiHvT6Im1FFaT5TWQogZCjmpE7WoM2rrISSqOAk5PfI+wRxlbm5uRqqXNXtrExIMCnkf4FRMZMYpM2MvbAZmihASDAp5H+CnIMguM6NTxTFWmClCSDAo5F1krzbVvAqCnNIL7SyPtbU1TE9PQ1VDmxUZh0IhQqJMYCEXkQ8C+B8AHgWgAM6r6ktBj9vruPWfBpq7EIYd8YoIRkdHMTg46PgadicAVcXW1lb953b97FaGKBBC3AkjIt8E8JuqOisiDwIoicgrqvrtEI7dszht8L366quYn58PdTq6U8R7/Phx12N6VYQatOpncwI8IeESeGanqt5S1dndf78D4BoAmpseOG3wLSwshD7L0iiNHx8fx9jYGMbHx32JpnVupBut+Nl2J7H79+/j4sWLnrMs/dLqjExC4kyoHrmIPAbgGQDftLnvFIBTAJDNZsN82VjitMFneM9mwsjgaKc03jgBXL58GXNzc3CqORCRlvxsJ89+dXUV586dazkyt2s7e+HCBUb8pG8IHJEbiMgDAP4KwK+r6vet96vqeVWdUNWJ0dHRsF42tjhNST906FBHp6O3GqmmUikMDw87ijgAHD58uCWBtJsAb9Dq1Ydh05RKJSwvL6NUKmF6ejr0qxpCokwoEbmIJLEj4hdU9athHLPXceoECAA3btzoSAZHu96019XA8PBwS+swPPv79+833dfq1YedTWMH89JJLxNG1ooA+CKAa6r6R8GX1D842R2daPVaq9Vw8eLFBvH0m3GSyWSapgqZqVQqLa3FOIldvHgRq6urDfe1evXht9c689JJLxNGRJ4H8BkAcyJydfe231XVQgjH7kvCbvVqROLtRsD5fB7lcrlpHBzQvkCmUikcP37ctsdKK1cfTqmV5v0G5qWTXiewkKvq/wUgIayFdAjDfnDCa88ilUrhzJkz+PrXv45r167Vbw8qkGEMmnBKrWReOukn2P0wgoRd8Tk1NeVqjQwPD+PMmTO+XiOKLV6juCZCOgHb2MaETrR0LRQKKJVKrqPexsfH27ZzOi2kFGpCdmAb25jQiZauTmPiDIJkdHS6SvPmzZuYnp6utwVgTjghzYSWR07CoRMtXc2VnSMjI033B8nocDvxBKVWqzWIeNjHJ6RXoJBHDLtimbAm2B87dgynTp3Cvn37mgqR2t2w7GQv8WKx2CDiYR+fkF6B1krE6ERL1052GuxkL3E3sWZOOCHvQSGPGGGk5JnptIcd5onHesJJp9O4detWU3uAgYEBX8fnJinpF5i10uPYZawYWSr5fD4UoQtDMO2ydZLJJICd/ivmz2k6ncbjjz/u+joc6Ex6EWat9ClOHvbi4mJDJB0kUg+jEtVu03RjYwNHjx7F0NAQFhcXUa1WoaqoVqt4++23XdfLgc6kn+BmZ49jV7VplK9HqUOg0wmnUqng2LFjyGazDS1+vdbLgc6kn2BEHhPasS9qtRquX7/edHsymYSIRErovDZNWxVmDnQm/QSFPAa0O82+WCxiY2Oj6XiHDx/G8PAwKpVKx4Su1ROP16Zpq8LMgc6kn6CQRxhDDN944w3b9rNe0+ydWrxWq1V88pOf7JjQtZMp45Wt06owh539Q0iUoZBHFKsYWrGzGaybeV5R7BNPPIGFhQWoKkZGRjA4OIhisRhY8NrdaHTbNG1HmMNuB0xIVKGQRxSrGPrB6hk7RbG5XK7pJHHv3j0AO0MiguaZd2qj0SrMxtg6Rtyk32HWSkRxm3yTSCQwMDCAneFMjbebPWNzj4nACB8AAA0YSURBVJWxsTGMj4/j9OnTKJfLrpF+0OyVTrUZMGM3q/PcuXOeM0gJ6UUYkUcUO1sEAPbv348jR440TYp38ozt7AWv8WhG9NzKhqX5saOjoxgcHGwYZpFMJkPdaGzXvmG1J+lFKOQRxckWOXnyZF142t3MczpJGCQSCaTTad8bllY/366sfmNjA++8847tc9v5HdqxbzrdroCQbkFrJaI42SJmwTGi7ZMnT+LYsWO+xSifz2NoaKjJ/gDe64YoIr4LhqzRsV3bh+3tbUxPTzdYH4awzszMYHl5GVeuXMHLL7+Mmzdvev4O7dg3nWy5S0g3YUQeYTqVdWHNAEmn0xARVCqVelR86dIl24j36tWd+dq5XK6ew16r1Xxtym5tbeHixYs4fvw4UqkUisUi1tbWGoR/a2sL09PTOHv2LACElo4IsNqT9C5smtUlou7Vuo2HSyQS2N7ehojYRt9eGDNCL1265DhLNJfL4caNG3WhFpG65ZPNZuuC3cp76NZAjGmKJA5wZmeEiENnPq88djf8CHwul8Pw8DCuXLlie//IyAju37/veCJp5/2Kw/tOiBtOQk6PvAvEwas1e/RGO1kvRkZGMDY2homJCXzqU5/CwMCA42MXFhaQz+dtH5NIJGx7wRi0+3752XcgJI7QI+8CcfBqzdbP+973Prz11luuj08kEnjqqacaLIqzZ8/ic5/7HDY3N5seb6QmnjhxomEupxElHzp0CHNzc65i3s77xWpP0otQyLtA1Dvz+bVVDAvFGAKxvr6OqampBr/6qaeewmuvvdb03M3NTZw7dw6nT5/G2bNnm7xuAJifn3dcQ5TeL0K6DYW8C0S9M5+f9gA/8iM/ggceeKCe9TI/P1+PoM352c899xyuX7+OtbW1pmOYC3jsomQjs8Y8VCKK7xch3YZC3gWi3pnPq/ITAN5++2088MADAIDbt29jY2PDscryzJkzmJqawp07dxqO4WWPmG2QqGf5ENJNKORdIsperZ/Kz2q1itXVVcfsFLNIp1IpHDlyxDb1z6894vR+UeAJYdYKscGr8lNEsLW15ZpiaBVp6zHDsEfYOIuQHZhHTmyxNsFaX1/H4uIiVBWqWm9768YjjzyCwcHBtgt4vGCBD+k3nPLIaa1EjKhYBVZ/up3iICNlcWVlBeVyGWfOnAlVYOOQxknIXhCKkIvIRwG8BGAAwBdU9Q/COG6/0anufH5ODm6PaWfIhZW1tTVcvnwZL774YstrcyLqaZyE7BWBhVxEBgD8CYB/B+AmgCsi8req+u2gx+432u2xbYchkNbUPbuTg9cJxE8Wix8WFhaa1hjkxBV2GmdUroYIaZUwNjt/HMCCqn5HVdcB/CWAF0I4bt8RllVg3gRcWVnB1taWazsAr5YBTi1jc7kcDhw44Htd1olGQVsVhFlyz41TEmfCsFYyAN40/XwTwL+xPkhETgE4BQDZbDaEl+09wrIKvKwQ68nB6wTiFPl++MMfxqVLl3yv69ChQw0/t3LicoqWw0rjDPNqiJC9Zs/SD1X1vKpOqOrE6OjoXr1srAgrRc/LCrGeHLyGNLhFvplMpinStmNgYAC3b99GoVCoR7l+h0PcvHkTL7/8Mq5cuYLl5WXMzMyEHi1z45TEmTAi8iUAHzT9fHD3NtIiYVV8uhX02J0c/HjNTpFvLpdzbEV74MABiAiq1Sq2t7exsrKCSqVS98H9vG6tVmtoqgXsTCBaW1trO1q2i+65cUriTBhCfgXAD4vI49gR8E8DOB7CcfsSq2DWajUUCoWWhN0qkHZDGawj49o9gZTLZcf7BgcHkclkUKlU6sVDVsvC63WLxWKDiBuoalvRstMG6+TkZKT73xDiRmAhV9VNEflVAF/HTvrhl1T1jcAr61PM0aLRjMroY+I3q6MdYW7Xa3YT00wm42lZeL2u1/FbxckLL5fLke5/Q4gboeSRq2oBQCGMY/Uz1mjROgatlQ24verlkslkcOvWraZy/YGBAeTzeRSLxUCWhZNNZBy/VdxOLFHuf0OIG+y1EiH8FN5EbQMun89jeHi4YcNzYGAAJ06cQCqVCryBa9f3xXz8VvG7wWrFsLimpqYaNmwJiQIs0Y8QfgpvorYB52XjBN3ADbvlbztFRJ2quCUkLNg0K0K4Ta4H2h86TBpptYKTzblIVGDTrBhgFy0mk0k8+eSTqFQq3IALiVa9cOaYk6hDIY8QUZ8c1K8wx5xEHQp5xGDmRPSI+oxVQijkpA67/9nDKyUSdbjZSQA0Z2ZwY5WQ6MHNTuJKlLv/8UqBEHco5ARAdDMzmMNNiDes7CQAolvxGHT4BCH9ACNyAiC6FY9RvVIgJEpQyAmA9jIz2vHVW/W7mcNNiDcUclKn0xWP7UTwzOEmxBt65KRt7Hx14D3f3OqXt+N3hzlgmZBehRE5aRtrtGxw584dlEqlpmi7Xb+b1a6EuMOInLSNOVoeGRlpuM8u2m43M6absA85iQMUchIII1p+//vf33SfNdoOOmRirzE8/VKphOXlZZRKJZw7d45iTiIHhZyEgp9oO25+N3PYSVygR05CwW92SZz8buawk7hAISeh0IsdApnDTuIChZyERpyibT8wh53EBQo5IQ704lUG6U0o5CQSRLVVba9dZZDehEJOug5b1RISDKYfkq7DND9CgkEhJ12HaX6EBIPWCgkdtqolZG+hkJNQYataQvYeWiskVNiqlpC9hxE5CRW2qiVk7wkUkYvIfxOR6yJSFpGviUhzCzzSV8SxVS0hcSeotfIKgKdVNQfgBoDfCb4kEmfi1qqWkF4gkLWiqt8w/fjPAH4h2HJI3GFZOyF7T5ge+a8A+J8hHo/EFPrdhOwtnkIuIv8A4AM2d/2eqv7N7mN+D8AmgAsuxzkF4BQAZLPZthZLCCGkGU8hV9WfdbtfRE4A+DiAj6iquhznPIDzADAxMeH4OEIIIa0RyFoRkY8C+C0A/1ZV74azJEIIIa0QNGvlcwAeBPCKiFwVkXMhrIkQQkgLBM1aORTWQgghhLSHuNjanXtRkQqAfw3hUGkA1RCO0ym4vmBwfcHg+oITtTX+kKqOWm/sipCHhYjMqOpEt9fhBNcXDK4vGFxfcOKwRoBNswghJPZQyAkhJObEXcjPd3sBHnB9weD6gsH1BScOa4y3R04IIST+ETkhhPQ9FHJCCIk5PSPkIvKbIqIiku72WsxEdfiGiHxUROZFZEFEfrvb6zEjIh8UkVdF5Nsi8oaIfLbba7JDRAZE5Fsi8r+6vRYrIvJ+EfnK7mfvmoj8ZLfXZEZEfmP3b/u6iPyFiOzr8nq+JCKrIvK66baHReQVEfl/u/9/qJtrdKMnhFxEPgjg3wNY7PZabIjc8A0RGQDwJwA+BuAIgP8gIke6u6oGNgH8pqoeAfATAP5TxNZn8FkA17q9CAdeAvD3qvokgA8hQusUkQyAXwMwoapPAxgA8OnurgrTAD5que23Afyjqv4wgH/c/TmS9ISQA/jv2GneFbmdW1X9hqpu7v74zwAOdnM9u/w4gAVV/Y6qrgP4SwAvdHlNdVT1lqrO7v77HeyIUKRmxYnIQQA/B+AL3V6LFRFJAfhpAF8EAFVdV9XvdXdVTQwC+AERGQQwAmC5m4tR1f8D4G3LzS8A+PPdf/85gE/s6aJaIPZCLiIvAFhS1de6vRYf/AqA/93tRWBHFN80/XwTERNKAxF5DMAzAL7Z3ZU08cfYCR62vR7YBR4HUAHwZ7vWzxdEZH+3F2WgqksA/hA7V9C3ANQs08aiwqOqemv337cBPNrNxbgRCyEXkX/Y9dKs/70A4HcB/JcIr894jOfwDdKIiDwA4K8A/Lqqfr/b6zEQkY8DWFXVUrfX4sAggB8D8HlVfQbAHUTIFtj1ml/AzglnDMB+EfnF7q7Knd1ZC5G74jcIc9Rbx3AabiEiR7HzYXhNRIAd22JWRH5cVW93e30Gfodv7CFLAD5o+vng7m2RQUSS2BHxC6r61W6vx0IewM+LyDEA+wC8T0S+rKpREaObAG6qqnEV8xVESMgB/CyAf1HVCgCIyFcB/BSAL3d1Vc2siMgPquotEflBAKvdXpATsYjInVDVOVU9oKqPqepj2PkA/9heirgXpuEbPx+h4RtXAPywiDwuIkPY2Wj62y6vqY7snJW/COCaqv5Rt9djRVV/R1UP7n7mPg3gcoREHLuf/zdF5PDuTR8B8O0uLsnKIoCfEJGR3b/1RxChzVgTfwvgl3b//UsA/qaLa3ElFhF5zPkcgGHsDN8AgH9W1dPdXJCqborIrwL4OnYyBr6kqm90c00W8gA+A2BORK7u3va7qlro4prixlkAF3ZP1N8B8MtdXk8dVf2miHwFwCx27MZvocul8CLyFwB+BkBaRG4C+H0AfwDgkoj8R+y03f5k91boDkv0CSEk5sTaWiGEEEIhJ4SQ2EMhJ4SQmEMhJ4SQmEMhJ4SQmEMhJ4SQmEMhJ4SQmPP/AchEprdffh/oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmgeHTCfBEDp",
        "colab_type": "text"
      },
      "source": [
        "### Reminder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgAwWi1nBEDq",
        "colab_type": "text"
      },
      "source": [
        "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{p(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
        "\n",
        "<b>E-step</b>:<br>\n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
        "<b>M-step</b>:<br> \n",
        "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
        "\n",
        "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
        "\n",
        "Latent variables $T$ are indices of components to which each data point is assigned, i.e. $t_i$  is the cluster index for object $x_i$.\n",
        "\n",
        "The joint distribution can be written as follows: $\\log p(T, X \\mid \\theta) =  \\sum\\limits_{i=1}^N \\log p(t_i, x_i \\mid \\theta) = \\sum\\limits_{i=1}^N \\sum\\limits_{c=1}^C q(t_i = c) \\log \\left (\\pi_c \\, f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)\\right)$,\n",
        "where $f_{\\!\\mathcal{N}}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma_c|}}\n",
        "\\exp\\left(-\\frac{1}{2}({x}-{\\mu_c})^T{\\boldsymbol\\Sigma_c}^{-1}({x}-{\\mu_c})\n",
        "\\right)$ is the probability density function (pdf) of the normal distribution $\\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmi1nAtBEDr",
        "colab_type": "text"
      },
      "source": [
        "### E-step\n",
        "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q_i(t_i) = p(t_i \\mid x_i, \\theta)$. We assume that $t_i$ equals to the cluster index of the true component of the $x_i$ object. To do so we need to compute $\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FR1RJ9BEDs",
        "colab_type": "text"
      },
      "source": [
        "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{y_i}}{\\sum_j e^{y_j}}$, which is called _softmax_. When you compute exponents of large numbers, some numbers may become infinity. You can avoid this by dividing numerator and denominator by $e^{\\max(y)}$: $\\frac{e^{y_i-\\max(y)}}{\\sum_j e^{y_j - \\max(y)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. So, to compute desired formula you first subtract maximum value from each component in vector $\\mathbf{y}$ and then compute everything else as before.\n",
        "\n",
        "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to directly solve equation $Ay = x$ by using a special function. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by methods which do not explicitely invert the matrix. You can use ```np.linalg.solve``` for this.\n",
        "\n",
        "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL87Q8-TBEDu",
        "colab_type": "text"
      },
      "source": [
        "<b>Task 1:</b> Implement E-step for GMM using template below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCUCaD28BEDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def E_step(X, pi, mu, sigma):\n",
        "    \"\"\"\n",
        "    Performs E-step on GMM model\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    pi: (C), mixture component weights \n",
        "    mu: (C x d), mixture component means\n",
        "    sigma: (C x d x d), mixture component covariance matrices\n",
        "    \n",
        "    Returns:\n",
        "    gamma: (N x C), probabilities of clusters for objects\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = pi.shape[0] # number of clusters\n",
        "    d = mu.shape[1] # dimension of each object\n",
        "    gamma = np.zeros((N, C)) # distribution q(T)\n",
        "    eps = 1e-12\n",
        "    C_pdf =  np.array([scipy.stats.multivariate_normal(mean=mu[i], cov=sigma[i]).pdf(X) for i in range(C)])\n",
        "    gamma = [pi] * C_pdf.T  + eps\n",
        "    gamma_normalized = gamma/gamma.sum(axis=1)[:, np.newaxis]\n",
        "    return gamma_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlhktOlMBED1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "69939a54-cdb6-4a72-cc4b-f3888daeb1cd"
      },
      "source": [
        "gamma = E_step(X, pi0, mu0, sigma0)\n",
        "grader.submit_e_step(gamma)\n",
        "gamma.sum(axis=0), gamma.sum(axis=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current answer for task Task 1 (E-step) is: 0.5337178740752773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([125.16817874, 154.20952273,   0.62229854]),\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fERrQWSCBED5",
        "colab_type": "text"
      },
      "source": [
        "### M-step\n",
        "\n",
        "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
        "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{i=1}^{n}\\pi_i = 1$.\n",
        "\n",
        "<br>\n",
        "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A31OYSogBED6",
        "colab_type": "text"
      },
      "source": [
        "<b>Task 2:</b> Implement M-step for GMM using template below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhOr5I1bBED7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def M_step(X, gamma):\n",
        "    \"\"\"\n",
        "    Performs M-step on GMM model\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    gamma: (N x C), distribution q(T)  \n",
        "    \n",
        "    Returns:\n",
        "    pi: (C)\n",
        "    mu: (C x d)\n",
        "    sigma: (C x d x d)\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = gamma.shape[1] # number of clusters\n",
        "    d = X.shape[1] # dimension of each object\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    #gamma_sum = gamma.sum(axis=0)\n",
        "    pi = gamma.mean(axis=0)\n",
        "    X_repeated = X[:,np.newaxis,:].repeat(C,axis=1)\n",
        "    gamma_repeated = gamma[:,:,np.newaxis].repeat(d, axis=2)\n",
        "    X_gamma = X_repeated * gamma_repeated\n",
        "    mu = X_gamma.sum(axis=0)/gamma.sum(axis=0)[:,np.newaxis]\n",
        "    \n",
        "    sigma = np.zeros((C,d,d))\n",
        "    for c in range(C):\n",
        "      x = X - mu[c, :] # (N x d)\n",
        "      gamma_diag = np.diag(gamma[:,c])\n",
        "\n",
        "      gamma_diag = np.matrix(gamma_diag)\n",
        "\n",
        "      sigma_c = x.T * gamma_diag * x\n",
        "      sigma[c,:,:]=(sigma_c) / np.sum(gamma, axis = 0)[:,np.newaxis][c]\n",
        "\n",
        "    return pi, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DeNkiseXX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4b56c8b6-c0b8-4fa1-ab53-559ad1e07e9c"
      },
      "source": [
        "gamma = E_step(X, pi0, mu0, sigma0)\n",
        "pi, mu, sigma = M_step(X, gamma)\n",
        "grader.submit_m_step(pi, mu, sigma)\n",
        "pi.shape, mu.shape, sigma.shape\n",
        "pi, mu, sigma"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current answer for task Task 2 (M-step: mu) is: 2.8992928485941185\n",
            "Current answer for task Task 2 (M-step: sigma) is: 5.976867951707076\n",
            "Current answer for task Task 2 (M-step: pi) is: 0.5507482954512751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.44702921, 0.5507483 , 0.00222249]),\n",
              " array([[ 1.05862244,  5.40760947],\n",
              "        [ 2.16783712,  2.89929285],\n",
              "        [-1.30554036,  1.435423  ]]),\n",
              " array([[[ 0.70702548,  1.0019933 ],\n",
              "         [ 1.0019933 ,  3.09523429]],\n",
              " \n",
              "        [[ 5.76281975,  1.49008134],\n",
              "         [ 1.49008134,  5.97686795]],\n",
              " \n",
              "        [[ 0.32226667, -0.0674601 ],\n",
              "         [-0.0674601 ,  3.39952859]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzq1C9uXBamK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "03d83978-c9e2-447e-c16b-d07a80d7f996"
      },
      "source": [
        "gamma = E_step(X, pi0, mu0, sigma0)\n",
        "pi, mu, sigma = M_step(X, gamma)\n",
        "grader.submit_m_step(pi, mu, sigma)\n",
        "pi.shape, mu.shape, sigma.shape\n",
        "pi, mu, sigma"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current answer for task Task 2 (M-step: mu) is: 2.8992928485941185\n",
            "Current answer for task Task 2 (M-step: sigma) is: 5.976867951707076\n",
            "Current answer for task Task 2 (M-step: pi) is: 0.5507482954512751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.44702921, 0.5507483 , 0.00222249]),\n",
              " array([[ 1.05862244,  5.40760947],\n",
              "        [ 2.16783712,  2.89929285],\n",
              "        [-1.30554036,  1.435423  ]]),\n",
              " array([[[ 0.70702548,  1.0019933 ],\n",
              "         [ 1.0019933 ,  3.09523429]],\n",
              " \n",
              "        [[ 5.76281975,  1.49008134],\n",
              "         [ 1.49008134,  5.97686795]],\n",
              " \n",
              "        [[ 0.32226667, -0.0674601 ],\n",
              "         [-0.0674601 ,  3.39952859]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svyzjt7XBEEC",
        "colab_type": "text"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qexOXBLUBEED",
        "colab_type": "text"
      },
      "source": [
        "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
        "\n",
        "<b>Task 3:</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
        "\n",
        "$$\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{c=1}^{C} q(t_i =c) (\\log \\pi_c + \\log f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)) - \\sum_{i=1}^{N} \\sum_{c=1}^{K} q(t_i =c) \\log q(t_i =c)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5tKCZe0BEEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_vlb(X, pi, mu, sigma, gamma):\n",
        "    \"\"\"\n",
        "    Each input is numpy array:\n",
        "    X: (N x d), data points\n",
        "    gamma: (N x C), distribution q(T)  \n",
        "    pi: (C)\n",
        "    mu: (C x d)\n",
        "    sigma: (C x d x d)\n",
        "    \n",
        "    Returns value of variational lower bound\n",
        "    \"\"\"\n",
        "    N = X.shape[0] # number of objects\n",
        "    C = gamma.shape[1] # number of clusters\n",
        "    d = X.shape[1] # dimension of each object\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    logpdfs = np.array([scipy.stats.multivariate_normal(mean=mu[i], cov=sigma[i]).logpdf(X) for i in range(C)]).T\n",
        "    \n",
        "    eps = 1e-12\n",
        "    log_gamma = np.log(gamma+eps)\n",
        "    #pdfs = np.where(pdfs==0, eps, pdfs)\n",
        "    log_gamma = np.where(log_gamma==-np.Infinity, eps, log_gamma)           \n",
        "    loss = np.sum(gamma * (np.log([pi + eps]) + logpdfs - log_gamma))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNva3XRTBEEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c16a2542-8701-4c9f-fcb8-0d23a2d2dade"
      },
      "source": [
        "pi, mu, sigma = pi0, mu0, sigma0\n",
        "gamma = E_step(X, pi, mu, sigma)\n",
        "pi, mu, sigma = M_step(X, gamma)\n",
        "loss = compute_vlb(X, pi, mu, sigma, gamma)\n",
        "grader.submit_VLB(loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss1  -1214.6031220946288\n",
            "Current answer for task Task 3 (VLB) is: -1214.6031220946288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5p8XC-eBEEM",
        "colab_type": "text"
      },
      "source": [
        "### Bringing it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQsdsNVCBEEU",
        "colab_type": "text"
      },
      "source": [
        "Now that we have E step, M step and VLB, we can implement the training loop. We will initialize values of $\\pi$, $\\mu$ and $\\Sigma$ to some random numbers, train until $\\mathcal{L}$ stops changing, and return the resulting points. We also know that the EM algorithm converges to local optima. To find a better local optima, we will restart the algorithm multiple times from different (random) starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
        "\n",
        "Remember, that initial (random) values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
        "\n",
        "You will also sometimes get numerical errors because of component collapsing. The easiest way to deal with this problems is to restart the procedure.\n",
        "\n",
        "<b>Task 4:</b> Implement training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1AAcyl0BEEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_EM(X, C, rtol=1e-6, max_iter=100, restarts=20):\n",
        "    '''\n",
        "    Starts with random initialization *restarts* times\n",
        "    Runs optimization until saturation with *rtol* reached\n",
        "    or *max_iter* iterations were made.\n",
        "    \n",
        "    X: (N, d), data points\n",
        "    C: int, number of clusters\n",
        "    '''\n",
        "    N = X.shape[0] # number of objects\n",
        "    d = X.shape[1] # dimension of each object\n",
        "    best_loss = None\n",
        "    best_pi = None\n",
        "    best_mu = None\n",
        "    best_sigma = None\n",
        "\n",
        "    for _ in range(restarts):\n",
        "        try:\n",
        "            ### YOUR CODE HERE\n",
        "            pi = np.random.randint(0, 10, C)/10\n",
        "            pi = pi/pi.sum()\n",
        "            mu = np.random.rand(C,d)\n",
        "            sigma = np.zeros((C,d,d))\n",
        "            \n",
        "            for i in range(C):\n",
        "              sigma[i] = np.eye(d)\n",
        "            epoch_losses = []\n",
        "            print('pi is',pi)\n",
        "            for i in range(max_iter):\n",
        "              gamma = E_step(X, pi, mu, sigma)\n",
        "              pi, mu, sigma = M_step(X, gamma)\n",
        "              vlb = compute_vlb(X, pi, mu, sigma, gamma)\n",
        "              epoch_losses += [vlb]\n",
        "              print('_ ==> ',_,' i ==>', i, 'Loss.    ',vlb)\n",
        "              if not best_loss or vlb >= best_loss: \n",
        "                best_loss, best_pi, best_mu, best_sigma  = vlb, pi, mu, sigma\n",
        "              \n",
        "              if len(epoch_losses) > 1 and abs((vlb-epoch_losses[i-1]) / epoch_losses[i-1]) <= rtol:\n",
        "                print('stagnant loss')\n",
        "                break\n",
        "          \n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Singular matrix: components collapsed\")\n",
        "            pass\n",
        "\n",
        "    return best_loss, best_pi, best_mu, best_sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f8A5sbmBEEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ac7d3a3-1950-47ef-d6c3-3a8dcce860eb"
      },
      "source": [
        "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)\n",
        "grader.submit_EM(best_loss)\n",
        "print(best_loss, best_pi, best_mu, best_sigma)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pi is [0.39130435 0.30434783 0.30434783]\n",
            "loss1  -1246.071995219273\n",
            "_ ==>  0  i ==> 0 Loss.     -1246.071995219273\n",
            "loss1  -1228.0266518935393\n",
            "_ ==>  0  i ==> 1 Loss.     -1228.0266518935393\n",
            "loss1  -1199.0517646703006\n",
            "_ ==>  0  i ==> 2 Loss.     -1199.0517646703006\n",
            "loss1  -1179.0030090326736\n",
            "_ ==>  0  i ==> 3 Loss.     -1179.0030090326736\n",
            "loss1  -1172.6622358518546\n",
            "_ ==>  0  i ==> 4 Loss.     -1172.6622358518546\n",
            "loss1  -1169.8875154633997\n",
            "_ ==>  0  i ==> 5 Loss.     -1169.8875154633997\n",
            "loss1  -1167.418157890877\n",
            "_ ==>  0  i ==> 6 Loss.     -1167.418157890877\n",
            "loss1  -1164.282895668301\n",
            "_ ==>  0  i ==> 7 Loss.     -1164.282895668301\n",
            "loss1  -1159.6468231693334\n",
            "_ ==>  0  i ==> 8 Loss.     -1159.6468231693334\n",
            "loss1  -1152.113755795731\n",
            "_ ==>  0  i ==> 9 Loss.     -1152.113755795731\n",
            "loss1  -1139.6888287227293\n",
            "_ ==>  0  i ==> 10 Loss.     -1139.6888287227293\n",
            "loss1  -1122.3264917397833\n",
            "_ ==>  0  i ==> 11 Loss.     -1122.3264917397833\n",
            "loss1  -1105.3910548864396\n",
            "_ ==>  0  i ==> 12 Loss.     -1105.3910548864396\n",
            "loss1  -1093.1639973132621\n",
            "_ ==>  0  i ==> 13 Loss.     -1093.1639973132621\n",
            "loss1  -1084.467969912671\n",
            "_ ==>  0  i ==> 14 Loss.     -1084.467969912671\n",
            "loss1  -1078.919601903508\n",
            "_ ==>  0  i ==> 15 Loss.     -1078.919601903508\n",
            "loss1  -1075.2972185963654\n",
            "_ ==>  0  i ==> 16 Loss.     -1075.2972185963654\n",
            "loss1  -1072.4022682154884\n",
            "_ ==>  0  i ==> 17 Loss.     -1072.4022682154884\n",
            "loss1  -1069.9632497835896\n",
            "_ ==>  0  i ==> 18 Loss.     -1069.9632497835896\n",
            "loss1  -1067.9372460915756\n",
            "_ ==>  0  i ==> 19 Loss.     -1067.9372460915756\n",
            "loss1  -1066.3241450357427\n",
            "_ ==>  0  i ==> 20 Loss.     -1066.3241450357427\n",
            "loss1  -1065.122517102083\n",
            "_ ==>  0  i ==> 21 Loss.     -1065.122517102083\n",
            "loss1  -1064.3594972541782\n",
            "_ ==>  0  i ==> 22 Loss.     -1064.3594972541782\n",
            "loss1  -1063.9935197161249\n",
            "_ ==>  0  i ==> 23 Loss.     -1063.9935197161249\n",
            "loss1  -1063.8642362609844\n",
            "_ ==>  0  i ==> 24 Loss.     -1063.8642362609844\n",
            "loss1  -1063.8266747053517\n",
            "_ ==>  0  i ==> 25 Loss.     -1063.8266747053517\n",
            "loss1  -1063.8161842356099\n",
            "_ ==>  0  i ==> 26 Loss.     -1063.8161842356099\n",
            "loss1  -1063.8130411009915\n",
            "_ ==>  0  i ==> 27 Loss.     -1063.8130411009915\n",
            "loss1  -1063.8119783754548\n",
            "_ ==>  0  i ==> 28 Loss.     -1063.8119783754548\n",
            "stagnant loss\n",
            "pi is [0.2 0.4 0.4]\n",
            "loss1  -1232.7062971256137\n",
            "_ ==>  1  i ==> 0 Loss.     -1232.7062971256137\n",
            "loss1  -1203.3488161123007\n",
            "_ ==>  1  i ==> 1 Loss.     -1203.3488161123007\n",
            "loss1  -1169.9890059958423\n",
            "_ ==>  1  i ==> 2 Loss.     -1169.9890059958423\n",
            "loss1  -1150.2226405114552\n",
            "_ ==>  1  i ==> 3 Loss.     -1150.2226405114552\n",
            "loss1  -1133.4239106935693\n",
            "_ ==>  1  i ==> 4 Loss.     -1133.4239106935693\n",
            "loss1  -1115.2545708762527\n",
            "_ ==>  1  i ==> 5 Loss.     -1115.2545708762527\n",
            "loss1  -1096.8264936563758\n",
            "_ ==>  1  i ==> 6 Loss.     -1096.8264936563758\n",
            "loss1  -1083.4795624962132\n",
            "_ ==>  1  i ==> 7 Loss.     -1083.4795624962132\n",
            "loss1  -1075.8477762550742\n",
            "_ ==>  1  i ==> 8 Loss.     -1075.8477762550742\n",
            "loss1  -1071.7915838143733\n",
            "_ ==>  1  i ==> 9 Loss.     -1071.7915838143733\n",
            "loss1  -1069.236390336342\n",
            "_ ==>  1  i ==> 10 Loss.     -1069.236390336342\n",
            "loss1  -1067.5607730922884\n",
            "_ ==>  1  i ==> 11 Loss.     -1067.5607730922884\n",
            "loss1  -1066.3720891117732\n",
            "_ ==>  1  i ==> 12 Loss.     -1066.3720891117732\n",
            "loss1  -1065.4229892650965\n",
            "_ ==>  1  i ==> 13 Loss.     -1065.4229892650965\n",
            "loss1  -1064.6711682492494\n",
            "_ ==>  1  i ==> 14 Loss.     -1064.6711682492494\n",
            "loss1  -1064.1733397681946\n",
            "_ ==>  1  i ==> 15 Loss.     -1064.1733397681946\n",
            "loss1  -1063.9300711778073\n",
            "_ ==>  1  i ==> 16 Loss.     -1063.9300711778073\n",
            "loss1  -1063.8438936451066\n",
            "_ ==>  1  i ==> 17 Loss.     -1063.8438936451066\n",
            "loss1  -1063.8196050630356\n",
            "_ ==>  1  i ==> 18 Loss.     -1063.8196050630356\n",
            "loss1  -1063.8134195030268\n",
            "_ ==>  1  i ==> 19 Loss.     -1063.8134195030268\n",
            "loss1  -1063.8118581410208\n",
            "_ ==>  1  i ==> 20 Loss.     -1063.8118581410208\n",
            "loss1  -1063.8114439062392\n",
            "_ ==>  1  i ==> 21 Loss.     -1063.8114439062392\n",
            "stagnant loss\n",
            "pi is [0.66666667 0.16666667 0.16666667]\n",
            "loss1  -1239.9208641031003\n",
            "_ ==>  2  i ==> 0 Loss.     -1239.9208641031003\n",
            "loss1  -1193.1449806331661\n",
            "_ ==>  2  i ==> 1 Loss.     -1193.1449806331661\n",
            "loss1  -1160.687307619688\n",
            "_ ==>  2  i ==> 2 Loss.     -1160.687307619688\n",
            "loss1  -1151.2116078996753\n",
            "_ ==>  2  i ==> 3 Loss.     -1151.2116078996753\n",
            "loss1  -1146.393725224806\n",
            "_ ==>  2  i ==> 4 Loss.     -1146.393725224806\n",
            "loss1  -1142.3026671092953\n",
            "_ ==>  2  i ==> 5 Loss.     -1142.3026671092953\n",
            "loss1  -1138.6945116782726\n",
            "_ ==>  2  i ==> 6 Loss.     -1138.6945116782726\n",
            "loss1  -1135.7567361872518\n",
            "_ ==>  2  i ==> 7 Loss.     -1135.7567361872518\n",
            "loss1  -1134.1077326393024\n",
            "_ ==>  2  i ==> 8 Loss.     -1134.1077326393024\n",
            "loss1  -1133.4024945581996\n",
            "_ ==>  2  i ==> 9 Loss.     -1133.4024945581996\n",
            "loss1  -1133.040627227093\n",
            "_ ==>  2  i ==> 10 Loss.     -1133.040627227093\n",
            "loss1  -1132.7859716229234\n",
            "_ ==>  2  i ==> 11 Loss.     -1132.7859716229234\n",
            "loss1  -1132.5411582556133\n",
            "_ ==>  2  i ==> 12 Loss.     -1132.5411582556133\n",
            "loss1  -1132.2334227458443\n",
            "_ ==>  2  i ==> 13 Loss.     -1132.2334227458443\n",
            "loss1  -1131.7611573726826\n",
            "_ ==>  2  i ==> 14 Loss.     -1131.7611573726826\n",
            "loss1  -1130.9518395814896\n",
            "_ ==>  2  i ==> 15 Loss.     -1130.9518395814896\n",
            "loss1  -1129.6121357367326\n",
            "_ ==>  2  i ==> 16 Loss.     -1129.6121357367326\n",
            "loss1  -1127.9462770017378\n",
            "_ ==>  2  i ==> 17 Loss.     -1127.9462770017378\n",
            "loss1  -1126.7875009867385\n",
            "_ ==>  2  i ==> 18 Loss.     -1126.7875009867385\n",
            "loss1  -1126.2826066632836\n",
            "_ ==>  2  i ==> 19 Loss.     -1126.2826066632836\n",
            "loss1  -1125.9506527739115\n",
            "_ ==>  2  i ==> 20 Loss.     -1125.9506527739115\n",
            "loss1  -1125.6816951564779\n",
            "_ ==>  2  i ==> 21 Loss.     -1125.6816951564779\n",
            "loss1  -1125.4936338191787\n",
            "_ ==>  2  i ==> 22 Loss.     -1125.4936338191787\n",
            "loss1  -1125.374814816456\n",
            "_ ==>  2  i ==> 23 Loss.     -1125.374814816456\n",
            "loss1  -1125.3019043526383\n",
            "_ ==>  2  i ==> 24 Loss.     -1125.3019043526383\n",
            "loss1  -1125.2563387873654\n",
            "_ ==>  2  i ==> 25 Loss.     -1125.2563387873654\n",
            "loss1  -1125.2264242163928\n",
            "_ ==>  2  i ==> 26 Loss.     -1125.2264242163928\n",
            "loss1  -1125.2054538096604\n",
            "_ ==>  2  i ==> 27 Loss.     -1125.2054538096604\n",
            "loss1  -1125.189682439051\n",
            "_ ==>  2  i ==> 28 Loss.     -1125.189682439051\n",
            "loss1  -1125.1770298516944\n",
            "_ ==>  2  i ==> 29 Loss.     -1125.1770298516944\n",
            "loss1  -1125.166343077453\n",
            "_ ==>  2  i ==> 30 Loss.     -1125.166343077453\n",
            "loss1  -1125.1569814424697\n",
            "_ ==>  2  i ==> 31 Loss.     -1125.1569814424697\n",
            "loss1  -1125.148583294313\n",
            "_ ==>  2  i ==> 32 Loss.     -1125.148583294313\n",
            "loss1  -1125.140936316755\n",
            "_ ==>  2  i ==> 33 Loss.     -1125.140936316755\n",
            "loss1  -1125.1339067650179\n",
            "_ ==>  2  i ==> 34 Loss.     -1125.1339067650179\n",
            "loss1  -1125.1274017636242\n",
            "_ ==>  2  i ==> 35 Loss.     -1125.1274017636242\n",
            "loss1  -1125.1213498071743\n",
            "_ ==>  2  i ==> 36 Loss.     -1125.1213498071743\n",
            "loss1  -1125.1156910409838\n",
            "_ ==>  2  i ==> 37 Loss.     -1125.1156910409838\n",
            "loss1  -1125.1103726222125\n",
            "_ ==>  2  i ==> 38 Loss.     -1125.1103726222125\n",
            "loss1  -1125.1053465904974\n",
            "_ ==>  2  i ==> 39 Loss.     -1125.1053465904974\n",
            "loss1  -1125.1005688793146\n",
            "_ ==>  2  i ==> 40 Loss.     -1125.1005688793146\n",
            "loss1  -1125.0959987681194\n",
            "_ ==>  2  i ==> 41 Loss.     -1125.0959987681194\n",
            "loss1  -1125.091598438799\n",
            "_ ==>  2  i ==> 42 Loss.     -1125.091598438799\n",
            "loss1  -1125.0873324896854\n",
            "_ ==>  2  i ==> 43 Loss.     -1125.0873324896854\n",
            "loss1  -1125.0831673521718\n",
            "_ ==>  2  i ==> 44 Loss.     -1125.0831673521718\n",
            "loss1  -1125.079070592331\n",
            "_ ==>  2  i ==> 45 Loss.     -1125.079070592331\n",
            "loss1  -1125.0750100872172\n",
            "_ ==>  2  i ==> 46 Loss.     -1125.0750100872172\n",
            "loss1  -1125.070953055878\n",
            "_ ==>  2  i ==> 47 Loss.     -1125.070953055878\n",
            "loss1  -1125.0668649044608\n",
            "_ ==>  2  i ==> 48 Loss.     -1125.0668649044608\n",
            "loss1  -1125.0627078144078\n",
            "_ ==>  2  i ==> 49 Loss.     -1125.0627078144078\n",
            "loss1  -1125.058438959858\n",
            "_ ==>  2  i ==> 50 Loss.     -1125.058438959858\n",
            "loss1  -1125.0540081784006\n",
            "_ ==>  2  i ==> 51 Loss.     -1125.0540081784006\n",
            "loss1  -1125.0493548259954\n",
            "_ ==>  2  i ==> 52 Loss.     -1125.0493548259954\n",
            "loss1  -1125.044403401769\n",
            "_ ==>  2  i ==> 53 Loss.     -1125.044403401769\n",
            "loss1  -1125.0390572969732\n",
            "_ ==>  2  i ==> 54 Loss.     -1125.0390572969732\n",
            "loss1  -1125.0331896467305\n",
            "_ ==>  2  i ==> 55 Loss.     -1125.0331896467305\n",
            "loss1  -1125.0266296463408\n",
            "_ ==>  2  i ==> 56 Loss.     -1125.0266296463408\n",
            "loss1  -1125.019141677378\n",
            "_ ==>  2  i ==> 57 Loss.     -1125.019141677378\n",
            "loss1  -1125.010392930193\n",
            "_ ==>  2  i ==> 58 Loss.     -1125.010392930193\n",
            "loss1  -1124.999902602418\n",
            "_ ==>  2  i ==> 59 Loss.     -1124.999902602418\n",
            "loss1  -1124.9869620607901\n",
            "_ ==>  2  i ==> 60 Loss.     -1124.9869620607901\n",
            "loss1  -1124.9705116493888\n",
            "_ ==>  2  i ==> 61 Loss.     -1124.9705116493888\n",
            "loss1  -1124.948962055409\n",
            "_ ==>  2  i ==> 62 Loss.     -1124.948962055409\n",
            "loss1  -1124.9199777835477\n",
            "_ ==>  2  i ==> 63 Loss.     -1124.9199777835477\n",
            "loss1  -1124.8803563691836\n",
            "_ ==>  2  i ==> 64 Loss.     -1124.8803563691836\n",
            "loss1  -1124.8264373128786\n",
            "_ ==>  2  i ==> 65 Loss.     -1124.8264373128786\n",
            "loss1  -1124.7558657997463\n",
            "_ ==>  2  i ==> 66 Loss.     -1124.7558657997463\n",
            "loss1  -1124.6709456385731\n",
            "_ ==>  2  i ==> 67 Loss.     -1124.6709456385731\n",
            "loss1  -1124.580668892047\n",
            "_ ==>  2  i ==> 68 Loss.     -1124.580668892047\n",
            "loss1  -1124.4969606793825\n",
            "_ ==>  2  i ==> 69 Loss.     -1124.4969606793825\n",
            "loss1  -1124.4280722362917\n",
            "_ ==>  2  i ==> 70 Loss.     -1124.4280722362917\n",
            "loss1  -1124.3764377904113\n",
            "_ ==>  2  i ==> 71 Loss.     -1124.3764377904113\n",
            "loss1  -1124.3405776591687\n",
            "_ ==>  2  i ==> 72 Loss.     -1124.3405776591687\n",
            "loss1  -1124.3172478608185\n",
            "_ ==>  2  i ==> 73 Loss.     -1124.3172478608185\n",
            "loss1  -1124.3028568398959\n",
            "_ ==>  2  i ==> 74 Loss.     -1124.3028568398959\n",
            "loss1  -1124.2943119851361\n",
            "_ ==>  2  i ==> 75 Loss.     -1124.2943119851361\n",
            "loss1  -1124.2893511309555\n",
            "_ ==>  2  i ==> 76 Loss.     -1124.2893511309555\n",
            "loss1  -1124.286496900695\n",
            "_ ==>  2  i ==> 77 Loss.     -1124.286496900695\n",
            "loss1  -1124.284853427364\n",
            "_ ==>  2  i ==> 78 Loss.     -1124.284853427364\n",
            "loss1  -1124.2839005102974\n",
            "_ ==>  2  i ==> 79 Loss.     -1124.2839005102974\n",
            "stagnant loss\n",
            "pi is [0.69230769 0.15384615 0.15384615]\n",
            "loss1  -1243.2211716436336\n",
            "_ ==>  3  i ==> 0 Loss.     -1243.2211716436336\n",
            "loss1  -1218.235701490103\n",
            "_ ==>  3  i ==> 1 Loss.     -1218.235701490103\n",
            "loss1  -1186.7090633860385\n",
            "_ ==>  3  i ==> 2 Loss.     -1186.7090633860385\n",
            "loss1  -1162.5668631651524\n",
            "_ ==>  3  i ==> 3 Loss.     -1162.5668631651524\n",
            "loss1  -1146.7150943426013\n",
            "_ ==>  3  i ==> 4 Loss.     -1146.7150943426013\n",
            "loss1  -1124.418597801501\n",
            "_ ==>  3  i ==> 5 Loss.     -1124.418597801501\n",
            "loss1  -1103.3521262738293\n",
            "_ ==>  3  i ==> 6 Loss.     -1103.3521262738293\n",
            "loss1  -1093.3825750795406\n",
            "_ ==>  3  i ==> 7 Loss.     -1093.3825750795406\n",
            "loss1  -1089.0103724711294\n",
            "_ ==>  3  i ==> 8 Loss.     -1089.0103724711294\n",
            "loss1  -1085.7793507793908\n",
            "_ ==>  3  i ==> 9 Loss.     -1085.7793507793908\n",
            "loss1  -1082.815373715795\n",
            "_ ==>  3  i ==> 10 Loss.     -1082.815373715795\n",
            "loss1  -1079.9603934502452\n",
            "_ ==>  3  i ==> 11 Loss.     -1079.9603934502452\n",
            "loss1  -1077.3013278439425\n",
            "_ ==>  3  i ==> 12 Loss.     -1077.3013278439425\n",
            "loss1  -1074.9326509664347\n",
            "_ ==>  3  i ==> 13 Loss.     -1074.9326509664347\n",
            "loss1  -1072.7671993898298\n",
            "_ ==>  3  i ==> 14 Loss.     -1072.7671993898298\n",
            "loss1  -1070.704247220618\n",
            "_ ==>  3  i ==> 15 Loss.     -1070.704247220618\n",
            "loss1  -1068.7726335732405\n",
            "_ ==>  3  i ==> 16 Loss.     -1068.7726335732405\n",
            "loss1  -1067.10341517805\n",
            "_ ==>  3  i ==> 17 Loss.     -1067.10341517805\n",
            "loss1  -1065.8240173831553\n",
            "_ ==>  3  i ==> 18 Loss.     -1065.8240173831553\n",
            "loss1  -1064.9598498153612\n",
            "_ ==>  3  i ==> 19 Loss.     -1064.9598498153612\n",
            "loss1  -1064.4338679076207\n",
            "_ ==>  3  i ==> 20 Loss.     -1064.4338679076207\n",
            "loss1  -1064.1364375859166\n",
            "_ ==>  3  i ==> 21 Loss.     -1064.1364375859166\n",
            "loss1  -1063.976581374715\n",
            "_ ==>  3  i ==> 22 Loss.     -1063.976581374715\n",
            "loss1  -1063.8937140429555\n",
            "_ ==>  3  i ==> 23 Loss.     -1063.8937140429555\n",
            "loss1  -1063.851841397981\n",
            "_ ==>  3  i ==> 24 Loss.     -1063.851841397981\n",
            "loss1  -1063.8310530239992\n",
            "_ ==>  3  i ==> 25 Loss.     -1063.8310530239992\n",
            "loss1  -1063.8208544402128\n",
            "_ ==>  3  i ==> 26 Loss.     -1063.8208544402128\n",
            "loss1  -1063.815890874584\n",
            "_ ==>  3  i ==> 27 Loss.     -1063.815890874584\n",
            "loss1  -1063.8134880551893\n",
            "_ ==>  3  i ==> 28 Loss.     -1063.8134880551893\n",
            "loss1  -1063.8123290712235\n",
            "_ ==>  3  i ==> 29 Loss.     -1063.8123290712235\n",
            "loss1  -1063.8117714158157\n",
            "_ ==>  3  i ==> 30 Loss.     -1063.8117714158157\n",
            "stagnant loss\n",
            "pi is [0.5 0.  0.5]\n",
            "loss1  -1227.9104875099752\n",
            "_ ==>  4  i ==> 0 Loss.     -1227.9104875099752\n",
            "loss1  -1201.8562496224347\n",
            "_ ==>  4  i ==> 1 Loss.     -1201.8562496224347\n",
            "loss1  -1185.8706738342048\n",
            "_ ==>  4  i ==> 2 Loss.     -1185.8706738342048\n",
            "loss1  -1170.0709498233468\n",
            "_ ==>  4  i ==> 3 Loss.     -1170.0709498233468\n",
            "loss1  -1151.4963390887729\n",
            "_ ==>  4  i ==> 4 Loss.     -1151.4963390887729\n",
            "loss1  -1132.01333249773\n",
            "_ ==>  4  i ==> 5 Loss.     -1132.01333249773\n",
            "loss1  -1124.9976540460189\n",
            "_ ==>  4  i ==> 6 Loss.     -1124.9976540460189\n",
            "loss1  -1123.2522443378239\n",
            "_ ==>  4  i ==> 7 Loss.     -1123.2522443378239\n",
            "loss1  -1121.8774780931096\n",
            "_ ==>  4  i ==> 8 Loss.     -1121.8774780931096\n",
            "loss1  -1120.0760977904101\n",
            "_ ==>  4  i ==> 9 Loss.     -1120.0760977904101\n",
            "loss1  -1117.304655975711\n",
            "_ ==>  4  i ==> 10 Loss.     -1117.304655975711\n",
            "loss1  -1112.5139040544902\n",
            "_ ==>  4  i ==> 11 Loss.     -1112.5139040544902\n",
            "loss1  -1103.7391539232408\n",
            "_ ==>  4  i ==> 12 Loss.     -1103.7391539232408\n",
            "loss1  -1089.3446089587887\n",
            "_ ==>  4  i ==> 13 Loss.     -1089.3446089587887\n",
            "loss1  -1074.4075220025263\n",
            "_ ==>  4  i ==> 14 Loss.     -1074.4075220025263\n",
            "loss1  -1067.876659485607\n",
            "_ ==>  4  i ==> 15 Loss.     -1067.876659485607\n",
            "loss1  -1066.3222736857717\n",
            "_ ==>  4  i ==> 16 Loss.     -1066.3222736857717\n",
            "loss1  -1065.372296449711\n",
            "_ ==>  4  i ==> 17 Loss.     -1065.372296449711\n",
            "loss1  -1064.6502184802978\n",
            "_ ==>  4  i ==> 18 Loss.     -1064.6502184802978\n",
            "loss1  -1064.2236301140606\n",
            "_ ==>  4  i ==> 19 Loss.     -1064.2236301140606\n",
            "loss1  -1064.0083064970395\n",
            "_ ==>  4  i ==> 20 Loss.     -1064.0083064970395\n",
            "loss1  -1063.905474839937\n",
            "_ ==>  4  i ==> 21 Loss.     -1063.905474839937\n",
            "loss1  -1063.8565141774181\n",
            "_ ==>  4  i ==> 22 Loss.     -1063.8565141774181\n",
            "loss1  -1063.8330453653662\n",
            "_ ==>  4  i ==> 23 Loss.     -1063.8330453653662\n",
            "loss1  -1063.8217491023731\n",
            "_ ==>  4  i ==> 24 Loss.     -1063.8217491023731\n",
            "loss1  -1063.8163055773966\n",
            "_ ==>  4  i ==> 25 Loss.     -1063.8163055773966\n",
            "loss1  -1063.8136836366257\n",
            "_ ==>  4  i ==> 26 Loss.     -1063.8136836366257\n",
            "loss1  -1063.8124221157439\n",
            "_ ==>  4  i ==> 27 Loss.     -1063.8124221157439\n",
            "loss1  -1063.8118158602626\n",
            "_ ==>  4  i ==> 28 Loss.     -1063.8118158602626\n",
            "stagnant loss\n",
            "pi is [0.42857143 0.28571429 0.28571429]\n",
            "loss1  -1237.0192320174701\n",
            "_ ==>  5  i ==> 0 Loss.     -1237.0192320174701\n",
            "loss1  -1211.160737390292\n",
            "_ ==>  5  i ==> 1 Loss.     -1211.160737390292\n",
            "loss1  -1172.1596675724038\n",
            "_ ==>  5  i ==> 2 Loss.     -1172.1596675724038\n",
            "loss1  -1149.2116561085522\n",
            "_ ==>  5  i ==> 3 Loss.     -1149.2116561085522\n",
            "loss1  -1136.554833287351\n",
            "_ ==>  5  i ==> 4 Loss.     -1136.554833287351\n",
            "loss1  -1126.348834800927\n",
            "_ ==>  5  i ==> 5 Loss.     -1126.348834800927\n",
            "loss1  -1117.9388561048004\n",
            "_ ==>  5  i ==> 6 Loss.     -1117.9388561048004\n",
            "loss1  -1108.4250896280962\n",
            "_ ==>  5  i ==> 7 Loss.     -1108.4250896280962\n",
            "loss1  -1093.8257805091012\n",
            "_ ==>  5  i ==> 8 Loss.     -1093.8257805091012\n",
            "loss1  -1079.3010934311408\n",
            "_ ==>  5  i ==> 9 Loss.     -1079.3010934311408\n",
            "loss1  -1072.530219355156\n",
            "_ ==>  5  i ==> 10 Loss.     -1072.530219355156\n",
            "loss1  -1069.409040705854\n",
            "_ ==>  5  i ==> 11 Loss.     -1069.409040705854\n",
            "loss1  -1067.509688155514\n",
            "_ ==>  5  i ==> 12 Loss.     -1067.509688155514\n",
            "loss1  -1066.216321277513\n",
            "_ ==>  5  i ==> 13 Loss.     -1066.216321277513\n",
            "loss1  -1065.2655857239656\n",
            "_ ==>  5  i ==> 14 Loss.     -1065.2655857239656\n",
            "loss1  -1064.5782808418758\n",
            "_ ==>  5  i ==> 15 Loss.     -1064.5782808418758\n",
            "loss1  -1064.152033001804\n",
            "_ ==>  5  i ==> 16 Loss.     -1064.152033001804\n",
            "loss1  -1063.9438454753276\n",
            "_ ==>  5  i ==> 17 Loss.     -1063.9438454753276\n",
            "loss1  -1063.8613191859843\n",
            "_ ==>  5  i ==> 18 Loss.     -1063.8613191859843\n",
            "loss1  -1063.831183256739\n",
            "_ ==>  5  i ==> 19 Loss.     -1063.831183256739\n",
            "loss1  -1063.8197622992957\n",
            "_ ==>  5  i ==> 20 Loss.     -1063.8197622992957\n",
            "loss1  -1063.8150780780697\n",
            "_ ==>  5  i ==> 21 Loss.     -1063.8150780780697\n",
            "loss1  -1063.8130257707876\n",
            "_ ==>  5  i ==> 22 Loss.     -1063.8130257707876\n",
            "loss1  -1063.812088846504\n",
            "_ ==>  5  i ==> 23 Loss.     -1063.812088846504\n",
            "stagnant loss\n",
            "pi is [0.63636364 0.36363636 0.        ]\n",
            "loss1  -1226.68120939813\n",
            "_ ==>  6  i ==> 0 Loss.     -1226.68120939813\n",
            "loss1  -1193.329686786683\n",
            "_ ==>  6  i ==> 1 Loss.     -1193.329686786683\n",
            "loss1  -1170.9968303060639\n",
            "_ ==>  6  i ==> 2 Loss.     -1170.9968303060639\n",
            "loss1  -1159.818402194506\n",
            "_ ==>  6  i ==> 3 Loss.     -1159.818402194506\n",
            "loss1  -1153.3037468431166\n",
            "_ ==>  6  i ==> 4 Loss.     -1153.3037468431166\n",
            "loss1  -1144.9846344524235\n",
            "_ ==>  6  i ==> 5 Loss.     -1144.9846344524235\n",
            "loss1  -1131.758875427233\n",
            "_ ==>  6  i ==> 6 Loss.     -1131.758875427233\n",
            "loss1  -1109.3359697731014\n",
            "_ ==>  6  i ==> 7 Loss.     -1109.3359697731014\n",
            "loss1  -1083.5459585029832\n",
            "_ ==>  6  i ==> 8 Loss.     -1083.5459585029832\n",
            "loss1  -1069.9731397089756\n",
            "_ ==>  6  i ==> 9 Loss.     -1069.9731397089756\n",
            "loss1  -1066.69405120126\n",
            "_ ==>  6  i ==> 10 Loss.     -1066.69405120126\n",
            "loss1  -1065.3309710482508\n",
            "_ ==>  6  i ==> 11 Loss.     -1065.3309710482508\n",
            "loss1  -1064.4545450354549\n",
            "_ ==>  6  i ==> 12 Loss.     -1064.4545450354549\n",
            "loss1  -1064.0345210880946\n",
            "_ ==>  6  i ==> 13 Loss.     -1064.0345210880946\n",
            "loss1  -1063.8872565719148\n",
            "_ ==>  6  i ==> 14 Loss.     -1063.8872565719148\n",
            "loss1  -1063.8396633732186\n",
            "_ ==>  6  i ==> 15 Loss.     -1063.8396633732186\n",
            "loss1  -1063.8229906758165\n",
            "_ ==>  6  i ==> 16 Loss.     -1063.8229906758165\n",
            "loss1  -1063.8164491506755\n",
            "_ ==>  6  i ==> 17 Loss.     -1063.8164491506755\n",
            "loss1  -1063.8136470102713\n",
            "_ ==>  6  i ==> 18 Loss.     -1063.8136470102713\n",
            "loss1  -1063.8123799238772\n",
            "_ ==>  6  i ==> 19 Loss.     -1063.8123799238772\n",
            "loss1  -1063.8117898803027\n",
            "_ ==>  6  i ==> 20 Loss.     -1063.8117898803027\n",
            "stagnant loss\n",
            "pi is [0.5625 0.125  0.3125]\n",
            "loss1  -1243.3742531598903\n",
            "_ ==>  7  i ==> 0 Loss.     -1243.3742531598903\n",
            "loss1  -1209.1834655927623\n",
            "_ ==>  7  i ==> 1 Loss.     -1209.1834655927623\n",
            "loss1  -1172.4057003992953\n",
            "_ ==>  7  i ==> 2 Loss.     -1172.4057003992953\n",
            "loss1  -1145.4392029716703\n",
            "_ ==>  7  i ==> 3 Loss.     -1145.4392029716703\n",
            "loss1  -1125.0251252210664\n",
            "_ ==>  7  i ==> 4 Loss.     -1125.0251252210664\n",
            "loss1  -1103.8828235827639\n",
            "_ ==>  7  i ==> 5 Loss.     -1103.8828235827639\n",
            "loss1  -1085.418584084256\n",
            "_ ==>  7  i ==> 6 Loss.     -1085.418584084256\n",
            "loss1  -1073.3315448469623\n",
            "_ ==>  7  i ==> 7 Loss.     -1073.3315448469623\n",
            "loss1  -1069.0208037792152\n",
            "_ ==>  7  i ==> 8 Loss.     -1069.0208037792152\n",
            "loss1  -1067.2911663989473\n",
            "_ ==>  7  i ==> 9 Loss.     -1067.2911663989473\n",
            "loss1  -1066.0846123609995\n",
            "_ ==>  7  i ==> 10 Loss.     -1066.0846123609995\n",
            "loss1  -1065.1269545352852\n",
            "_ ==>  7  i ==> 11 Loss.     -1065.1269545352852\n",
            "loss1  -1064.4321391861956\n",
            "_ ==>  7  i ==> 12 Loss.     -1064.4321391861956\n",
            "loss1  -1064.0379380989555\n",
            "_ ==>  7  i ==> 13 Loss.     -1064.0379380989555\n",
            "loss1  -1063.876947474426\n",
            "_ ==>  7  i ==> 14 Loss.     -1063.876947474426\n",
            "loss1  -1063.82779474348\n",
            "_ ==>  7  i ==> 15 Loss.     -1063.82779474348\n",
            "loss1  -1063.8151829089302\n",
            "_ ==>  7  i ==> 16 Loss.     -1063.8151829089302\n",
            "loss1  -1063.8121727243788\n",
            "_ ==>  7  i ==> 17 Loss.     -1063.8121727243788\n",
            "loss1  -1063.8114701565482\n",
            "_ ==>  7  i ==> 18 Loss.     -1063.8114701565482\n",
            "stagnant loss\n",
            "pi is [0.1875 0.5625 0.25  ]\n",
            "loss1  -1244.063818704639\n",
            "_ ==>  8  i ==> 0 Loss.     -1244.063818704639\n",
            "loss1  -1202.2709918625603\n",
            "_ ==>  8  i ==> 1 Loss.     -1202.2709918625603\n",
            "loss1  -1183.9855597302858\n",
            "_ ==>  8  i ==> 2 Loss.     -1183.9855597302858\n",
            "loss1  -1176.9394064302821\n",
            "_ ==>  8  i ==> 3 Loss.     -1176.9394064302821\n",
            "loss1  -1173.6886692291087\n",
            "_ ==>  8  i ==> 4 Loss.     -1173.6886692291087\n",
            "loss1  -1171.5450084544586\n",
            "_ ==>  8  i ==> 5 Loss.     -1171.5450084544586\n",
            "loss1  -1169.5016536665262\n",
            "_ ==>  8  i ==> 6 Loss.     -1169.5016536665262\n",
            "loss1  -1167.031575378836\n",
            "_ ==>  8  i ==> 7 Loss.     -1167.031575378836\n",
            "loss1  -1163.5851744353736\n",
            "_ ==>  8  i ==> 8 Loss.     -1163.5851744353736\n",
            "loss1  -1158.4113112898722\n",
            "_ ==>  8  i ==> 9 Loss.     -1158.4113112898722\n",
            "loss1  -1150.77070547898\n",
            "_ ==>  8  i ==> 10 Loss.     -1150.77070547898\n",
            "loss1  -1141.2772723635514\n",
            "_ ==>  8  i ==> 11 Loss.     -1141.2772723635514\n",
            "loss1  -1132.1379826093103\n",
            "_ ==>  8  i ==> 12 Loss.     -1132.1379826093103\n",
            "loss1  -1125.33076475069\n",
            "_ ==>  8  i ==> 13 Loss.     -1125.33076475069\n",
            "loss1  -1118.9467368699154\n",
            "_ ==>  8  i ==> 14 Loss.     -1118.9467368699154\n",
            "loss1  -1109.6733228539042\n",
            "_ ==>  8  i ==> 15 Loss.     -1109.6733228539042\n",
            "loss1  -1099.5767706467832\n",
            "_ ==>  8  i ==> 16 Loss.     -1099.5767706467832\n",
            "loss1  -1091.4907937309663\n",
            "_ ==>  8  i ==> 17 Loss.     -1091.4907937309663\n",
            "loss1  -1085.4668750726057\n",
            "_ ==>  8  i ==> 18 Loss.     -1085.4668750726057\n",
            "loss1  -1080.7283026203563\n",
            "_ ==>  8  i ==> 19 Loss.     -1080.7283026203563\n",
            "loss1  -1076.3965233198962\n",
            "_ ==>  8  i ==> 20 Loss.     -1076.3965233198962\n",
            "loss1  -1072.410205493292\n",
            "_ ==>  8  i ==> 21 Loss.     -1072.410205493292\n",
            "loss1  -1069.1230437747672\n",
            "_ ==>  8  i ==> 22 Loss.     -1069.1230437747672\n",
            "loss1  -1066.7842261880471\n",
            "_ ==>  8  i ==> 23 Loss.     -1066.7842261880471\n",
            "loss1  -1065.3322929680867\n",
            "_ ==>  8  i ==> 24 Loss.     -1065.3322929680867\n",
            "loss1  -1064.5292296347623\n",
            "_ ==>  8  i ==> 25 Loss.     -1064.5292296347623\n",
            "loss1  -1064.1327120372343\n",
            "_ ==>  8  i ==> 26 Loss.     -1064.1327120372343\n",
            "loss1  -1063.9538495587842\n",
            "_ ==>  8  i ==> 27 Loss.     -1063.9538495587842\n",
            "loss1  -1063.875639652442\n",
            "_ ==>  8  i ==> 28 Loss.     -1063.875639652442\n",
            "loss1  -1063.840934140614\n",
            "_ ==>  8  i ==> 29 Loss.     -1063.840934140614\n",
            "loss1  -1063.825137749633\n",
            "_ ==>  8  i ==> 30 Loss.     -1063.825137749633\n",
            "loss1  -1063.8178071629086\n",
            "_ ==>  8  i ==> 31 Loss.     -1063.8178071629086\n",
            "loss1  -1063.814364097078\n",
            "_ ==>  8  i ==> 32 Loss.     -1063.814364097078\n",
            "loss1  -1063.8127354288656\n",
            "_ ==>  8  i ==> 33 Loss.     -1063.8127354288656\n",
            "loss1  -1063.811961767943\n",
            "_ ==>  8  i ==> 34 Loss.     -1063.811961767943\n",
            "stagnant loss\n",
            "pi is [0.         0.58333333 0.41666667]\n",
            "loss1  -1230.0802768570052\n",
            "_ ==>  9  i ==> 0 Loss.     -1230.0802768570052\n",
            "loss1  -1200.3688692352173\n",
            "_ ==>  9  i ==> 1 Loss.     -1200.3688692352173\n",
            "loss1  -1184.6297241536877\n",
            "_ ==>  9  i ==> 2 Loss.     -1184.6297241536877\n",
            "loss1  -1175.1923734384627\n",
            "_ ==>  9  i ==> 3 Loss.     -1175.1923734384627\n",
            "loss1  -1170.1710586094155\n",
            "_ ==>  9  i ==> 4 Loss.     -1170.1710586094155\n",
            "loss1  -1167.3014214718587\n",
            "_ ==>  9  i ==> 5 Loss.     -1167.3014214718587\n",
            "loss1  -1165.3302118660213\n",
            "_ ==>  9  i ==> 6 Loss.     -1165.3302118660213\n",
            "loss1  -1163.7619477931464\n",
            "_ ==>  9  i ==> 7 Loss.     -1163.7619477931464\n",
            "loss1  -1162.38704473825\n",
            "_ ==>  9  i ==> 8 Loss.     -1162.38704473825\n",
            "loss1  -1161.0921181252638\n",
            "_ ==>  9  i ==> 9 Loss.     -1161.0921181252638\n",
            "loss1  -1159.7949347101267\n",
            "_ ==>  9  i ==> 10 Loss.     -1159.7949347101267\n",
            "loss1  -1158.4176835969063\n",
            "_ ==>  9  i ==> 11 Loss.     -1158.4176835969063\n",
            "loss1  -1156.8635432272927\n",
            "_ ==>  9  i ==> 12 Loss.     -1156.8635432272927\n",
            "loss1  -1154.9727716457762\n",
            "_ ==>  9  i ==> 13 Loss.     -1154.9727716457762\n",
            "loss1  -1152.4180800009317\n",
            "_ ==>  9  i ==> 14 Loss.     -1152.4180800009317\n",
            "loss1  -1148.4583847584\n",
            "_ ==>  9  i ==> 15 Loss.     -1148.4583847584\n",
            "loss1  -1141.456581493484\n",
            "_ ==>  9  i ==> 16 Loss.     -1141.456581493484\n",
            "loss1  -1128.8582810952557\n",
            "_ ==>  9  i ==> 17 Loss.     -1128.8582810952557\n",
            "loss1  -1112.4647723774551\n",
            "_ ==>  9  i ==> 18 Loss.     -1112.4647723774551\n",
            "loss1  -1102.313441135747\n",
            "_ ==>  9  i ==> 19 Loss.     -1102.313441135747\n",
            "loss1  -1097.7354103694677\n",
            "_ ==>  9  i ==> 20 Loss.     -1097.7354103694677\n",
            "loss1  -1094.1537356892995\n",
            "_ ==>  9  i ==> 21 Loss.     -1094.1537356892995\n",
            "loss1  -1089.8867169943546\n",
            "_ ==>  9  i ==> 22 Loss.     -1089.8867169943546\n",
            "loss1  -1083.7578579960764\n",
            "_ ==>  9  i ==> 23 Loss.     -1083.7578579960764\n",
            "loss1  -1076.016575364767\n",
            "_ ==>  9  i ==> 24 Loss.     -1076.016575364767\n",
            "loss1  -1066.640472484747\n",
            "_ ==>  9  i ==> 25 Loss.     -1066.640472484747\n",
            "loss1  -1064.0390206991192\n",
            "_ ==>  9  i ==> 26 Loss.     -1064.0390206991192\n",
            "loss1  -1063.8691688082408\n",
            "_ ==>  9  i ==> 27 Loss.     -1063.8691688082408\n",
            "loss1  -1063.82828742953\n",
            "_ ==>  9  i ==> 28 Loss.     -1063.82828742953\n",
            "loss1  -1063.816689413248\n",
            "_ ==>  9  i ==> 29 Loss.     -1063.816689413248\n",
            "loss1  -1063.8132094781495\n",
            "_ ==>  9  i ==> 30 Loss.     -1063.8132094781495\n",
            "loss1  -1063.8120412286257\n",
            "_ ==>  9  i ==> 31 Loss.     -1063.8120412286257\n",
            "loss1  -1063.8115975567962\n",
            "_ ==>  9  i ==> 32 Loss.     -1063.8115975567962\n",
            "stagnant loss\n",
            "pi is [0.25 0.5  0.25]\n",
            "loss1  -1238.0974522864421\n",
            "_ ==>  10  i ==> 0 Loss.     -1238.0974522864421\n",
            "loss1  -1205.1062091618571\n",
            "_ ==>  10  i ==> 1 Loss.     -1205.1062091618571\n",
            "loss1  -1163.507051172853\n",
            "_ ==>  10  i ==> 2 Loss.     -1163.507051172853\n",
            "loss1  -1131.240863636551\n",
            "_ ==>  10  i ==> 3 Loss.     -1131.240863636551\n",
            "loss1  -1100.856655917561\n",
            "_ ==>  10  i ==> 4 Loss.     -1100.856655917561\n",
            "loss1  -1079.1992417270903\n",
            "_ ==>  10  i ==> 5 Loss.     -1079.1992417270903\n",
            "loss1  -1071.313030588225\n",
            "_ ==>  10  i ==> 6 Loss.     -1071.313030588225\n",
            "loss1  -1068.3773813067805\n",
            "_ ==>  10  i ==> 7 Loss.     -1068.3773813067805\n",
            "loss1  -1066.8124391018955\n",
            "_ ==>  10  i ==> 8 Loss.     -1066.8124391018955\n",
            "loss1  -1065.7539858847776\n",
            "_ ==>  10  i ==> 9 Loss.     -1065.7539858847776\n",
            "loss1  -1064.9490462210779\n",
            "_ ==>  10  i ==> 10 Loss.     -1064.9490462210779\n",
            "loss1  -1064.3753750443757\n",
            "_ ==>  10  i ==> 11 Loss.     -1064.3753750443757\n",
            "loss1  -1064.044155759382\n",
            "_ ==>  10  i ==> 12 Loss.     -1064.044155759382\n",
            "loss1  -1063.8969382461853\n",
            "_ ==>  10  i ==> 13 Loss.     -1063.8969382461853\n",
            "loss1  -1063.8427631137833\n",
            "_ ==>  10  i ==> 14 Loss.     -1063.8427631137833\n",
            "loss1  -1063.823700742584\n",
            "_ ==>  10  i ==> 15 Loss.     -1063.823700742584\n",
            "loss1  -1063.8165619518372\n",
            "_ ==>  10  i ==> 16 Loss.     -1063.8165619518372\n",
            "loss1  -1063.813640888147\n",
            "_ ==>  10  i ==> 17 Loss.     -1063.813640888147\n",
            "loss1  -1063.8123609790443\n",
            "_ ==>  10  i ==> 18 Loss.     -1063.8123609790443\n",
            "loss1  -1063.8117763751486\n",
            "_ ==>  10  i ==> 19 Loss.     -1063.8117763751486\n",
            "stagnant loss\n",
            "pi is [0.39130435 0.34782609 0.26086957]\n",
            "loss1  -1239.5310897577301\n",
            "_ ==>  11  i ==> 0 Loss.     -1239.5310897577301\n",
            "loss1  -1235.8262633725278\n",
            "_ ==>  11  i ==> 1 Loss.     -1235.8262633725278\n",
            "loss1  -1230.0966518809043\n",
            "_ ==>  11  i ==> 2 Loss.     -1230.0966518809043\n",
            "loss1  -1214.2338914777688\n",
            "_ ==>  11  i ==> 3 Loss.     -1214.2338914777688\n",
            "loss1  -1197.2573766175399\n",
            "_ ==>  11  i ==> 4 Loss.     -1197.2573766175399\n",
            "loss1  -1182.7184224545283\n",
            "_ ==>  11  i ==> 5 Loss.     -1182.7184224545283\n",
            "loss1  -1173.1507229811714\n",
            "_ ==>  11  i ==> 6 Loss.     -1173.1507229811714\n",
            "loss1  -1168.623314877099\n",
            "_ ==>  11  i ==> 7 Loss.     -1168.623314877099\n",
            "loss1  -1165.8607688362788\n",
            "_ ==>  11  i ==> 8 Loss.     -1165.8607688362788\n",
            "loss1  -1163.0636162504668\n",
            "_ ==>  11  i ==> 9 Loss.     -1163.0636162504668\n",
            "loss1  -1159.5290313677988\n",
            "_ ==>  11  i ==> 10 Loss.     -1159.5290313677988\n",
            "loss1  -1154.349086819297\n",
            "_ ==>  11  i ==> 11 Loss.     -1154.349086819297\n",
            "loss1  -1145.63059074338\n",
            "_ ==>  11  i ==> 12 Loss.     -1145.63059074338\n",
            "loss1  -1130.1371595829526\n",
            "_ ==>  11  i ==> 13 Loss.     -1130.1371595829526\n",
            "loss1  -1107.416818555494\n",
            "_ ==>  11  i ==> 14 Loss.     -1107.416818555494\n",
            "loss1  -1085.7645345052201\n",
            "_ ==>  11  i ==> 15 Loss.     -1085.7645345052201\n",
            "loss1  -1071.7396219748916\n",
            "_ ==>  11  i ==> 16 Loss.     -1071.7396219748916\n",
            "loss1  -1066.7012988709535\n",
            "_ ==>  11  i ==> 17 Loss.     -1066.7012988709535\n",
            "loss1  -1065.103448249235\n",
            "_ ==>  11  i ==> 18 Loss.     -1065.103448249235\n",
            "loss1  -1064.3627144338684\n",
            "_ ==>  11  i ==> 19 Loss.     -1064.3627144338684\n",
            "loss1  -1064.0515740020523\n",
            "_ ==>  11  i ==> 20 Loss.     -1064.0515740020523\n",
            "loss1  -1063.9211805189643\n",
            "_ ==>  11  i ==> 21 Loss.     -1063.9211805189643\n",
            "loss1  -1063.863158809962\n",
            "_ ==>  11  i ==> 22 Loss.     -1063.863158809962\n",
            "loss1  -1063.8361100866168\n",
            "_ ==>  11  i ==> 23 Loss.     -1063.8361100866168\n",
            "loss1  -1063.8232127367792\n",
            "_ ==>  11  i ==> 24 Loss.     -1063.8232127367792\n",
            "loss1  -1063.8170117434897\n",
            "_ ==>  11  i ==> 25 Loss.     -1063.8170117434897\n",
            "loss1  -1063.8140246937496\n",
            "_ ==>  11  i ==> 26 Loss.     -1063.8140246937496\n",
            "loss1  -1063.8125865660684\n",
            "_ ==>  11  i ==> 27 Loss.     -1063.8125865660684\n",
            "loss1  -1063.8118949973\n",
            "_ ==>  11  i ==> 28 Loss.     -1063.8118949973\n",
            "stagnant loss\n",
            "pi is [0.4 0.4 0.2]\n",
            "loss1  -1238.3486401954679\n",
            "_ ==>  12  i ==> 0 Loss.     -1238.3486401954679\n",
            "loss1  -1213.481195046245\n",
            "_ ==>  12  i ==> 1 Loss.     -1213.481195046245\n",
            "loss1  -1179.6031882712387\n",
            "_ ==>  12  i ==> 2 Loss.     -1179.6031882712387\n",
            "loss1  -1157.163330130839\n",
            "_ ==>  12  i ==> 3 Loss.     -1157.163330130839\n",
            "loss1  -1141.0553747148547\n",
            "_ ==>  12  i ==> 4 Loss.     -1141.0553747148547\n",
            "loss1  -1124.4138863541111\n",
            "_ ==>  12  i ==> 5 Loss.     -1124.4138863541111\n",
            "loss1  -1108.6525158516952\n",
            "_ ==>  12  i ==> 6 Loss.     -1108.6525158516952\n",
            "loss1  -1091.202428976583\n",
            "_ ==>  12  i ==> 7 Loss.     -1091.202428976583\n",
            "loss1  -1077.8582443018695\n",
            "_ ==>  12  i ==> 8 Loss.     -1077.8582443018695\n",
            "loss1  -1072.111669775088\n",
            "_ ==>  12  i ==> 9 Loss.     -1072.111669775088\n",
            "loss1  -1069.0363496411176\n",
            "_ ==>  12  i ==> 10 Loss.     -1069.0363496411176\n",
            "loss1  -1067.0264725846882\n",
            "_ ==>  12  i ==> 11 Loss.     -1067.0264725846882\n",
            "loss1  -1065.691846675837\n",
            "_ ==>  12  i ==> 12 Loss.     -1065.691846675837\n",
            "loss1  -1064.810236885924\n",
            "_ ==>  12  i ==> 13 Loss.     -1064.810236885924\n",
            "loss1  -1064.276662808639\n",
            "_ ==>  12  i ==> 14 Loss.     -1064.276662808639\n",
            "loss1  -1064.0059998713089\n",
            "_ ==>  12  i ==> 15 Loss.     -1064.0059998713089\n",
            "loss1  -1063.8904702341113\n",
            "_ ==>  12  i ==> 16 Loss.     -1063.8904702341113\n",
            "loss1  -1063.8446541919336\n",
            "_ ==>  12  i ==> 17 Loss.     -1063.8446541919336\n",
            "loss1  -1063.8260352454859\n",
            "_ ==>  12  i ==> 18 Loss.     -1063.8260352454859\n",
            "loss1  -1063.8180273394785\n",
            "_ ==>  12  i ==> 19 Loss.     -1063.8180273394785\n",
            "loss1  -1063.8144215232294\n",
            "_ ==>  12  i ==> 20 Loss.     -1063.8144215232294\n",
            "loss1  -1063.8127520487167\n",
            "_ ==>  12  i ==> 21 Loss.     -1063.8127520487167\n",
            "loss1  -1063.8119672662694\n",
            "_ ==>  12  i ==> 22 Loss.     -1063.8119672662694\n",
            "stagnant loss\n",
            "pi is [0.36363636 0.63636364 0.        ]\n",
            "loss1  -1228.1054618109101\n",
            "_ ==>  13  i ==> 0 Loss.     -1228.1054618109101\n",
            "loss1  -1198.6188877775496\n",
            "_ ==>  13  i ==> 1 Loss.     -1198.6188877775496\n",
            "loss1  -1176.3317297604347\n",
            "_ ==>  13  i ==> 2 Loss.     -1176.3317297604347\n",
            "loss1  -1159.8596422939827\n",
            "_ ==>  13  i ==> 3 Loss.     -1159.8596422939827\n",
            "loss1  -1151.2480847532063\n",
            "_ ==>  13  i ==> 4 Loss.     -1151.2480847532063\n",
            "loss1  -1141.2833538181203\n",
            "_ ==>  13  i ==> 5 Loss.     -1141.2833538181203\n",
            "loss1  -1120.554430032791\n",
            "_ ==>  13  i ==> 6 Loss.     -1120.554430032791\n",
            "loss1  -1098.1537267883405\n",
            "_ ==>  13  i ==> 7 Loss.     -1098.1537267883405\n",
            "loss1  -1080.7067292830518\n",
            "_ ==>  13  i ==> 8 Loss.     -1080.7067292830518\n",
            "loss1  -1071.558379181903\n",
            "_ ==>  13  i ==> 9 Loss.     -1071.558379181903\n",
            "loss1  -1067.38139252998\n",
            "_ ==>  13  i ==> 10 Loss.     -1067.38139252998\n",
            "loss1  -1065.2558872197467\n",
            "_ ==>  13  i ==> 11 Loss.     -1065.2558872197467\n",
            "loss1  -1064.2370914717671\n",
            "_ ==>  13  i ==> 12 Loss.     -1064.2370914717671\n",
            "loss1  -1063.9093523582276\n",
            "_ ==>  13  i ==> 13 Loss.     -1063.9093523582276\n",
            "loss1  -1063.8327826265831\n",
            "_ ==>  13  i ==> 14 Loss.     -1063.8327826265831\n",
            "loss1  -1063.8160688964535\n",
            "_ ==>  13  i ==> 15 Loss.     -1063.8160688964535\n",
            "loss1  -1063.8123533720052\n",
            "_ ==>  13  i ==> 16 Loss.     -1063.8123533720052\n",
            "loss1  -1063.8115104131596\n",
            "_ ==>  13  i ==> 17 Loss.     -1063.8115104131596\n",
            "stagnant loss\n",
            "pi is [0.28571429 0.42857143 0.28571429]\n",
            "loss1  -1247.030665027546\n",
            "_ ==>  14  i ==> 0 Loss.     -1247.030665027546\n",
            "loss1  -1217.2324586881846\n",
            "_ ==>  14  i ==> 1 Loss.     -1217.2324586881846\n",
            "loss1  -1184.5958640941888\n",
            "_ ==>  14  i ==> 2 Loss.     -1184.5958640941888\n",
            "loss1  -1167.2941984929957\n",
            "_ ==>  14  i ==> 3 Loss.     -1167.2941984929957\n",
            "loss1  -1162.0367160577096\n",
            "_ ==>  14  i ==> 4 Loss.     -1162.0367160577096\n",
            "loss1  -1157.7268305403722\n",
            "_ ==>  14  i ==> 5 Loss.     -1157.7268305403722\n",
            "loss1  -1152.4131365129838\n",
            "_ ==>  14  i ==> 6 Loss.     -1152.4131365129838\n",
            "loss1  -1145.6449627251209\n",
            "_ ==>  14  i ==> 7 Loss.     -1145.6449627251209\n",
            "loss1  -1137.022465076329\n",
            "_ ==>  14  i ==> 8 Loss.     -1137.022465076329\n",
            "loss1  -1128.3421561613377\n",
            "_ ==>  14  i ==> 9 Loss.     -1128.3421561613377\n",
            "loss1  -1119.9762831552132\n",
            "_ ==>  14  i ==> 10 Loss.     -1119.9762831552132\n",
            "loss1  -1109.9030586666324\n",
            "_ ==>  14  i ==> 11 Loss.     -1109.9030586666324\n",
            "loss1  -1102.1875506140373\n",
            "_ ==>  14  i ==> 12 Loss.     -1102.1875506140373\n",
            "loss1  -1096.7710314224905\n",
            "_ ==>  14  i ==> 13 Loss.     -1096.7710314224905\n",
            "loss1  -1091.878459810719\n",
            "_ ==>  14  i ==> 14 Loss.     -1091.878459810719\n",
            "loss1  -1087.2908957128975\n",
            "_ ==>  14  i ==> 15 Loss.     -1087.2908957128975\n",
            "loss1  -1082.9055265362945\n",
            "_ ==>  14  i ==> 16 Loss.     -1082.9055265362945\n",
            "loss1  -1078.5419661454218\n",
            "_ ==>  14  i ==> 17 Loss.     -1078.5419661454218\n",
            "loss1  -1074.2857831658157\n",
            "_ ==>  14  i ==> 18 Loss.     -1074.2857831658157\n",
            "loss1  -1070.4876021234106\n",
            "_ ==>  14  i ==> 19 Loss.     -1070.4876021234106\n",
            "loss1  -1067.5728250452048\n",
            "_ ==>  14  i ==> 20 Loss.     -1067.5728250452048\n",
            "loss1  -1065.7103413857099\n",
            "_ ==>  14  i ==> 21 Loss.     -1065.7103413857099\n",
            "loss1  -1064.703598019886\n",
            "_ ==>  14  i ==> 22 Loss.     -1064.703598019886\n",
            "loss1  -1064.2184698786355\n",
            "_ ==>  14  i ==> 23 Loss.     -1064.2184698786355\n",
            "loss1  -1063.9970969381145\n",
            "_ ==>  14  i ==> 24 Loss.     -1063.9970969381145\n",
            "loss1  -1063.8969966119564\n",
            "_ ==>  14  i ==> 25 Loss.     -1063.8969966119564\n",
            "loss1  -1063.851248056105\n",
            "_ ==>  14  i ==> 26 Loss.     -1063.851248056105\n",
            "loss1  -1063.8300617830014\n",
            "_ ==>  14  i ==> 27 Loss.     -1063.8300617830014\n",
            "loss1  -1063.8201488224618\n",
            "_ ==>  14  i ==> 28 Loss.     -1063.8201488224618\n",
            "loss1  -1063.8154773092367\n",
            "_ ==>  14  i ==> 29 Loss.     -1063.8154773092367\n",
            "loss1  -1063.8132651509995\n",
            "_ ==>  14  i ==> 30 Loss.     -1063.8132651509995\n",
            "loss1  -1063.8122141489894\n",
            "_ ==>  14  i ==> 31 Loss.     -1063.8122141489894\n",
            "stagnant loss\n",
            "pi is [0.4 0.3 0.3]\n",
            "loss1  -1228.0514304225724\n",
            "_ ==>  15  i ==> 0 Loss.     -1228.0514304225724\n",
            "loss1  -1183.1712439491284\n",
            "_ ==>  15  i ==> 1 Loss.     -1183.1712439491284\n",
            "loss1  -1157.4739955012524\n",
            "_ ==>  15  i ==> 2 Loss.     -1157.4739955012524\n",
            "loss1  -1147.5990999637625\n",
            "_ ==>  15  i ==> 3 Loss.     -1147.5990999637625\n",
            "loss1  -1140.1147876766152\n",
            "_ ==>  15  i ==> 4 Loss.     -1140.1147876766152\n",
            "loss1  -1131.3310803958746\n",
            "_ ==>  15  i ==> 5 Loss.     -1131.3310803958746\n",
            "loss1  -1114.5977469220343\n",
            "_ ==>  15  i ==> 6 Loss.     -1114.5977469220343\n",
            "loss1  -1092.1318372463454\n",
            "_ ==>  15  i ==> 7 Loss.     -1092.1318372463454\n",
            "loss1  -1077.7162316646366\n",
            "_ ==>  15  i ==> 8 Loss.     -1077.7162316646366\n",
            "loss1  -1072.1605699768597\n",
            "_ ==>  15  i ==> 9 Loss.     -1072.1605699768597\n",
            "loss1  -1069.4930186939343\n",
            "_ ==>  15  i ==> 10 Loss.     -1069.4930186939343\n",
            "loss1  -1067.8394651358917\n",
            "_ ==>  15  i ==> 11 Loss.     -1067.8394651358917\n",
            "loss1  -1066.7122096523208\n",
            "_ ==>  15  i ==> 12 Loss.     -1066.7122096523208\n",
            "loss1  -1065.8113014988844\n",
            "_ ==>  15  i ==> 13 Loss.     -1065.8113014988844\n",
            "loss1  -1065.0416864853921\n",
            "_ ==>  15  i ==> 14 Loss.     -1065.0416864853921\n",
            "loss1  -1064.4417335030755\n",
            "_ ==>  15  i ==> 15 Loss.     -1064.4417335030755\n",
            "loss1  -1064.0704164462925\n",
            "_ ==>  15  i ==> 16 Loss.     -1064.0704164462925\n",
            "loss1  -1063.900313401303\n",
            "_ ==>  15  i ==> 17 Loss.     -1063.900313401303\n",
            "loss1  -1063.84006461462\n",
            "_ ==>  15  i ==> 18 Loss.     -1063.84006461462\n",
            "loss1  -1063.8211085697048\n",
            "_ ==>  15  i ==> 19 Loss.     -1063.8211085697048\n",
            "loss1  -1063.8149968187272\n",
            "_ ==>  15  i ==> 20 Loss.     -1063.8149968187272\n",
            "loss1  -1063.8128166662627\n",
            "_ ==>  15  i ==> 21 Loss.     -1063.8128166662627\n",
            "loss1  -1063.811949694768\n",
            "_ ==>  15  i ==> 22 Loss.     -1063.811949694768\n",
            "stagnant loss\n",
            "pi is [0.25   0.1875 0.5625]\n",
            "loss1  -1232.5492174594947\n",
            "_ ==>  16  i ==> 0 Loss.     -1232.5492174594947\n",
            "loss1  -1214.9573448737153\n",
            "_ ==>  16  i ==> 1 Loss.     -1214.9573448737153\n",
            "loss1  -1180.9655463008583\n",
            "_ ==>  16  i ==> 2 Loss.     -1180.9655463008583\n",
            "loss1  -1147.2314353491295\n",
            "_ ==>  16  i ==> 3 Loss.     -1147.2314353491295\n",
            "loss1  -1127.1903855249882\n",
            "_ ==>  16  i ==> 4 Loss.     -1127.1903855249882\n",
            "loss1  -1108.0211930824935\n",
            "_ ==>  16  i ==> 5 Loss.     -1108.0211930824935\n",
            "loss1  -1090.8419371938867\n",
            "_ ==>  16  i ==> 6 Loss.     -1090.8419371938867\n",
            "loss1  -1082.2909838783655\n",
            "_ ==>  16  i ==> 7 Loss.     -1082.2909838783655\n",
            "loss1  -1077.4931763019717\n",
            "_ ==>  16  i ==> 8 Loss.     -1077.4931763019717\n",
            "loss1  -1071.921655493878\n",
            "_ ==>  16  i ==> 9 Loss.     -1071.921655493878\n",
            "loss1  -1066.6328246532962\n",
            "_ ==>  16  i ==> 10 Loss.     -1066.6328246532962\n",
            "loss1  -1064.9348378952136\n",
            "_ ==>  16  i ==> 11 Loss.     -1064.9348378952136\n",
            "loss1  -1064.3154002033953\n",
            "_ ==>  16  i ==> 12 Loss.     -1064.3154002033953\n",
            "loss1  -1063.9999290276273\n",
            "_ ==>  16  i ==> 13 Loss.     -1063.9999290276273\n",
            "loss1  -1063.8706624757178\n",
            "_ ==>  16  i ==> 14 Loss.     -1063.8706624757178\n",
            "loss1  -1063.828952067519\n",
            "_ ==>  16  i ==> 15 Loss.     -1063.828952067519\n",
            "loss1  -1063.8168414283784\n",
            "_ ==>  16  i ==> 16 Loss.     -1063.8168414283784\n",
            "loss1  -1063.8132315402845\n",
            "_ ==>  16  i ==> 17 Loss.     -1063.8132315402845\n",
            "loss1  -1063.8120386834853\n",
            "_ ==>  16  i ==> 18 Loss.     -1063.8120386834853\n",
            "loss1  -1063.8115931637292\n",
            "_ ==>  16  i ==> 19 Loss.     -1063.8115931637292\n",
            "stagnant loss\n",
            "pi is [0.29411765 0.47058824 0.23529412]\n",
            "loss1  -1235.6542449292815\n",
            "_ ==>  17  i ==> 0 Loss.     -1235.6542449292815\n",
            "loss1  -1204.8548902368648\n",
            "_ ==>  17  i ==> 1 Loss.     -1204.8548902368648\n",
            "loss1  -1182.255179295377\n",
            "_ ==>  17  i ==> 2 Loss.     -1182.255179295377\n",
            "loss1  -1173.8820430913893\n",
            "_ ==>  17  i ==> 3 Loss.     -1173.8820430913893\n",
            "loss1  -1170.2158998570821\n",
            "_ ==>  17  i ==> 4 Loss.     -1170.2158998570821\n",
            "loss1  -1167.2546742684403\n",
            "_ ==>  17  i ==> 5 Loss.     -1167.2546742684403\n",
            "loss1  -1163.7761335394407\n",
            "_ ==>  17  i ==> 6 Loss.     -1163.7761335394407\n",
            "loss1  -1159.1583331918064\n",
            "_ ==>  17  i ==> 7 Loss.     -1159.1583331918064\n",
            "loss1  -1152.2413326278238\n",
            "_ ==>  17  i ==> 8 Loss.     -1152.2413326278238\n",
            "loss1  -1140.34996132477\n",
            "_ ==>  17  i ==> 9 Loss.     -1140.34996132477\n",
            "loss1  -1123.0819977542346\n",
            "_ ==>  17  i ==> 10 Loss.     -1123.0819977542346\n",
            "loss1  -1104.2382261818266\n",
            "_ ==>  17  i ==> 11 Loss.     -1104.2382261818266\n",
            "loss1  -1088.6432995268829\n",
            "_ ==>  17  i ==> 12 Loss.     -1088.6432995268829\n",
            "loss1  -1079.8005679834414\n",
            "_ ==>  17  i ==> 13 Loss.     -1079.8005679834414\n",
            "loss1  -1074.8658694328356\n",
            "_ ==>  17  i ==> 14 Loss.     -1074.8658694328356\n",
            "loss1  -1071.094108111196\n",
            "_ ==>  17  i ==> 15 Loss.     -1071.094108111196\n",
            "loss1  -1068.2212680785774\n",
            "_ ==>  17  i ==> 16 Loss.     -1068.2212680785774\n",
            "loss1  -1066.2743523162537\n",
            "_ ==>  17  i ==> 17 Loss.     -1066.2743523162537\n",
            "loss1  -1065.075769189942\n",
            "_ ==>  17  i ==> 18 Loss.     -1065.075769189942\n",
            "loss1  -1064.4043551323084\n",
            "_ ==>  17  i ==> 19 Loss.     -1064.4043551323084\n",
            "loss1  -1064.0721542367396\n",
            "_ ==>  17  i ==> 20 Loss.     -1064.0721542367396\n",
            "loss1  -1063.9247837206842\n",
            "_ ==>  17  i ==> 21 Loss.     -1063.9247837206842\n",
            "loss1  -1063.861835072797\n",
            "_ ==>  17  i ==> 22 Loss.     -1063.861835072797\n",
            "loss1  -1063.8344053491264\n",
            "_ ==>  17  i ==> 23 Loss.     -1063.8344053491264\n",
            "loss1  -1063.8220494792777\n",
            "_ ==>  17  i ==> 24 Loss.     -1063.8220494792777\n",
            "loss1  -1063.8163436367481\n",
            "_ ==>  17  i ==> 25 Loss.     -1063.8163436367481\n",
            "loss1  -1063.8136690444126\n",
            "_ ==>  17  i ==> 26 Loss.     -1063.8136690444126\n",
            "loss1  -1063.812404703323\n",
            "_ ==>  17  i ==> 27 Loss.     -1063.812404703323\n",
            "loss1  -1063.8118041561302\n",
            "_ ==>  17  i ==> 28 Loss.     -1063.8118041561302\n",
            "stagnant loss\n",
            "pi is [0.2        0.53333333 0.26666667]\n",
            "loss1  -1246.0450397557986\n",
            "_ ==>  18  i ==> 0 Loss.     -1246.0450397557986\n",
            "loss1  -1224.0763188318072\n",
            "_ ==>  18  i ==> 1 Loss.     -1224.0763188318072\n",
            "loss1  -1193.7082268209765\n",
            "_ ==>  18  i ==> 2 Loss.     -1193.7082268209765\n",
            "loss1  -1168.8001410302586\n",
            "_ ==>  18  i ==> 3 Loss.     -1168.8001410302586\n",
            "loss1  -1158.117368791607\n",
            "_ ==>  18  i ==> 4 Loss.     -1158.117368791607\n",
            "loss1  -1149.49899945667\n",
            "_ ==>  18  i ==> 5 Loss.     -1149.49899945667\n",
            "loss1  -1135.355628628694\n",
            "_ ==>  18  i ==> 6 Loss.     -1135.355628628694\n",
            "loss1  -1121.4480229038895\n",
            "_ ==>  18  i ==> 7 Loss.     -1121.4480229038895\n",
            "loss1  -1110.8321038707486\n",
            "_ ==>  18  i ==> 8 Loss.     -1110.8321038707486\n",
            "loss1  -1098.24123882313\n",
            "_ ==>  18  i ==> 9 Loss.     -1098.24123882313\n",
            "loss1  -1087.2240072562668\n",
            "_ ==>  18  i ==> 10 Loss.     -1087.2240072562668\n",
            "loss1  -1081.2807096946456\n",
            "_ ==>  18  i ==> 11 Loss.     -1081.2807096946456\n",
            "loss1  -1077.5975345269726\n",
            "_ ==>  18  i ==> 12 Loss.     -1077.5975345269726\n",
            "loss1  -1074.6041353832109\n",
            "_ ==>  18  i ==> 13 Loss.     -1074.6041353832109\n",
            "loss1  -1072.0477500905045\n",
            "_ ==>  18  i ==> 14 Loss.     -1072.0477500905045\n",
            "loss1  -1069.8468993686427\n",
            "_ ==>  18  i ==> 15 Loss.     -1069.8468993686427\n",
            "loss1  -1067.946633405514\n",
            "_ ==>  18  i ==> 16 Loss.     -1067.946633405514\n",
            "loss1  -1066.4210323101634\n",
            "_ ==>  18  i ==> 17 Loss.     -1066.4210323101634\n",
            "loss1  -1065.337497776633\n",
            "_ ==>  18  i ==> 18 Loss.     -1065.337497776633\n",
            "loss1  -1064.653421552717\n",
            "_ ==>  18  i ==> 19 Loss.     -1064.653421552717\n",
            "loss1  -1064.2571520415386\n",
            "_ ==>  18  i ==> 20 Loss.     -1064.2571520415386\n",
            "loss1  -1064.0403826112984\n",
            "_ ==>  18  i ==> 21 Loss.     -1064.0403826112984\n",
            "loss1  -1063.9264494586514\n",
            "_ ==>  18  i ==> 22 Loss.     -1063.9264494586514\n",
            "loss1  -1063.8682756200446\n",
            "_ ==>  18  i ==> 23 Loss.     -1063.8682756200446\n",
            "loss1  -1063.8391781370979\n",
            "_ ==>  18  i ==> 24 Loss.     -1063.8391781370979\n",
            "loss1  -1063.8248296903548\n",
            "_ ==>  18  i ==> 25 Loss.     -1063.8248296903548\n",
            "loss1  -1063.8178220852299\n",
            "_ ==>  18  i ==> 26 Loss.     -1063.8178220852299\n",
            "loss1  -1063.8144217903987\n",
            "_ ==>  18  i ==> 27 Loss.     -1063.8144217903987\n",
            "loss1  -1063.8127790742114\n",
            "_ ==>  18  i ==> 28 Loss.     -1063.8127790742114\n",
            "loss1  -1063.8119878135456\n",
            "_ ==>  18  i ==> 29 Loss.     -1063.8119878135456\n",
            "stagnant loss\n",
            "pi is [0.23809524 0.33333333 0.42857143]\n",
            "loss1  -1244.3282168381033\n",
            "_ ==>  19  i ==> 0 Loss.     -1244.3282168381033\n",
            "loss1  -1212.002608784691\n",
            "_ ==>  19  i ==> 1 Loss.     -1212.002608784691\n",
            "loss1  -1177.5600312379831\n",
            "_ ==>  19  i ==> 2 Loss.     -1177.5600312379831\n",
            "loss1  -1160.9871837153798\n",
            "_ ==>  19  i ==> 3 Loss.     -1160.9871837153798\n",
            "loss1  -1154.4994048731648\n",
            "_ ==>  19  i ==> 4 Loss.     -1154.4994048731648\n",
            "loss1  -1147.8445915996167\n",
            "_ ==>  19  i ==> 5 Loss.     -1147.8445915996167\n",
            "loss1  -1138.4488504506317\n",
            "_ ==>  19  i ==> 6 Loss.     -1138.4488504506317\n",
            "loss1  -1123.329378516321\n",
            "_ ==>  19  i ==> 7 Loss.     -1123.329378516321\n",
            "loss1  -1104.126820332136\n",
            "_ ==>  19  i ==> 8 Loss.     -1104.126820332136\n",
            "loss1  -1091.0380940247833\n",
            "_ ==>  19  i ==> 9 Loss.     -1091.0380940247833\n",
            "loss1  -1083.9638451455075\n",
            "_ ==>  19  i ==> 10 Loss.     -1083.9638451455075\n",
            "loss1  -1078.838823541243\n",
            "_ ==>  19  i ==> 11 Loss.     -1078.838823541243\n",
            "loss1  -1074.492893458676\n",
            "_ ==>  19  i ==> 12 Loss.     -1074.492893458676\n",
            "loss1  -1070.761611943754\n",
            "_ ==>  19  i ==> 13 Loss.     -1070.761611943754\n",
            "loss1  -1067.8995990040303\n",
            "_ ==>  19  i ==> 14 Loss.     -1067.8995990040303\n",
            "loss1  -1065.999121342183\n",
            "_ ==>  19  i ==> 15 Loss.     -1065.999121342183\n",
            "loss1  -1064.8855366357238\n",
            "_ ==>  19  i ==> 16 Loss.     -1064.8855366357238\n",
            "loss1  -1064.3031702874985\n",
            "_ ==>  19  i ==> 17 Loss.     -1064.3031702874985\n",
            "loss1  -1064.0295690439457\n",
            "_ ==>  19  i ==> 18 Loss.     -1064.0295690439457\n",
            "loss1  -1063.9088495698966\n",
            "_ ==>  19  i ==> 19 Loss.     -1063.9088495698966\n",
            "loss1  -1063.8558109851047\n",
            "_ ==>  19  i ==> 20 Loss.     -1063.8558109851047\n",
            "loss1  -1063.8319622322042\n",
            "_ ==>  19  i ==> 21 Loss.     -1063.8319622322042\n",
            "loss1  -1063.8209897227582\n",
            "_ ==>  19  i ==> 22 Loss.     -1063.8209897227582\n",
            "loss1  -1063.815863128038\n",
            "_ ==>  19  i ==> 23 Loss.     -1063.815863128038\n",
            "loss1  -1063.8134456670136\n",
            "_ ==>  19  i ==> 24 Loss.     -1063.8134456670136\n",
            "loss1  -1063.8122994739692\n",
            "_ ==>  19  i ==> 25 Loss.     -1063.8122994739692\n",
            "loss1  -1063.8117542377777\n",
            "_ ==>  19  i ==> 26 Loss.     -1063.8117542377777\n",
            "stagnant loss\n",
            "Current answer for task Task 4 (EM) is: -1063.8114439062392\n",
            "-1063.8114439062392 [0.55092109 0.34171223 0.10736668] [[1.22311136 5.82977118]\n",
            " [0.92067873 0.96570772]\n",
            " [6.294515   4.42960688]] [[[ 0.94347549  0.06727821]\n",
            "  [ 0.06727821  1.10729601]]\n",
            "\n",
            " [[ 1.49853937 -0.37383191]\n",
            "  [-0.37383191  1.42740653]]\n",
            "\n",
            " [[ 1.71384777  1.39534081]\n",
            "  [ 1.39534081  1.32558449]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYd6tPHKBEEd",
        "colab_type": "text"
      },
      "source": [
        "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using a matrix $\\gamma$ computed on last E-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK_M6QLnBEEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "dd8e0369-682a-45a1-dfcf-afaeed948d82"
      },
      "source": [
        "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
        "labels = gamma.argmax(axis=1)\n",
        "colors = np.array([(31, 119, 180), (255, 127, 14), (44, 160, 44)]) / 255.\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colors[labels], s=30)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3wUZf7H3zOzPZ0UQhIg9I4IoYuAFHvHehasBxb0p56e9fQ87/TO885T7yx4dj0VCxZAuvReQwskISEhvW6yfWZ+f2yyZLO7KRBKwrxfL/7I7MwzzyzJZ77P9/kWQVVVNDQ0NDTaL+LpnoCGhoaGxomhCbmGhoZGO0cTcg0NDY12jibkGhoaGu0cTcg1NDQ02jm603HTuLg4NTU19XTcWkNDQ6PdsnXr1lJVVeMbHz8tQp6amsqWLVtOx601NDQ02i2CIOQEO665VjQ0NDTaOZqQa2hoaLRzNCHX0NDQaOdoQq6hoaHRztGEXENDQ6Odc1qiVjTaH+szy3hufjqZJTUkRpl48qIBXD4s6XRPS0NDA03INVrAoWIrd364GbtbBuBopYPHv9lFpEXPxL7ekNbiagfrMsuItug5r3ccOklb7GlonCraRMgFQfg/4G5ABXYDd6iq6miLsTVOPx+uO4zTI/sds7tl3lpxiIl94/lo3WH+vGAfOlEAASJMeubNGktKjOU0zVhD4+zihM0mQRCSgTlAmqqqgwEJuPFEx9U4cyiscqAEKVtfYnVypNzGnxfsw+lRqHXJ1DplSqqd/O7rXQHnuzwKRyvtuGXlFMxaQ+Psoa1cKzrALAiCG7AAR9toXI0zgOmDEll7qMznWgEwSALTBnRmxYFiBMH/fFlV2ZhdhqyoSKL3w/fXZPHa4gwUFSRR4ImL+nHLmO58sPYw7/yaSbXDzYQ+8fzxysEkRplO5eNpaLR7TtgiV1U1H3gVyAUKgCpVVRc3Pk8QhHsFQdgiCMKWkpKSE72txink6nOTGdYtGotBQhIhzCCR0snC/Rf0JsygQ2ys5IBeEqnTcJbtK+LVXzKodcnY3TI1Tg9/XrCfJ77ZxZ8X7KPI6sTuVli8t4hpr63E5dEsdg2N1tAWrpUY4EqgB5AEhAmCcEvj81RVfVdV1TRVVdPi4wNqvmicweglkc/uGs27t6bx6PR+/P36Yfzy8PlEmfVMH9QZqZGQm3Qi143oilB3/IO1h/2sefD62OdtzcPTyGdjdcq8vizj5D6QhkYHoy1CC6YC2aqqlqiq6ga+Bca1wbgaZxCiKHBenzjum9SbiwYnoq+LSokw6fni3jH0jAtDLwkYdCKXn5PEs5cP8F1b4/QEHTOY3x1gUXphm89fQ6Mj0xY+8lxgjCAIFsAOTAG00oZnEYOTo1j+2CQqal2YDRImvYTbI/POr5l8tP4w5TUuJAHkBsJtNkh4ZAW3HKjmZr106iavodEBOGEhV1V1oyAI84BtgAfYDrx7ouNqtD9iwgyoqspbKw7xzyUZuIOY3GEGEVmBqf0T6BZr4a0VmX6fS6LAbWNTT9GMNTQ6Bm0StaKq6h+AP7TFWBrtmy83H+GNZQeDirhFL3Hz6G7cMLIrvRMikBWVg8U1LN93LPLlosGJXDsi5RTPWkOjfaNldmq0Ke+tzsIRIurEo6ikxFjonRABeK3vd29NI6ukhoyiGvp2DqdnfPipnK6GRodAE3KNFrH3aBX/9+VOMktq6BRm4P+m9eWmUd0Czgu1sQngURRG9egUcLxnfNsK+Kbscl5bfICcchujUjvx2IX96NpJyzLV6LhoQq7hh6qqrD5YytJ9RXQKM3BdWlfKrE6ufGst9c6SYquT5+anIwA3NhLziwYl8vH6HIIFpIQZdOSW23j710w6WQzcPLobfTpHNDmX+gSilrIxq4zbP9iEw+1dFfy46yi/ZpSw7NGJxIYbWzyOhkZ7QlDVEDFgJ5G0tDRV69l5ZvK7r3fy8+4CbC4ZvSSgl0TCjTqKrc6AcztHGtn41FS/Y9UON1NeXUlJjcvvuAhEW/TYXDIOj4IkgEEn8u9bRjC5X4LfuS6Pwks/7+XLzUdwyV4r/m8zzmmRVX3d2+vYfLjC75hRJ/LglN48MLlPC78FDY0zE0EQtqqqmtb4uFaiTsNHen4VP+3yijiAW1axueSgIg5QXusKOBZp0rPs0UkkRprQS15LWgAUoNzm9vnPZRXsboWnvt1NY2PiDz+k8+WWIzg8CorqdZXMeHtdi2q05JTZAo45PQoHi2qavVZDo72iCbmGj605FSitWKH1CuHXjjTrWf7YRP5w+UC6d7IE1GJpSHG1w+dX35ZbwdX/XssXm474XCPgTRyqdXpYfbD50g4jusfQ2BNj1kuM6xXb/ANpaLRTNCHX8JEUbUYnBapuKB2ONOvYlVcZ9DOLQcctY1KpdXlCZnCC1/9tMejIKLLym/c2sj03+HiKChW17uYegScu6k+4UYdB5/3VNuslusVauHJYcrPXami0V7TNTg0fk/rF0ynMgNPt8NVAMeslXB6ZIAmYbMqu4Pp31vP2LSMY0CWSCpuLXvHhvvR98Kbwl9YEumDq6RxpQhIF3vk1M6DmeUNkRWV877hmnyE1Loylj07ksw25HCquYVzvWK4dnoJJyxbV6MBoQq7hQy+JfHffeF5euJ9l+4qIMuuZNbEXz/+wBzlEbLjDrTD7023IqopeFNBJIv+8YRiT+3s3MH97fk9e+HFvQNEsAFHA12Eop8wW1HIX8G6KPn5R/xaXt02IMPF/0/oCXr//DzuO0ishnOHdon2FvDQ0OhKakGv4EW32tm/TSyLdYy1MG9iZ7bmVfLc9H1eIzcZ6kfba3TKzP9vKr7+bTOdIEzeM7EqV3c3ryw76NlEBBMFbb+Xe83sB3tXA7vwqnA1eGDpRYGyvWF66agjdYlsXBy4rKrM/3crqg6Ve15AAQ5Kj+OjOUZp1rtHh0IRcw4dHVrj5vY2kH63C5pIx6kT+veIQX9wzhrxKG5uyyoOm3jdGVWHh7gJmju+BIAj8dmIv7pnQk935Vbyx/CD7C60MTo7isel9fQJ9+7hUvt2eT2GVA5tLxmKQiA038OZNw4my6Fv9LN9tz2f1wVK/lcDOvEo+XHeYWRN7tXo8DY0zGU3INXws3lvkE3Hwhu25ZIXXlmTw2d1jeOrbXXy+6Uiz46gQUGdcFAXO6RrN3NtHBr0mwqRnwZwJ/LSrgN15lQxKiuLyc5IwG6S6uci8vvQg87bmAXB9WlfmTOnj29RszPwd+QHuHIdb4YcdRzUh1+hwaEKu4WNdZqmf+wO81vXmw+Vsz63gu+3+HfwEIC7CQKXN7VeO1iMrbM4uZ1BSFGN7xaKqKuszy/hsYw4FVQ5GdI/hjvE9SIo2+41n0kvMGJHCjCBFsx74fDurMkp8rpf3VmdxqKSGt28ZEfRZoi16BAjIMI0yt96619A409GEXMNHamwYJr3oF8MNYHV6+O/abByNLFwVsLtkBiVFcaDQiqwouGRvWv0ve4tYdbCU2ZN6si6zjI1Z5T5R3Z5byecbc/nhwfN8seiqqrL9SCXZJbUMTYnyS93Pr7T7iTh4Vwsr9hdTUGWnS5T/CwFg5rgeLN1b7GeVm/US95zf4wS/JQ2NMw8tjlzDx4wRKRilwF8JVYXFe4rQBal5YjHo+P7+8Xx29+gA69fulvnXskNszi73+0wFbC6ZV3854D3PJTPj7fXcMncjz85P5/I31/DoVztQ6twzhVV2DEHmpZdECqscQZ9lRPcYXrl2CLFhBvSSQKRJx9OXDuCC/p1b9F1oaLQnNItcw0e0xcBfZ5zDfZ9vQ27k49aJAh4VGjorzHqRO8anen8QwKSTcMv+1Q+VusJXjVGBHUe8yT9v/5pJeqOIlZ93F9A1xsLt41LplxiJWwmMmKl1evh+Rz494sKIthgCPr9iWDKXDU3C6vAQbtK1qviWhkZ7QrPINfzo0zncVyOlIYoKL10zhORoM3pJwKQXuXVsKr+tCx9MiTHjDBKe2FSMS+8Er1vl2215fiIO3o3JN1ccYsxflvHh2myeuXQgJr3ol2WqAp9vzOXKt9aGTCYSRYEoi14TcY0OjWaRa/jRMz6cvp0j2Hu02hd5ohMFesWHMWNECtcOT6bS5iasQRo8eJNwrjgniZ93Ffj80jpRoGd8GDlltTg9/pJukEQend6PNQdLyauwB52LR1HxKCpvrcjkwztG8u3s8dz47nqqHcesfresUmJ1sii9UEvD78CU2EpYnLMYRVWY0m0KSeFJp3tKZxSaRa4RwAczRzKuVxw6UfAl5Xx45ygABEEgJszgE/GdRyp5f002i9ILePHKwTw2vS/hRslb8VBVyauw09BQF4BBXSL4ZvY4zkmJ4rGvdzZptQM43DLfbs9nYFJkwEYseP3tWSW1bfPwGmccq/JWccm3l/CPrf/gn1v/yRXfX8HPWT+f7mmdUWgW+RmI3SXz/tosFuwqpFOYgd9O7MmEPvGn7P6x4UY+vmsUdpeMiorFEPhroqoqj3y1k0XphXgUBYNOJNps4HcX9kNRvW4PVSUgnNGoE5g6MJEhKVGUWJ2U20LXYalHEECqS63v0zmcPUer/T63GCSGJEcd/wPXYXW4sbtk4iOMWir/GYJbcfPk6idxyP6b2s+ve57JXSdj0Wudn0AT8jMORVG58b31HCiw+mp3b82p4KWrB3PN8FPblLg+GScYv2aU8MueQp8bxS3LONwOXl+WESDeDXF4VOauzuKhKX2IMOlatCQ06ESuH9kVgBeuGMRv5m7086k73HLQqJaWYnN5eOSrnSzbV4QgCCRHm3nz5nMZlHTiLweN4FQ5q3h92+ssz11OuCGcmYNmcm2fawNeoFmVWchq4O+TTtSRXprOqC6jTtWUz2g018oZxvqsMg4W1fg1MLa7Zf68YF9AA4ZTgaqqQe+7bF9xgGDLikpBpSNomGJD7G4Zt6Jg0kvcOrY75ga1T3QiRJp0GHXezkRmvcQzlwxgWNdoAIZ1jcbYKJtTUeH+L7bhClHYqzl+/81uVuwvxi2ruDwK2aW13PTeBuxNvJA0jh9FVbh94e18f+h7yhxl5FTn8NfNf2Xu7rkB50Ybo3ErgeWLPYqHWLNWY74ezSI/wzhYZA0I/QNvNx6XrGDUta7gk6qqfLM1j//8mkm1w8PUAZ15/MJ+VNnd7C+spndCuK+rfUMyCq089OV2DhRaEQSBiwcncu3wFF5fdpDMkhoijDpvSGKjucZHGMirCB7bXU+fhAjfczx58QA6hRn5cF02DrfC9IGdefrSASgqHK200ys+3G9lsL/QihzkxaKqsDu/ihHdY1r1/Tg9MgvTC/wyU8G7Mlq+v5hLh3Zp1XgazbOhYAMFtQV+Am332Jm7ey53Dr4TSTz2/905rDOjE0ezsWAjLsXrhtOLevrF9KNXtFZqoR5NyM8wBiVHBQ2V6xxpOi73wXurs/jHkoM+F8jXW47w486juDwyBp2ER1aY1C+BN28+F13d+GsOlnLbfzcei/9WVRbsLuDn3QXUa6jV4Qm4l1kvcf/kPvz+291B5yKJAiadyF9nDPUdE0WB2ZN6MXtS4B9lp7DA2PBwoy7oi05WVCJMrf91lpXgce6K6nW5aLQ9R2uOoqiBqyeX7MIhOwgTw/yOvzrxVV7Z9AoLshegqApTu0/l6TFPn6rptgs0IT/DSOsew/BuMWzNqcDulhHqEm2ev2JQqzfgVFXljeWH/NLUPYrqa63mqkve+TWjmC825XLr2FQURWXO/7YHiFswsRMFiDDqqXK4iTbreXBKH24Y2ZW//XKAskb9PCVRYMbwZB6/qD8xFgOL9xSyML2Q2DADlw7tQkGVgyiznjE9Y5uM+U6NC6N/YgTp+f7hkT3iwuiTcKz1nFtWkBW12ZK1FoOOc1Ki2HGk0u8ZZUX11VTXaFuGxQ8LejzBkoBFF7h5adFbeGH8C7ww/oWTPbV2iybkZxiCIPDBHSOZtzWPn3YdJS7cyJ3je3BOnY+4NTg9CrXO5q1Ku1th3tY8bh2bSrHVidXRfEs18Ip7vy4R/O+eMYiigFqXxfnY9L48/X26nzAqikpyjJnYcCMPfbGdJfuKsLlkBGDummxMehFJFIgy6/l61jiSowPrp9Tz35mjePh/O1iXWQrAqB6duHZ4Mpe9sYaiagcmvURhlQMV74vxtRuG+Y13tNLOl5uPUGJ1MmVAAv+4fhg3vLeBGocbEHDLCi9fM4S4cGOLvgeN1tE7pjeX9bqMn7N+xu6xoxN06EQdL4x/QYsWOk40IT8D0UsiN43qxk2jurX4mrIaJ99syyO/0s75feKZ3C8Bk16iWycLh4N0lm9MvR860qwL2aOzMUadyOR+8bgVhRfn7+WrrXl4ZIWhydEBY6jAf1ZmcV7veBbvLfKtEuq1vj4+3OFSeOTLHXz527Eh79spzMDHd43C5vKgqt4VxaNf7QrahWjL4QpueGc9q343GVEU2J5bwW/mbsQjewt8fb8jn8n9E1j9u0lszK7A6nAztlds0JR/jbbjuTHPMa3bNJbmLiXaGM3Vva+ma2TX0z2tdosm5B2Ag0VWrvnPOlweBadH4esteYzpGcvc29L4yzVDufPDzXgUBbesYpDEgE4/Jr3IHeO9VQEtBh03juzGF5tz/TYABbwCanfLvsYPXaJM3Do2lSe/2c2Pu476zt8RoiFz/cai2kQKkKyqbMmpwOmRm93YrY9v/9svGUFFvH68CpuLzYfLGd0zlqe+2+0XbWNzySzfV8yu/GrO69N8T1CNtkEQBMYlj2Nc8rjTPZUOgRZ+2AH4ww97qHF4fLHVNpfMhqwyVh8qZWyvWN65dQRmvYQkeGuFN6aTxcD0gceqAj53+UDun9ybThY9Rp3IsK5RLHnkfF6ZMYQosx69JNAlysSr152DKHibODSO+giGosKXm480+0snCYIvAaglhKqAWI+AQJXdjaqq7CuwBnzuURS25pS3+H4arafWXcue0j2UO7Tv+WSgWeTtHFVV2Z5bGWDj2lwyG7PKmNA7jie+2YXV4QlpBx+tcjDypaXMHJfKrIm90EkiD0/ty8NTvQ2MPbLCT7sKeOKbnb6aKZkltfxm7kbevmU4LdBwHx5FJcKsx6O4cAW50KAT6Rkbxj0fb2F0j07cPKY7kaamm0EMTYliY3ZogXB5FP6+OINZn25FEKBx9KJBJ5ISo2UIniw+2vMRb2x/A72oxyW7uKLXFTw79llEQbMj2wpNyNspB4usPPHNLrbnBndjmPUiyTFmdudXUW13N1vPpLTGxZsrDpFbbuOvM87xHV+8p5DfzduF1eEOiFxxexT+t+lI0E48obC5ZK4aloRRL7EovRCjTqTG6cHq8NSVvFU5VGJlf5GVdZllfLIhl4UPT2hSzJ+/YhAz3l7nS+gBb0SNSSfVFd5SOFAUaImDN+Il2mxg6gCtTvnJYEvhFt7c/iZO2YlTdgLwU9ZP9I3py00DbjrNs+s4tImQC4IQDcwFBuP9m75TVdX1bTF2R8fhltmeW0mEScegpMgW7dpbHW5mvL0+pECLgrdt2hXnJJFRVNPiSACHW+H7HUd56pIBRFsM5JbZmPO/7UELVQG4FRWr041OElrkWqnnm235fDN7HH+4fBDgXVWUWJ28+NNeft5d4HthOD0KpTVOPt+Qw6xJvUOON6BLJMsfncQXG3PJq7QztmcnREGgoMrB60szgoZOGiSBCJOeyf0TeOKi/iF7f2qcGN8d+s4n4PU4ZAdfZXylCXkb0lYW+evAIlVVZwiCYAC0dWoLWLaviDlfbEcUBGRVJSXGzKd3jyYhwtTkdQt3F+KWlQARFwVvwszYXrE8c+lAIkx6X0p7jTPoUIGokFNmI9piYP7O/KDJN/WY9RIXDkqke6cw5m3N8ysr0BROj8KfF+zj83vGAN6NL6dHYem+ogDRdXoUNmaXM2tS02N2jjTx8LS+fsfu+GATzhAvmCiLgc1PT8XqcLP2UClGncT43nFU2d0cLLKSGhcW0FNUo/XIqhx0cztYQpDG8XPCQi4IQhRwPjATQFVVF9B8SbuznIpaF/d/vs3P2s0sqeGRL3fy6d2jm7y2rNYVtK6IIAjsev5Cv2OSKPDhHaOY+cEmHB4Zj6wGNHFoiEtW+O0nW/j3b4bzS3phSEvbYpDoERfGjBFdMUgivRLC+eOPewP+ZE06MajAN6xgaHN5uOqttdiDWP4GSWRgUmTI+TbFpib85gMSI/h511Ee+3oXkuj1m3sUBUX1rmZcHoXLhnbhrzPO0ZpSnABX9b6KZTnL/KoXmiQT1/S55jTOquPRFuvJHkAJ8IEgCNsFQZgrCEJY45MEQbhXEIQtgiBsKSkpaYPbtm+W7y8OiMyQFW/RrMZNjhszoU8cukZdfEQBRqV2Cnr+kJQoNj41hY/uGMVd5/XArG/6v73E6uT6dzawt1G52Hriwg28dNVgvpk1ji2HK/hsUy61Tg8WY2C4oMOjEEwHe8Qd+xVZsLswZDq8ThK4fWxqk/MNRafw0LHgW3PKuf/z7djdMjVOmVqXjNOj4pZVrHURQAt2F/K/TbnHdW8NL2O6jOGuIXdhEA2E68MxSAYu6HYBvxnwm9M9tQ5FW7hWdMBw4EFVVTcKgvA68Hvg2YYnqar6LvAuQFpa2qkv43eG0ViI6xHw1t9uisHJUdwyujufbsxBVryx4Ua9xF+uGdLE/UTSUjvRMz6cD9YebnJ8ub6YeAhqHB7WZZYxd002h0trfany7iChjRCY3m/Si/z+4v6+n4uqHSH98HHhRhIim3Y1heLhKX155vv0oDHmta7ml/Z2t8z/Nh/hN2O6H9f9NbzMOmcWN/W/iYyKDLpGdCUxLPF0T6nD0RZCngfkqaq6se7neXiFXKMJLuifEOCG0EsCk/sltKjC4TOXDeTaESmsPVRKXLiRCwclNlk/vJ5OYQb+c8tw5nyx3a9lWmtweBTmbcur6wLU8uvCDCKje8bx4AW9ObfbsSqFI1NDVyzMLbdRYnUSH9H6dPnz+8YzukcM6zLLkFWa9PeHQnOrnBj1G51RxihGJo48zbPpuJywa0VV1ULgiCAI/eoOTQH2nui4HZ0Ik54P7xhFbJgBi0HCqBMZ3j2GvzUI/WuOAV0iuXtCT646N7lFIl7PpH4JbH56KqZmXCxNoaqtE3HwNpV4eGofPxEHqLa7m1yF3PXh5lbPr7zWxcWvr2JtZlldvHrrRdysl7hVs8aPiwpHBbOXzmbMZ2MY89kYZi2ZpSUDnUTaKmrlQeCzuoiVLOCONhq3QzOqRyc2PT2VQ8U1RJh0pyxKIqukhlvmbgy5kTmlfwJrM0txugMjY04EWVG54s21/OnKQdzSwO+9dF9xky+FjGIrOWW1dI8N2HoJyUfrDmN1eHzPWO/1aRzzLnDMzZUSbeZolQO9JOKWFW4e3Y1rhp+dDZ1VVT2hAlb3L7uffWX78KjeVd/Ggo3ct/Q+/nfZ/9pqihoNaBMhV1V1B5DWFmOdbUiiQL/EwMYOJ5N7P9lKQbUjwA2uE6F3QgT/vmU4+wqsvLHsIEcqbEQYdewpqA7px24tz87fgyQJ3DTKa+0mRBjRNxGLLgpCk+3jwOuf319gJdqip2snCzuOVAaNzjHpRWTVmwhkMUi8dfNweieEo5NEosx6apweDpfW0jXGQpSl6YzSjoaqqny05yPm7p5LtauaQbGDeH7c8/Tr1K/5ixtwuOowBysO+kQcwKN6yKzMJLsqmx5RPQKuKawtRBREEixa6eDjQcvsPMvIr7RzpNwWRMQF7jqvB49M74tRJzGsazTvzzzm09yUXc57q7NYsb+I4+yo5kMFXll4wCfkw7vHNGmRhxt19Osc+mW38kAxc77w1lB3ywrDukVzbtdoNmSV+Ym5KMDUAZ159vKBVNs99IwLQ2zkAw836hjcBo2c2yOf7f+Mt3a+hcPjDRVML0tn5qKZLLxmIdGmlpdRrnHXeLv8NHr3SqKE1eWfYZtbncucFXPIs+ahqir9OvXjn5P/qQl6K9HS2c4yQhWjMuklJvaLD9hoLaiy88Dn27jn4y0cLLLSKczQbE/OllBl99Y8X3mgmFmfbkVppOQGSSDcqCPSpOPd29ICBLeeshonsz/dSrXDQ43TGza4LaeCrJIawo069HXXSYK3VO9DU/uSEGGid0J4yDHPVj7Y/YFPxOtxK25+zv65VeP069QPSQjcsxEFkQGxA3w/K6rC3YvvJqsyC6fsxKW42Fu2lweXP3h8D3AWo1nkHZhVGSV8vjEXWVG5fmRXpg5IIDHKxIAuEezOq/IrdmXSiwFx6A63zJVvrqW0xomiesXXpBfp2zmMvQU1AfdLjDTiUVRKa5rPB1OBLzbm8O+VmQEuG7Ne5P7JvRnQJZLxveN8XX5yymr5fvtR3LLMJUOSGJgUyeK9RdCo+rlbVlm+v4S1T1zAu6uz2Hy4nAGJkcye1IvUuJb72c82rO7AejQu2UW5vXWblHpRz98n/Z05y+cg1P3fKKrCqxNfRS8ec1ell6ZT5azyy/yUVZnMykzyrHmkRKQc55OcfWhC3kF5a8Uh3mzQ5m1NZim3jenOk5cM4N1b07j7oy0cKLIiCgJxEQbm3jbS17OznkXphVQ3KpblcCvkVzoJM0rUOv3XzhU2N1MGJLAovbBFES3P/7g3qB/b4VbQiQJTGhSy+iW9gDn/24FHUVFVlblrsnlser+6muTBbxYfYeTZywY2PxENAEYljmJ1/mq/9HmTzsT45PF+55XYSph3cB5HrUcZnzKeqd2mohP9pWRMlzEsvW4pq/JWAXB+yvlEGvwzdJ2yM+iGqiiIvkbLGi1DE/IOSI3Tw7+WHfQTSbtL5sN1h7nn/J4kRJr44cHzyKuw4fIo9IgLC/oH9dOuo0E3OKvtbixBwh1VFe46rwe/ZpRid3lQVK9LQxLArQTKbahSASr41Q3PLqlh9mfbAl4of/vlAD8/eF5gPL4oMGVgZ8110kqeGv0UN/98Mza3DafsREVFQGB1/moGxA7ArDOTUZHBbQtvwy27cSkufsn5ha8PfM270971+sUbEGmI5LKel4W837D4YUFL2UYbo+kRGbghqhEazeZGywsAACAASURBVEfeTpm3NY+xf1lG76cWcMUba9jVoCvP4dJa9FLgf61BJ5LRoJxrSoyFnvHhQUXc4ZZZfbA06L17J4T7sjkbkhBpZHi3GK451xuyJwreWHO32voo7i05FYA3kmLmB5tDVDAUOVJp5183nkuYUSLcqMOslxiUHMXLTWS5agQnKTyJRdcuYnSX0YiiiIqKzWPjkz2fMHvpbABe3vQyte5an8Vs99hJL01nTf6aVt9PL+l584I3CdeHE6YPI0wfRowxhjcueEPr3dlKNIu8HbJgdwHPNkg935VfxY3vbmDpIxNJijaTEmMOmi5fb33XU17r4p9LM1i+v5j4CCNzLujj6xyfV2EnRBUBbh7dDZtL5o3lB3F7FHSSt3Hya9cPY0tOBfO25vkL73EEo9dvqGaV1lJUHbwDkEtW6BJlon9iJFufmUZ6fhXRFgO9E8Jbf0MNAFRUNhRswKMcCx10Kk72lu1lb9le0kvTA66xeWxsLd7KxK4TW32/4Z2Hs/KGlWwt3IokSgzvPNzPj67RMjQhb4f8a9nBgPohHlnhf5tyeWR6P6ItBm4Z053PN+b6zjPrJS4d2oUuUd6kI6dH5sq31lBY6cCtqORV2Jn92Tb+du1QLh+WRGKUKWjnH6MkMKlfAj3iwpgyIIGle4sIM+q4bGgS8RFGnpuf3mzRr+Yw6QR+M8bbeFpW1JAuEp0kUF63sWrSS6SFKBqm0XIqHBVBj4uCyBHrERIsCeRU5/h9ZtaZ6Rpx/I2TjZJR6915gmiulXZCaY2Tzzfm8tnGHEqsgcXFXbJKYQPL9ZlLB/DiVYMZ1jWaIclRPH1Jf24Y2ZVN2eW4ZYVf9hRRXuPC3cB0drhlXl60H/DGU98xPtUv9d+sF5nUP8Fn1fdPjOSBC/pwx/gexEcYqXF6u/ycCDoRrjo3hbvO6wlAn4RwYkJ0tK91ytz10Ra+3ZZ3QvfUOEaCJQGjFFjXxqN4GBI3hDnnzsEkHStiJiJi0Vm4pMclVDoq2Vu2F5vbdiqnrIFmkbcLVuwvZvZnW+tCubz1xOv9z/VYDBIX9D+WRCEIAjNGpDBjRAqHiq3cMncTVqcbAQG9JHDx4MSgVQGPVtopq3ESG27kiYv60zshgg/XZeP2qFyXlsLt41IDrrG7ZB6bt5PF6YV+L4bWIADXpSXzuwv7E9+gsYYgCPx35khufX8jVoc7oGa53S3z0s/7uGpYsra52QboRB3PjHmGZ9c+i0t2oaBg1pmZ0WcGSeFJJIUnYZAMvLXjLUpsJYzuMpqZg2bywvoXWJqzFINkQFZk7h92PzMHz2zyXmvy1/D6ttc5WnOUoXFDeWzkY/SK7nVqHrSDIagnaEEdD2lpaeqWLVtO+X3bIy6Pwog/LcHaqFKhgDfBxeGWMeklxvSM5b3b0gKq9amqyqRXV5JbZvNzVYcbdciKEiCMAhAbbmDRw+cTF96yioOPfLWDH3cebbLdm0ESGNAlkj1Hq4NulII3ugVBIDbcwPUjUrh3Yi9fr05ZUXll0T4+XHs4oGmzJArs/MN0wo2aXXKi7Cjewcd7PybPmodFZ0Ev6cmtzqXYVkxqVCqPj3ycsUljAW+y0HNrn2Nh9kJk1d8oMEkm3pzyJqO7BG+Ssv7oeuYsn+NrOCEgEKYPY/5V87WsziYQBGGrqqoB5VA018oZzr6CaoK9bC0GiXsn9OCRaX2Ze1sa798eKOIA2aW1FFc7A/YbVVWti8FudByodnj475rsFs3PLSv8uKNpEQeY1D+B+Q+cx+8uDF23o77UbHG1k/+szOTKN9f6/O2SKHDhoC4Bse4A0WY9Ya2o/qgRnCWHl3DP4ntYmrOUfeX72F26m40FGzlaexSP6uFQ5SHmLJ/DntI9ALy7612W5iwNEHHw9uX8JuObkPd6a8dbfl2DVFRcsouvD3zd9g92FqCZMGc4MRZDUAtWVlWuGJZMz/imIzR0dWFkjbG55JCbki6PwtYc/02vZfuKmLs6G6vDzVXnJnPb2FQMOhFZUZGbWdWZdCKjUzvx8sJ9fL/9aJPn1iOr3oYTC9MLGNcrjvJaF0OSIxneLYatOeW+lYRZL/HcZQO1cLUTRFVVXt78sp+4BkvKccpOPtjzAa9OfJVvMr7xO78xDYtmNaagtiDgmEtxkWvVOjIdD5qQn+F0i7UwNDma7UcqfFavXhIY2CWyWRGvv75HbBgZRTUBghvKiNZLgl/hqA/XZvPKogM+n/qhkhpWHijh07tHY9JL9E+MYG9BYHo3eGPJI8x6/vbLgRY3Z67H5pJ5bUkGv/9mNzpJQCeKvHrdOVw4qDM/7DxKpzADd53Xk1E9tGiVE8WjeCixNd+CUUWloMYrwsEs8XrMOjNX97465OcjO49kYfZCFBS/a8YladErx4PmWmkHvHd7GhcOSkQnCuhEgWkDOvPBzFEtvn7uzJH06RyOSS9i1ktEm0PH6UqC18q9e4I3s84jK7y6OMNvY9Th9lrsu/OqABiaEroyXpRZT2WtK6SIi0AQb4l3LiIUVDpwehRqnTJVdjcPfrGNCwcn8vWscbxza5om4m2ETtT5RaOEwigZmdR1EgCX9LgEgxgYUaQTddwy4BYmpEwIOc5Dwx8iyhjlu6dZZ6ZPdB8u7nHx8T3AWY5mkbcDosx63rx5uK9CYGujM5KjzSx6+Hxyyrz9NfPKbcz+bFtAjW+DJHL18GQemNzbF29eaXcHTS4SBDhYbGVISlRd4argVNvdIS1/gIn94ukcaeLbbfm4GtzHKAm4ZDXQLaTCDzuOIgkCKw4U0y3Wwp3je7RodaIRmjxrXsj6JmbJjF22Y9FZSApP8jVOnjN8DhkVGewq2YVO1OGSXVzd+2ruO/c+OpmafsF2Ce/CT9f8xA+HfiC7KpuRiSOZ0n2Klgx0nGhC3o440fC6+g47PWLDOKdrNDuOVGJ3yUgiGCSJj+4cFWDhxlgMmPVSQF0Up1um0uZCritiFQydKGDSS9Q4Q/tKVx8s5dax3fl5zngWpheyObsCu9vDlAGdeW1JRsAmqqwovLXiEFaHB4+iIh6Eb7bm88W9YxjWteU1szX82VW6C6NkxObxjwHXCTqeGPUEhbZC+kT3YXK3yT6xNevMvH/h+2RVZVFYU8iguEFEGVteyz3SEMktA29p0+c4W9HCD88SiqsdvPDjXlZmlBBulLhjfA/iwgws3ltElygTM0akcKTCjkESOa9PHDaXV6i7x4bx/fb8oN3oLQaJUT06kRobxhebcv3EXgBemTGUw6W1/HdtdpPdhcx6iT9cPpAbR3XzO/7g59tYtKfQT8wbt2qrZ0T3GL6ZrflXj5ftxduZtWRWgJCbJBNrb1qLQQqelKVxagkVfqhZ5GcBblnhmv+so6DKjqxArdPD60sPMmtiT969LY0VB4q5/p0NXotf9SYcCQLoJa9P/fUbz+Xju0bx20+2Ul57bPltc8lsyi7nplFdySypYfPhcvSiiFtReO6ygVyf1hW3rFBW6+L77fmAiqwQEIVjd8t8vD4nQMj/dNUQjlTYOFBYg04UcHoUPIoS0N0IYH9B9cn46s4ahsUPo3tkdw5VHsKteJt+mCUzvxnwG03E2wGakJ8FrDxQQnmtk4aubrtb5t3VWdx9Xk/u/2xb0CxPtyxjc8nc8/EWVj0+GXuQvpk2l8zOI1V8ctdocspqKaxyMCg5ypeco5dEXrl2KM9eNpBqu5s//bSXBemFAeN4lECLPcqi5/v7zyOjyEqp1cmXW44wf0fw8MXWNGbWCEQQBP574X/51/Z/seTwEsx6M7cMuIWb+t90uqem0QI0IT8LOFBYjc0VKJQ2p8y6rLKQ7d/qUVFZmF5AfISR3HL/pbfFING1kwXwimkoQQ036gg36rgurSsrDpT4vThMepHr00IXXerbOYK+nSNYl1katEmzADx5Sf8mn0GjecIN4Tw1+imeGv3U6Z6KRivRwg/PAjZmBW/VlRhlIsaib7bQlaJ6k4QendYXs/5YBqVYF6p4+TlJIa89WGTl/TXZfL89H5vLw6R+8dw+LhWjTiTCqMOoE5k6oDMzg9RwacxNo7sH1FkXBXh0el8m9Ilv9noNjY6KZpGfBexo0HSiIfER3kYQ0RYDdrc9ZHs2EZg+MJFusRb0OpF/LMmgpMbJuF6xPH3pwJA1Tl79ZT9z12SjqN6uPS/8uId5s8fx+4v7c+d5qRwotJIaG+az6JsjOdrM5/eM4dnvd7O3wEp8hJHHpvdjxgitt6PG2Y0WtXIKqXa42Xu0mi5RplPq003705KgDZHDjRLpL1zEkXIb93++zVfXxaiXUBRvfRO3rPDCFYMCNiKb4pc9hby2OIMDRf7ZnoIAI7rFME+LLtHQOC60qJXTzIfrsvnLgv0YJBGXrDC2Vyxv3zLC1yH+ZJLWPYZFewKTdmx1zZO7drLwwwPnUVHrQpIEIow60vOrKa1xMrx7DFFNZII25ouNOfzxp31BN09VFbbmVqA00SyiNeRX2lm+rwijXuLCQYmtmufZTIWjgjX5azBKRiakTMCsM5/uKWmcIJqQnwLS86t4ZeF+nB7FF2u9PrOMfyzJ4MlLBpz0+98xvgfL9hcHbBI2rKcCEBN2LMxsSErLEzvqUVWVvzVK52+MWS/y/ppsfth5lBiLnnvO78l5veMAWlX46ouNOTz/414EvIlSL/y4h0/uGs3wbjGtnvfZxC/Zv/D02qeRBAlBEBAFkbnT5zIwduDpnprGCaBtdp4C5u/ID8yM9Ch8uy3/lNx/VI9OTO6XgKWu1KtBEgkzSLx41eA2vY9HUamoDZ7mDd4qiNEWA39fcoDd+VWsOljKzA820/fphfR6agG3vr+Ro5X2Zu9TXuvi+R/34vQoODwKNpdMrVPmwc+3h8wyPVtxyk7fd2J1WXlm7TM4ZSc2j41ady1Wl5VHVz6qfW/tHM0iPwVIore3T2NOVeVVQRB4+5YRLNtfzPL9xXSOMHLDqK6+eirHS1ZJDcVWJ0OSowgz6tBLIl07WQJCFMEr4pcO7cLC9EK/LE9ZUam339dlljHjP+tY9fjkoHXH69mUXYZeEgNejqU1TgqrHSf8XB2BzYWbeWHdC+Racwk3hDNr6CxSIlKQRAkaLZhK7CUU2YpIDEs8PZPVOGE0IT8FXH1uCh+uO4zcQMBMepEbRx5/w9rWIooC0wZ2ZtrAzs2e+2tGCa8s3M+RchuDk6N45rIBDEryuloqal3szKvkX8sOsregGr0k4pYV/nTVEGaMSOGlqwdz78dbcXlkZNUbZz6uVyzv3jqCL7fksWB3YDJQPbKiUuVwszazjIl9Q4cTRlsMQWusqyqEaV2COGI9wv3L7sfu8a5urC4rb+54M2Ryj6Iqmp+8naP91p8C+iVG8KerhvDc/HREwRsJMm1gZx6c0ud0Ty2A9Zll/PaTLT6reX1WGde/vZ5FD0/gqy15vLsqC0VRfb056897+rvdpHWPYUKfeH58cDwfr8+h1OrkoiFduHRIF0TR2+qtWVQorw1sLt2QUamdiA0z4nDZfJUVRcH77+4Pt/DI9L6M6Rl7/F9CO+fbg9/ilt1+x+weOytyVxBhiMDusaOo3v83g2hgXNK4VhW70jjz0IT8FDFjRAqXDe3CoeIaEiKNJEQ0X/v5dPD6soyAAlcuWeGln/ex6mBpgDujHkVRWZheyOxJveidEMEfrwz0v2cVW3F6Qm+EgtfPPq5XXJPniKLAV78dy+PzdrL6YCkqXmvc4VHYdLicmR9s4sM7Rp21Yl7hqAjancfqtvLpJZ/y+1W/J700HUEQmNR1En8c/8fTMEuNtqTNhFwQBAnYAuSrqnpZW43bkTDppYBIkTON/IrAzUa3rLIttzKgfnlDVFSaiijMLbPx1HfpfklHAl4r2qiX8CgqAvD4Rf3pHNn8Sy4xysTHd43mX8sO8taKQ34vGIdb4e+LD/D1rDMjXl1WZJyyE7POfFwt6VRVpdpVjUVvCVqv+3DVYV7c8CLbircRoY9gUtdJmCSTXxs2vahnctfJJIcn88kln2Bz25BECaPUsgbbGmc2bWmRPwTsA1qwftY4U5nQJ56vthzxq1Bo1oskRBgotoZ2eXgUKKwO3b9xYXoBcqPUURUw6ESev3wQNU4PUwYktDpRKqukJugqIacscMP1VKOqKu/seocP0j/AKTvpEtaFP47/IyMTR7Z4jHX563hu3XOUO8rRiTpmDprJ7HNm+14INreNWxfeSpWzChWVCmcFC7MX0iOqB9lV2aioSIJEvCWeOefO8Y1r0bcsm1ajfdAm4YeCIKQAlwJz22I8jdPHw9P60CnM4KupYjFI9E6I4OGp/fzqrATj8425ZJXUBP1MECBY6I4oClw/sit3ntfjuLJdx/aKDZiXKEBa6umPJ//ywJf8N/2/2Dw2ZFUmryaP+5fdz9GaljWgPlJ9hIdWPESRrQi34sbusfNh+od8lfGV75xfDv+C0+P02/x1yA4Kagr4+OKPeTTtUf428W/Mv3I+0Sat8UZHpa3iyP8JPA6E7B4gCMK9giBsEQRhS0lJ801eNU4PCREmlj82iWcuHcDMcam8cu1Qvr1vHFMHdub+yb0w6kTCDaEFfc2h0qDHLxnSBamR78WgE7l6WPIJzffKYcmkxll8Ym7UiUSY9Dxx0emvhvjhng99kSP1eBQP8w/Nb9H13x36Do/i7+u2y3Y+3vOx7+cyRxl2OdAdVumqZEDsAG7qfxPnp5zvDTvU6LCcsGtFEITLgGJVVbcKgjAp1Hmqqr4LvAveWisnel+Nk0e4UcdvxnQPOP7ABX24bVwqh4pruPm9DQGbojpJIDYsuM81JcbCa9efw+PzdgPeZhdj64puhaLE6uSTDYfZd9TKqB6duHFUVyJM/j5ik17iu/vG8/32fNZlltErPpybR3cjPuL0+35r3bUBxzyKh0pn8CJmjalyVgXdtGw4bu+o3iGvn/L1FC5OvZj7ht2nuVI6OG3hIx8PXCEIwiWACYgUBOFTVVW1ZnwdkEiTnuHdYrh9bCofr8/xpeOLAph0ElMGJIS89pIhSUwZ0JkDhVbiwo0kRYeOXc6vtHPp66uxuWVcHoXVh0r4aP1hFjw0gcggYn7jqG6tKux1KpiUMomfsn/ys6pNOhNTuk1p0fXTUqfxY9aPfla9XtQztftU389GnRG9qPd19WlIsa2YL/Z/wdbirXx+yefHtdGq0T44YdeKqqpPqqqaoqpqKnAjsFwT8Y7P4xd5S9GGGSUEwVsG4JvZ45otAmbUSQxNiW5SxAH+uSQDq8ONq24j0+FWKLU6+Xxjbps9w8nmkbRHSA5PJkwXhkkyYZSMXNHrihZvdo5OHM1Vva/CKBkx68xYdBa6R3ZnzvA5lNpLWXR4kdc6b2J961JcZFVmsbNkZxs9lcaZiBZH3hFQZKjMAUssmE5NeKMkCvzuwv787sL+qKra5tbe5pxyGtX4wuFR2JBZxqyJvdr0XieLGFMM86+cz4aCDRTUFjA8YTg9o3u2+HpBEHhq9FPc1P8mdpbsJCksibTEND7Z+wn/2vYv9JIeVVUx681IsuQXbtiYI9YjDEsY1haPpXEG0qZCrqrqSmBlW46p0QwZi+H7WeC2ewV96PVw2T9AOnUlXU/Gkr13fDg5pTY/Y1MvCfRvSXboGYQkSoxPHn9CY/SI6kGPqB4AZFVm8cb2N3ApLlyKt0CZJEgMiRtCnDmOVXmrfMfrkVWZofFDT2gOGmc2mkXenqnIga9ug4aREbvnQUQiXPBMyMtUVeXtXzN5d1UW1Q4PI7rH8JdrhtArPvwUTLpl/N+0vqw9VIbDLaNS54PXSy1qCdeRWZC9ICCSRVZldpXuYsPNG9hwdAOPr3oct+JGVmXMOjNX9LqC7pGBm9caHQetQ1B7ZtXfYeXL0MgCwxIHj2eGvOztXw/x+tJDvo1KQYAok541v78gZNu200F6fhWvLcngYJGV4d1i+L9pfUiNO3NeNqcSp+zkydVPsjx3ObIaPMN2eMJwPrr4IzIrM5mXMY8aVw0X9biIcUnjtI3ODoLWIagj4nFAsD9qOXRNcIB3fs3ya/6gqt56Kgt2FzTZzf5EqbS5+GhdDltzyxmSHMXMcT2aDBMcnBzF+7en8f6abN5cfoj5O48yKCmSl68ZelyNL9oz/9jyD1blrQop4gD7yvext2wvA2MH8sSoJ07h7DRON1pjifbMwCtAMvgfkwze401gdQTGJjs9CqU1TVcdPBHKa11M/8cq/r3yEKsySnlvdTbT//ErBVVNN5L4fFMuf1+cQaXdG16352g1N763npImygV0ROZnzscpN/3MoiCSZ807RTPSOJPQhLw9kzgEJj8FkhGMEaAPg4SBMP2lJi9LS40JaGphkEQm9A5dA/xE+e+abCptLl9dFJdHwerw8NaKQ01e95+VmQGt4zyyyrfbzi7BauwXD4ZbcTM4rm27Pmm0DzQhb++MnwP/tweufgdm/gT3rgRz0zU1/nz1EKLMeiwGCUkUMOslrktLOanuis2Hy3E1iif0KCpbDlc0eV1lkNZxTo9CyUlcPZyJTOs+LWjlQ7HuT9isM3NTv5tICk861VPTOAPQfOQdgfB46H9pi0/vGR/O2icuYMHuAkprXIzvHcvQlJNbUGlgUiTbcip8DSkAJEGgf2LT4YSx4UZqgrSOO6+ZmuUdjd+P/j3Z1dlkVmYiCRJuxc3MQTOpcddg99i5KPUixnQZc7qnqXGa0IT8LCXMqOO6k7ix2Zi7J/Rk3tY8ZKcHRa2vQy7ywAWha4V4ZIWCqsAkF50oUOtq3tXQEXArbtyym0hDJF9c+gUHyg9QZi9jcPxgIg3tK6Ze4+ShCbnGKSE52sxPD57HP5ceZMeRSgYmRfLwlD70TggdTuhRVDxKYEFNvSRQZe/YQu6W3by86WW+z/weWZHpG9OXP5/3Z/p16uc7R1Zk9pfvRyfq6BvTVwsxPIvRhFzjlNE9Nox/3NDyNPH6jkq786r8MjwVFSb2O3kbs2cCr2x+hfmZ83HVhZLuL9/PzEUzWXLdEsw6M+ml6Tyw7AHsHjsqKgmWBN6Z9g7J4SdWFlijfaJtdmq0nvyt8M758EIMvNoHtnxwfOPIbqg4DK7Q3Xxeu34Y0RY9YUYJs17EqBP53YX9SG6m6FZ7RlZkvjv0nV+4oYqKR/Hw65FfcctuZi2ZRZmjDJvHht1j54j1CHOWz2liVI2OjGaRa7SO6qPw0eXgqquJXVMMvzwFegucc0PLx9nxBSx8AhS3NyNp3AMw+Wkax0X2Tghn/ZNTWL6/mEqbm/P7xpES07Fra8uqHDTcUFZlql3VbCveFlCnXFEVcqpzKKgpoEt4l1M1VY0zBM0i12gd2z/1WtINcdtg9astHyN/K/z8f+Cs8l7rscP6t2DnF0FPN+klLhnShZtHd2u3Il7jqmHlkZVsLNiIrITOzgQwSAaGxA1BaNQbT0VttgCX5ic/O9Esco3WYS0MXgLAVtbyMbZ8AO5G0ShuG2z4Nwy7+cTmdxLJqsqi0lHJwNiBmHSmFl+3PGc5T6x+wtduLUwXxgcXfUC3SP9GGIW1hXyT8Q0FtQVc3ONicqtzcckuFBQUVeGh4Q+RHJ5MgiUhIKZcFERSo1JJDEs88QfVaHdoQq7RPKUHYfVrULwXIpO9bhR3A7+2oINeLet6A9S5ZYIUa3M3na5/uqhyVnHf0vvIqMhAEiVUVeXP5/2ZKd2bf+ZqVzVPrH7CWyu8zhC3uW089utjfHX5sSbKe8v2cseiO7zhhoqbxTmLGRY/jOv6XofVbWVc0jifSOtFPW9Pe5sHlj2AzW1DRSUxLJF/Tf7XSXl+jTMfTcg1mqZoL7w/1WtBqzIU7QFBBJ3Z62LRm8AUCdNfbPmYQ2+AjEX+LwOdCc65qe3n3wY8v+559pXv87ZTqxPjJ1Y/waKERcSZm05M2nB0g9cSb+BNUVE5VHmISkelr7P9SxtewuY59n3YPXZ2luzk9kG3My11WsC4g2IHsey6ZRwoP4BBMtAzqqfmVjmL0YT8bMZa6BXl8Lo+m4oC2SuhLBOSzoXkEbD8T3VRJXUWtOL2iu7QG6BTKkR1hf6XeQW9OSpyYOkLkLMGDOHHXgSKDF1Hw7gHT9KDHj+KqrDiyIqAqoMCAstzl3N9v+ubvN4oha7uqBOP/fntK98X8LnNY2NXya6QfnFREBkQO6DJ+2ucHWhC3p6wFsKmuVCyD1InwPBbwRDWsmsVBbJ/hZL9EN4Z1v4Tivd7P0scAle/DfPuhPIsUDxege8xEYrSCXCDeBxgK4ErXm/53G3l8O4kcFSCWpfkozPB6N/CoGug86CWj3WKEQQh4CsQEFpkAY9NGhvgz9aLesYmjSXccCwZKsGSQH5Nvt95Fp1Fq52i0SI0IW8vlGd5hdBt92427v8Zlr8It/0AKSOavtZlg48u84q4x+UV6obKdHQ7fHAxOKvB06AYVcZCr6Aj+J+vM0FyyxoI+9jxmXfuaoNMTY8DDi6FC55t3Vgnmb1le1mSswSjZOTSnpcyrds0luYu9etUr6IypVvzPnKDZOD9C9/nkZWPUFhbiKqqjEwcyV8m/MXvvAfPfZDn1z3v67spCRLh+nCmp05v24fT6JBoQt5eWPYiOK0NhFAFVw3890K4e4nXFRKKjf/x+rY9IZrzqjLUlhJ0A9J3vzoxlwze6oppM1s3/9KD/i3p6qnMbd04J5n3d7/P2zvfxik70Yk63t/9Pi+Of5FSeym7SnehE3SIoshfz/8rnUydWjRm35i+/HjVjxTZijBJJp9fvCGX9rwUk87E2zvfpsxextiksTw8/GHMuo6b+KTRdmhC3l44stHfmq1HccPS5+G2+aGv3Ts/tIjXIwjexJxQGCyQMAhSx8PYB8Ac06Jp+0id4O0n6q5teFOvb/wModReyr93/NvXvNituHHj5qWNL7Hy+pUUPuwhbAAAIABJREFU2YqodFbSJ6ZP0JKyTSEIQrOhgVO6TWmRla+h0RhNyNsLsb2hOj/4Z8WBG2V+WFpQ8tUY6XWrBLOaARC8lv/xMvBK78qgeL9XzHVGb0OM1kS7nGR2lexCL+kDutA7PA6O1h6la0RXzWetcUaiZXa2Fy54FkJZgZ2b6Qozfg7ogyzR9RZvZyFTNNz6HVz8ilfQGyNI0Gty6+fcEJ0B7lgEl/7dG/Ey4TF4cCvE9TmxcduQzpbOKEFWPYqqEGNs5QpEQ+MUolnk7YWuI+GWefD5DQ3cJKJXoKc+3/S1PSfB5W/A4qe9tVGikmH6n7whgIIAqed7hTZ5OJx7K8y/H/Z85/WdSwbveRe9cuLPoDPAsJu8/85ABsYOpGdUTzIqMnwbmybJxKU9L/WLMNHQONMQ1Kb8oieJtLQ0dcuWLaf8vh0CewVseBuyVkBcXxj/UOusWtkNUp1l73F6xwtLALHR4ix/G+Sshcgk6Hdpy+LET4TaUtj3gzemvN8l3pfNacDqsvLaltdYnLMYvajn+n7Xc+/Qe/1ivjU0TheCIGxVVTUt4Lgm5Gchqgor/gzr3/RuoBoj4PLXW9Uurk3JXA7/u7kuaKbu9/GKN2HodadnPhoaZyihhFzzkZ9NFO6GjF+8lQbXv1lXedABtSUw765jCUKnEkWGb+72xph77N75eBzw44PgrDn189HQaIdo68WzAacVPr3WK+SiVCeQjVZisgu2fQgXvXxq51aaETw0UtRD3ibodcGpnY+GRjtEE/KzgaXPe7M3g5WfrUeVwWENPH5wKaz6G9QUQd+LYOLjYGlZIkyLMMeAHKT/puJpWdikhoaGJuSnleL9cGiJV8wGXOGtIngySP+2aREHbyji4Kv9j+35Hr6fday87Ja5cGABDL4WdnwOKHDOzTDpyePfDI1I/P/2zjs8qjL74593SkJCky4dKYIgAhIRGygKKhZERbEi6LL2upZVd/dn2XV3de26il1BLKjACqKI2BWlKV0RpEkVCBBIMsm8vz/ODJlyJ5lkhsxMOJ/nmYfkzsx7TyB8753znvM9UlWz4lMIjjZzeaVu/sDuVVtTUfYzVMhTxcwHxLjK75cqkml3wsipcOChsHMjbPgRGrRLTp2126n+3IDLI405/hLodXG0p/j0v4R7hJf6pKX+y0fYm5qZ9Yw0JF30FpVi5wZY/rFcQIY8BR//DRa8LZuvB58im69qy6oocZGwkBtjWgOvAs2Q/91jrLWVsMXbD/n9FxHxYG442Ek48SoRsa8eE4Et9UG7Y2H4OPm+qvQeKWuGdm26s+HsZySt0ao3NGwf/b4dvzks5uCEuPIzGaLcoF188cwfD+/fKIZcxiV5+xH/E0EHFXBFqSTJqFopAW6x1nYF+gLXGGO6JmHdmsuKmYCDWG1cCF8/ISmGoh0ivL9+IdN5EqHfrdD9XBFvb23p5DznOeg2VEr8nEQcoGmcXtfuLMiPYR8Qye6tIuIlhVI1U7wLCvPh7ZHyfLJE3Fco3i5fPgprvi/fR0ZRMpyE78itteuB9YGvdxpjlgAtgcWJrl1jqd1E7kKjMNFeJyWFMpT4hD9X/XxuDwx5UnxNCn6XO2d3HP/0gx+C14aCr4iwETeRlBbHn89e+VnAaiCiUmXrL/DscXD2c/FfQEAEe+4rsPR9GUN31DWQ0xCeHyDVOSWFcqE55AwY+qze7Ss1kqTmyI0x7YBewCyH50YDowHatGkT+fT+xcGnlM29DHp7eHOgYQeZixnp9+HOSs55cxrI3fhvcyVt0uoI2WyMRZu+MPozeLZf+WZag+6Lf6O2Vv3Yz21YKLa8Ny2SJqWK8JeKz/rGhZLLNy5xemzeE3ZtlkockD2AJe9Dj08T94xRlDQkaQ1Bxpg6wDvAjdbaHZHPW2vHWGvzrLV5TZo0SdZpMxNPtjgJtjtOxCerLvS9Wjb4IkeDeXPhiCuSc97CfHhuALx8Brx3FTx6GHz27/Lf4/dFt++H0rwn9Bkdfwzt+kGtuoGBFZFYEd3F5VjyhrJ8hmy0BjdkrV8ujmu+LRPxIL4C2VxVlBpIUu7IjTFeRMTHWWvfTcaaNZ4G7WDEZMndhn7cP+0h+OB2+dpfAj0vqpxQlsf0/5O719BSxC8fho4nynxOJ7LqyJ1vLH7/OfpnKA+3B0Z+AO/+UQQ3klKf5NHjYcOP4QOcgzj5tntqyYg7RamBJKNqxQAvAEustQnuyu2HRApgr4uh+zDYulLSHjnR02SqxG/zYP5rIpSh+Ipg0aTYQt6gLTTvAWu+w3GCkG+PiGm8s0NBLmKXfwgvDYbV34QLr3FDx5PiW6dJF/nEUhzRyp/TsGwzNYjLAz2Gxx+jomQQyUitHANcAgwwxswPPAYnYd39F082NO2SPBFfPUtmckaKOMimqze3/Pdf8EZsoc9tWPH7Y3HmEyK6WXXkZ/bUgr5XQbM4i54OPgXqtw4vzfTmwBmPi7VvbmO5ULboBZdNgTpNqxanoqQ56n6YjmxdAbOeldrsTidDzwsTs5F98RS583XCuGHUNGjdp/w1rIWXT4N1c8rq3725Isbdz616bL49Mki6YItsRDbpXLn3F+2Uks2lU6Buczj2JhlHpyg1ELWxzRTWzYWXT5dacn+JiGWTLnD59PhKBp34d3vY/bvzc8Yl5ZDXzam4UqSkWEr9Fk6A3EYyu7Pt0fJc8W5xUazXIkYnaQL49kiqqV6L5H1KUZQMJJaQa4t+ujHtjvABxb7dsGUZLJsicy+rwoGHBZqQHLB+uatdMAHyRpa/jicL+vxBHnvfb2HGffDt05LGcHvh1Aehx/lVizWSb54W069gfv6IK+Dkf2g9uKKEoH7k6caGBdHHigtg7fdVX3PgvbIZaWJct327YcvPVVt73msyVLkksOlZmA/v3yCbq4myeDJ8+Gf5dFJaLI9Zz8KclxNfW1FqECrk6cYBbaOPeWtLeqWqND8M/vgF9B7h3JDjrV1xjjwW3z4TXQJYUpQcsZ3m0M1qS8WnRlGUvaiQpxuD7gNPyMR7l1fywt3OTmzdRh3g9IeleiOrdlm3qDdXqkS6nF61dZ06Pq1fcuaJsmuD8/HC/MTXVpQahObI041OA+Git2WYQ/4aqanudxtkVbHErzAfPr5HuiW9OXDkVXDVNzDnJdlA7DQoYKhVxV+F7ueFOzmCXBy6J2HeZk4D2UCNpFUVPz0oSg1FhTwdOeg4eSSKtVIBs3lpWTfnzPtg13oYdD8U7pAKlJn/kOqTjgPLb8d34tibpLRx7fdSk15aDL0ukQtSZVk6Fb54SEoRO58CR98gG502dIKQkeqahe/KMI7yLkArv4BPH5AL4kH94YS7oF7zyselKGmOlh/WZFZ/C2PPls3SUDy14Movpb7ct1se3trQ5ki4aEIMZ8YK2LBA6t+b95Ru0Moy/3WYcnOZb4rbC/XbSJXK5w/Cnq0B73KPXCy8taXR59JJzmK+fAa8eVHZesYjzUvXzdl3k5gUZR8Tq/xQc+Q1mfy1OPqeA3x4l4hjcKPSVyAdoMumVu1cB3aX8siqiDhI+idyGtGujXBAa7h9paSYrC37ZOErkMqYWPFGrmdLpJX/xzerFp+ipDEq5JlC0U5Y+A78+Dbs2R7fe5odGi5mQWrVl1RIpLmUr0DuZKsba503NkuK5C4fAuWXEZ8efQWw6kvnNbevij7m2y1pJkWpYaiQZwKrvob/dIbJ18t0nYcPKbNk3fEbrPhM5nxGMvWW6Mk4xg1nPunsBOipVfU76kj8fslRL55csZuhMc6t+e6sMo+Xui2in/fkQIMY041a9Io+llUb2hxVfiyKkoGokKc7/lJ482LJcxfvkodvt4xGm3gNPNZTnn+0u3SFBoV70xLxRSHirtu4ofURcMKdUsVS9oQIZ8+LE485fx083hPGD4dJV8uFZ97Y8t9z2sNS7eIK5Lu9ubIB2zbgm3LiX8LLMo1L4o/VQXry38WMK7TMslEn2SBVlBqGVq2kKwVb4PN/SyWHUyqltFimzpcWyQNgzivQui90O0vy4y4vEJFa8Xhh1yboeqZ4uXxyn0y0b9QJDjldPFnqJDj4Y+LVcv7Q4Q5TboH2J0D9ls7vaXesNC3NfkE+ZRxyBnQ9q6wVv8tpMOwlmPl32LFejLEG3islik406wbXzILvnpcxch1PgsPOF5sBRalhaNVKOlJcAE8eIYLrd7CeBbkjdRqg0OFEuORduRA83LVM5INk14Nbf5FNv5l/l3O4swJ7okbu6A8bJlawVfEzKS2B+5tEx+bNgUF/hyMur/yasdi2SlJMtepD58FVr7VXlAxBq1YyiYXvyF14TBH3hHtwhxIsHazdOCR9YgLvyZFxcksmwwe3ws71ctdcsqdsQETJHljwTtXHogVLBJ2OV9W33IlvnoKn+kj1zf9ukPTNpiXJW19RMghNraQjm5eFOyCG4vJK7rjrEPjo7nCfE28u5IXc8R57o8wFXfSOzALtMVw2FZ/s41zNEsRXAIsmimvi5qXQuJNYyJbH+h9h9otQtEOGNq/6WlI3ZYFLeiQZbF8DM+4N7ybFwLuj4covknMORckgVMjTkZaHS4VFZCNPyyNgxCR5zlopsZv1rKRGSovhmBukIzKUVr3lEUosb/K9uKXB57HucgEoLYLu58vdvFPn56KJMPFKKRe0/kDaJzRlZ2DoM86NOKU+qQf35ki5ZDzpnBUzHYY3W9i0SP7OYo2dK9oF0/8q3azGJfNQB/wlsaEdipIGqJCnI4ecCV89Blt+kjtnl0fEevC/ykTKGNnsO/YmyRU3bB9/x2KHAdLiHtb6HoLbLecuKZIHiPi1OVJmiobiL4UpN0U030Tm7q0IfZNPoXHHssOrvobxF8g5/KVQpzGMnFZxCWROAwchp+zvKRZvXCBNT8F9g++fl7+74RVU1ChKmqM58nTE7YVRH8LA+8XUKm+UtNQ7zc3MaQAtelau7XzgvVKZklUHMJKucbnlz3otoGm3aFdD3+5wa9oNC0UIF0yIz+mwaCc8dwL8/ot8X7wbxp4LhdvlXP5iqVZ56kjxgCmPToPKcv9BPDlSOhlrOtGW5bDm+/DN35JC+PkjqdpRlAxG78hTSf462Ww8oE30c94c6HOFPJJNveZw/TxY8j/YvhpaHyl57eICqQAZPxzWO7xv7Wx4pJt4pq+bK8eMq+yuvSKKdsDTfeHUf0PdA8PLE4OU7JFqmuNugRn3yKZr7abQZbCIMcDhl8iFbuJV0vHpzpJPCif/I/a5d64XkY+8QLmzxAqg7oHx/QyKkoaokKeCHb/BGxdJThcjwnjBePEMry68OXDYeeHHgvMw80bBys+jB0ZgpT48f23EcZfc0ftjpGpCKS2GD26Ds/4r+XEnln0gF5ldG2XNnRtgw48hz08Vob/8I5kj6vJU7NrYvEfs8yUytENR0gBNraSC18+H9T8EctCFko9+9Uxpa08HDj5Zcu+eWuXnnPfil9ce2F1y9b1HhXdhRmKt5PhNDJdFl0d81GNdGHy7A46I26XBJx7r3Vr14NR/SZzGI2kkTw6c+UTsUk5FyRBUyKubbb+KcIelFayI0m9zUxVVNP1vg1t+Eo/yeKjTVPL418+DMx6BP3wSu27c75Pnzno6+jlPjuwFRFbsROLOkr/HytB7hHSP9r8N+t8OV38Nhw6t3BqKkoaokFc3pT7nigtj4s81VwffjYHHe8CyKRW/1psLXc6A92+WOZsbF8n4uOP+REwb3bdHSN76oglQr6W8rnYTOO0/0PPCipuHSouc55tWRJOD4fjbof+t8ulBUWoAmiOvbhp1lK7L7avDjxs3tDoiNTFFsmACTP+bQ448gLc21G4km7VZudDicPj+OSlBNC6Y/ZKkLLqfI34xYY07AUqKZDLR5R/CzYultT84IMJaaHsUrPom0BjlIsz8y5sLh54DdR0cHBVlP0SFvLoxBi54A14dAr7CwA2rCy58M30Mnb74T2wRB2h4kIyK+/FNEe4Fb5cNfAi2/E+5BW77BYY+C5OuEdfGSEqLYNEk2YgMnfJjDFz4llgJ/PQh1G0uue3FEwEDvS9LrmeLomQ4KuSpoFk3uHmpzLr0l0jLfTptuO3ZVv7zO9bBGxeWL/b+Emm26XYWdD5V/NQj1zXu8j1jug2VR5Djb49+na8Q1sySu/SWvSs/c1RRagAq5KnC7UnOgOV9QefBMPc1adKJxFNLGnacasBDsaWyAQoi1n1Gw1ePh9dxuzzitFhVVnwqXuwg6ZjcRjBiMjRoV/U1FSUD0dsXJZoBd0ODNtL5GdyYdXlExJt0jhhI4YDLKxuJb14sd+7LZ0C/W6H7ueLd4q0tHannvlD1DceiXTD+QukYLdopqZv8NfDWpVVbT1EyGL0jTzd2rJeKkU1LZNhC7xGQXbd6Y8htCFfPkq7Kbb/KAOSVn0u+On+d8+ZlKP4S2LS47PtfZoql7pAnJbe++3epOHEn8Ou38rPo6h/rl7+3XZsTH46hKBmECnk6sXUFPNtfhLK0WFIHs1+Q2ufsOlVft7gAfv2ybHyay6ERx1oR36JdMu/Sk1XmpPj1kzD31ZCceEUOhZFDknfDJ/dLx2jOAWUdpEHy10kDUJPOzrE54fLGDiPeNRSlhpAUITfGnAI8BriB5621/0zGuvsdM+6XFEHQPbBkj9RazxsLfa+s/HrbV8PiSVLm53KLWGfXhcumhNsB7NwArw2Vu2+XG3DB+a9B+/7SbfrZvyI2Ni1RJYEVYa3E0/SQsmOF+fDmJbDmW0ndeHNg2CvySaQi2veP7gx1ecQ3Jrdh/HGF4iuE35eLcVhV11CUFJBwjtwY4waeAk4FugIXGGO6Jrrufsna76ItYH17YNVXlVvHtwfGnQdP5JUNnwjmkXdugLdGhL9+wuWBYRaB1xXli71s0U65mDh1WXqy4ahroVaMmZmRlBZD/VbhxyZfL5U7JUVyjoItMG6Y84xSp/NfOknW9OZK/r7F4XDuS/HFE8m8cfBgB3jxZHi4C0y6Vqx1FSUDSMZmZx9gubV2hbW2GHgDGJKEdfc/GneKPubOlnLFyvDJ/ZJDjpzXCYCFLcskjwxSgbJmVnQVinFJjtybGy3AIGmNjieVX4IYSt0Dw3P9pT5Y+n5Z/fnedY2YZsVDi55w40L44+dw3Ry4YnrVcuPrf4Spt8iFrniXXFgWTICvn6j8WoqSApIh5C2BNSHfrw0cC8MYM9oYM9sYM3vz5s1JOG0N5IS7wytCjFs6J/Mq2fzyw/iKNySDG40xJ/JYEXNjZDKQN0fy0iBVJ4cMkTK/eOq2XV5o1082S0sDRljWEpVLDx6PNMsqzIdpd8qIurHnykCKIMbIBdDpYhMv81+Ptkco2SOj6xQlA6i2zU5r7RhgDEBeXp7D/2CFVr3hsqmS0/79Z2hzlFR7JLMCw+WVOZ45gZRIdl1oewys+jJaQDueJH92OAGu/ArmvAK7t8gEo4NPDi4Y+zzeHEmZ2FLJ1S+bIrXel0+X9vqDToCVn4af1/qlgSjIttXw1BFlF6Yty+DXL+C8V0NiqAR+v9jgLn1frBJ6jxQTL+vwKxmPLa+ipAHJEPJ1QOuQ71sFjilVoeXhcPGE8GPFBYCRu/N46HGBTO+JvCt3e6HNMXDO8+HHz3keXj9PqlZcHsk/nz8ufPZlow4w6N7w921bFXuIc/Me4ocy455AFU4hlCJfT7sDhr0EQ/8Lr55VtsnqL4Wzx4jAgojriydH/xwlhfDhnZUXcmvhnVHw00fi4eLywvcvwEn3yNzO0J/FUwt6XFi59RUlRSRDyL8HOhljDkIEfDig/wOSwa5NMhn+1y/l+4P6B4SuUfnvG3C3VF+s+FTEu7RE7ux7XOB8d1+nCYyeKWPYinfJEOR4Svg2LZbUipOP+vZVcvcembLwl8h4NZDOz6u+kkHPe7aJaVjoxWrjIiiIkYbbtqri+CJZN1fSO8G8vt8nj1nPQL/bpDonOMi63XHQ70+VP4eipICEhdxaW2KMuRb4ECk/fNFauyjhyPZHSkukbnzeWBHgXZuk/DD4EX/lZ/D6MPH6Lg9vjphwbV8jVSrNusaeLB9KZScUNT7YMc0NQKs+kr5xZ0VvuobWxBsDzQ9zXqN4l3xC8DtM9nEaj1cRa793rkTZugKOvg4OHwEbfpBmpeqc1qQoCZKUHLm1diowNRlr7de8N1ryt7HSFX4fbFwsd87xCM0BreWxr2jUQWZpLvkfYYruzoZB90mL/8wHwoXcmwt9r4lv/RaHS2NS5JxNgNMfqXy8DdoFPqFEXFhyDpALRu1G0GFA7Pf7/fLeiiwKFKWaUa+VdGHbr7B0SmwRD+JyyxDjdGHYyzL0uF4ryGkIh5wBN8wXka/bDC6dKB7sxi3VLkddI/Xn8eDJgvNeE/F3Z8saLq9Y47bvX/lYO54klrhub9kxb66knWJW7yC59a8eh3+1hX+0hMd6wsovKn9+RdlHGOu0W7+PycvLs7Nnz67286Y1yz+Gt0dWLNI5DeBPyxPzKUkFxQWygViV9vminfDLJ4ARMY5309eJ3VulKmjZVLkTP/ZmMfMqj3ljYeqt4TXz3ly4+ht1WlSqFWPMHGttXuTxDFODGkzTbjEaeJAUBQBW7lAzTcQhvhx9LLLrQtck9ZjlNoTTHpJHvHz5aHTjU6lP/GdO/Gty4lKUBMhARaih1GsuplJzXikTDW+uzL1sEJhN2WmQTINXqpfC/Ohjfp+4OCpKGqBCnk6c/AC0PRbmj5Na7t4jq5YLVpJLl8HixRJaPeOtDV1OT11MihKCCnk6YQwccro8lPThxL9JLf/ODVIKaoykeoKdr4qSYlTIFcFXKF2YP4yXNvnu58HAexLLbdcUchvCNd/JpKPtq8QqN1btu6KkABVyRZgwUipDgu3wc18Vv5dLJ6U2LpBGqV+/kIqeg/qV+cRUJy43HDyo+s+rKHGgQq7Ajt/glxnh7fSlRbD62/ibj/YVW1fAS4NlcpFBqkWGPA3dz0ldTIqSZmhDkCK5X1dW9HF3ljxXEb49IrT7ggmXw66NUBwYslxSCJOuhgKtGFGUICrkCjTtiqNpSqlPXAxjUbhDRrX9s7V0PT4/sGpmVuWtv2FB9NQkl0c+QSiKAqiQKyAWrmc8Bp7A8AiXR74e/GD5Q5/fuQJ+miaC7y+BdbPh5dOc3RCrgssTu3Ve/U4UZS+aI1eE7udCi16w6D0R4kOHOo+eC7J7K6yYGT6qzfrFjnb1N9DumOj37NwInz8oLo4N20P/26Bl79jnyMqFzoNl9Fto16vbCx0HVv5nVJQaigq5UkajDvF7cBcXyCi4SIxx9ospzIdn+0k3pN8HW34SQb9kIrTpG/s8Q56C9/4Y8DA3Yl877GX5FKEoCqBCrlSV+q2gTjOpqw6laCd8+k9JfbQ/vuz4/PFQlB/eHenbAx//H4yaFvs82XVg+Di5EPj2yBBnRVHC0By5UjWMgfNegez64gkTyvr58PpwWPFZ2bGNC5wterf8HN/5atVXEVeUGKiQK1WnRS+4ZanMznRFfLgr2SN2sUFaHxkt+FB+VUyqyV8H798MTx8lZZCbl6U6IkVxRIVcSYysXDAe54nzoWmX7sMkHeMJVJu4vNL+P/De6PelA7s2wTPHwtxXZDbponfhuRNg05JUR6YoUaiQK4nT4fjou23jgrYhlSveHPjDTBhwl4xTyxsFV34FBx5araHGzawxMjM0eIGyfkkNffpAauNSFAd0s1NJnO7D4LsxsGU5+AKTgLy50UMXsuvIkOOjr0tNnJVh/bzw0koQMd+wMDXxKEo5qJAriePJhsunSw36ys+hSWfodYm4BmYqrY8U69qgiRjIzNDy6t4VJUWokCvJwZMNPYbLoyZwxBUw+wXYvU2akVxeSQ8df0eqI1OUKFTIFcWJ3IZw1Tfw7X9h9dfQ7FA46lo4oHWqI1OUKFTIldTjL4U5L8vDuGQjtNcl4ErxXnxuQ9mcVZQ0R4VcST2Tr5fyvuDQ6Wl3iOthZSbdK8p+jJYfKqllx3pY+HaZiIN8Pe9V9RxXlDhRIVeST2mJtOf/PB2Kd5f/2q0rwJ0dfdydDdt+3SfhKUpNQ1MrSnLZvAxePl1a9AGsFdOr9sc7v75Jl3CL2iClxdC4476KUlFqFHpHriQPa+GNC6Fgs7ggFu2U7sg3LnI2zAKo3QiOvj68M9SbC/1uFaMsRVEqRO/IleSRvwby1xI1Ns4Y+PUr6HSS8/sG3A2t+sC816RqpfcIaeNXFCUuVMiV5OHOlrvySKyVhqHyOHiQPBRFqTQJpVaMMQ8aY5YaY340xrxnjDkgWYEpGUjdZtDy8AhLWwNZdaDt0SkLS1FqOonmyKcDh1prDwN+Av6ceEhKRnP+WEmTuLPEPKtxJxgxGVzuVEemKDWWhFIr1tqPQr79Fjg3sXCUjKd2Yxj1gfh5lxRpS7uiVAPJzJGPAt5M4npKJlOnaaojUJT9hgqF3BjzMeA0LPEua+2kwGvuAkqAceWsMxoYDdCmTZsqBasoiqJEU6GQW2tj1IwJxpjLgNOBE611KlnYu84YYAxAXl5ezNcpiqIolSOh1Iox5hTgNqC/tbaCXmxFURRlX5Bo1cqTQF1gujFmvjHmmSTEpCiKolSCRKtW1AxDURQlxZhy0tr77qTGbAZWJWGpxsCWJKyzr9D4EkPjSwyNL3HSLca21tomkQdTIuTJwhgz21qbl+o4YqHxJYbGlxgaX+JkQoyg7oeKoigZjwq5oihKhpPpQj4m1QFUgMaXGBpfYmh8iZMJMWZ2jlxRFEXJ/DtyRVGU/R4VckVRlAynxgi5MeYWY4w1xjROdSyhpOvwDWPMKcaYZcaY5caYO1IdTyjGmNbGmJnGmMXGmEXGmBtSHZMTxhi3MWaeMeb9VMcSiTHmAGPMhMDv3hJjzFGpjikUY8xNgX/bhcaY8cadFscJAAADV0lEQVSYWimO50VjzCZjzMKQYw2NMdONMT8H/myQyhjLo0YIuTGmNTAIWJ3qWBxIu+Ebxhg38BRwKtAVuMAY0zW1UYVRAtxire0K9AWuSbP4gtwALEl1EDF4DJhmre0C9CCN4jTGtASuB/KstYcCbmB4aqPiZeCUiGN3ADOstZ2AGYHv05IaIeTAI4h5V9rt3FprP7LWlgS+/RZolcp4AvQBlltrV1hri4E3gCEpjmkv1tr11tq5ga93IiLUMrVRhWOMaQWcBjyf6lgiMcbUB/oBLwBYa4uttdtTG1UUHiDHGOMBcoHfUhmMtfZzYGvE4SHAK4GvXwHOqtagKkHGC7kxZgiwzlr7Q6pjiYNRwAepDgIRxTUh368lzYQyiDGmHdALmJXaSKJ4FLl58Kc6EAcOAjYDLwVSP88bY2qnOqgg1tp1wEPIJ+j1QH7EtLF0oZm1dn3g6w1As1QGUx4ZIeTGmI8DubTIxxDgTuCvaRxf8DUVDt9QwjHG1AHeAW601u5IdTxBjDGnA5ustXNSHUsMPMDhwH+ttb2AAtIoLRDINQ9BLjgtgNrGmItTG1X5BGYtpN0n/iDJHPW2z4g13MIY0x35ZfjBGAOStphrjOljrd2Q6viCxDt8oxpZB4QO02wVOJY2GGO8iIiPs9a+m+p4IjgGONMYMxioBdQzxoy11qaLGK0F1lprg59iJpBGQg6cBKy01m4GMMa8CxwNjE1pVNFsNMY0t9auN8Y0BzalOqBYZMQdeSystQustU2tte2ste2QX+DDq1PEKyJk+MaZaTR843ugkzHmIGNMFrLRNDnFMe3FyFX5BWCJtfbhVMcTibX2z9baVoHfueHAJ2kk4gR+/9cYYzoHDp0ILE5hSJGsBvoaY3ID/9YnkkabsSFMBkYEvh4BTEphLOWSEXfkGc6TQDYyfAPgW2vtlakMyFpbYoy5FvgQqRh40Vq7KJUxRXAMcAmwwBgzP3DsTmvt1BTGlGlcB4wLXKhXACNTHM9erLWzjDETgLlIunEeKW6FN8aMB44HGhtj1gJ/A/4JvGWMuRyx3T4vdRGWj7boK4qiZDgZnVpRFEVRVMgVRVEyHhVyRVGUDEeFXFEUJcNRIVcURclwVMgVRVEyHBVyRVGUDOf/AQKk3/FjuyLPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4lnjrc7BEEl",
        "colab_type": "text"
      },
      "source": [
        "# Authorization & Submission\n",
        "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate a token on this programming assignment's page. <b>Note:</b> The token expires 30 minutes after generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uQRl29_BEEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "666ad0fa-daf3-4586-a0b0-83f5e672818e"
      },
      "source": [
        "STUDENT_EMAIL = # EMAIL HERE\n",
        "STUDENT_TOKEN = # TOKEN HERE\n",
        "grader.status()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-4abe722246ff>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    STUDENT_EMAIL = # EMAIL HERE\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCsqNOiBEEn",
        "colab_type": "text"
      },
      "source": [
        "If you want to submit these answers, run cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sc9imWNBEEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}