# -*- coding: utf-8 -*-
"""LT_assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-gH-4QEwl1YuSxmu3rAxaaic_lkXRsad
"""

import numpy as np
import pandas as pd
import re

training_file = open('de_text.train','r')
test_file = open('de_text.test','r')
training_data = training_file.read()
test_data = test_file.read()

trial_data = training_data[0:1000000]
#trial_test_data = test_data
#trial_data

def CreateUniGramFromString(data):
  splitted_data = re.split(' |\\n',data)
       
  #test_re = re.compile('\%\^\%')
  tokens = [tk for tk in splitted_data if tk not in ['%^%', '%$%']]
  return tokens

def CreateBiGramFromString(data):
  datalist = [sentence.split() for sentence in data.split('\n')]     
  biGrams = [sentence[ind]+' '+sentence[ind + 1] for sentence in datalist for ind, word in enumerate(sentence) if ind >0 and ind < len(sentence) - 2]
  return biGrams

def CreateTriGramFromString(data):
  datalist = [sentence.split() for sentence in data.split('\n')]     
  triGrams = [sentence[ind]+' '+sentence[ind + 1]+' '+sentence[ind + 2] for sentence in datalist for ind, word in enumerate(sentence) if ind < len(sentence) - 2]
  return triGrams

"""# Most Frequent Words"""

def mostFrequentWords(tokens_list, num_words):
  tokens_set = set(tokens_list)
  print(len(tokens_list), len(tokens_set))
  print(tokens_set)
  sorted_list = sorted(tokens_set, key=tokens_list.count, reverse=True)
  return sorted_list[:num_words]

#unigram token list
unigram_list = CreateUniGramFromString(trial_data)
mostFrequentWords(unigram_list, 20)

#bigram token list

bigram_list = CreateBiGramFromString(trial_data)
mostFrequentWords(bigram_list, 20)

#Trigram token list

trigram_list = CreateTriGramFromString(trial_data)
mostFrequentWords(trigram_list, 20)

"""# % of test and training words"""

def testDataAnalysis(training_data, test_data):
  test_set = set(test_data)
  train_set = set(training_data)

  set_intersection = set(test_set.intersection(train_set))

  print('No of unqiue tokens in training_data ', len(train_set))
  print('No of unqiue tokens in training_data ', len(test_set))
  print('No of common tokens in intersction ', len(set_intersection))
  print(set_intersection)
  print('% of new tokens in test ', (len(test_set) - len(set_intersection)) * 100 / len(test_set))

testDataAnalysis(unigram_list, CreateUniGramFromString(test_data))

#bigram analysis
testDataAnalysis(bigram_list, CreateBiGramFromString(test_data))

#Trigram analysis
testDataAnalysis(trigram_list, CreateTriGramFromString(test_data))

def biGrams(sentence):
  word_list = sentence.split()
  return [word_list[ind]+' '+word_list[ind + 1] for ind, word in enumerate(word_list) if ind > 0 and ind < len(word_list) -2 ]

datalist = [biGrams(sentence) for sentence in test_data[0:100000].split('\n')]     
newSentences = [sentence for sentence in datalist if not set(sentence).issubset(set(bigram_list))]
print(newSentences)
print(set(bigram_list))
print(len(newSentences))
print(len(datalist))

